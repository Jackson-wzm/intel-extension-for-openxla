diff --git a/setup.py b/setup.py
index 37238ba..19bec2f 100644
--- a/setup.py
+++ b/setup.py
@@ -27,8 +27,8 @@ from version import __version__  # pylint: disable=g-import-not-at-top
 with open('README.md') as fp:
   _LONG_DESCRIPTION = fp.read()
 
-_jax_version = '0.4.11'
-_jaxlib_version = '0.4.11'
+_jax_version = '0.4.13'
+_jaxlib_version = '0.4.13'
 
 setuptools.setup(
     name='t5x',
@@ -48,8 +48,8 @@ setuptools.setup(
     install_requires=[
         'absl-py',
         'cached_property',
-        'clu @ git+https://github.com/google/CommonLoopUtils#egg=clu',
-        'flax @ git+https://github.com/google/flax#egg=flax',
+        'clu == 0.0.9',
+        'flax == 0.6.11',
         'fiddle >= 0.2.5',
         'gin-config',
         f'jax >= {_jax_version}',
diff --git a/t5x/contrib/gpu/scripts_gpu/tfds_pile.py b/t5x/contrib/gpu/scripts_gpu/tfds_pile.py
index 77ef0d9..b641041 100644
--- a/t5x/contrib/gpu/scripts_gpu/tfds_pile.py
+++ b/t5x/contrib/gpu/scripts_gpu/tfds_pile.py
@@ -80,7 +80,8 @@ _CITATION = """
 _DATASET_MODES = ["lm"]
 
 _PILE_URL = 'https://the-eye.eu/public/AI/pile/train/{}.jsonl.zst'
-_PILE_SPLITS = 30
+#_PILE_SPLITS = 30
+_PILE_SPLITS = 1
 
 _URLS = {
     'the_pile': {
@@ -171,7 +172,21 @@ class ThePile(tfds.core.GeneratorBasedBuilder):
     )
 
   def _split_generators(self, dl_manager: tfds.download.DownloadManager):
+    dl_paths = {
+        'train': [
+            '/nfs2/yuqingding/datasets/ThePile/downloads/00.jsonl.zst'
+        ],
+        'test': '/nfs2/yuqingding/datasets/ThePile/downloads/test.jsonl.zst',
+        'validation': '/nfs2/yuqingding/datasets/ThePile/downloads/val.jsonl.zst',
+    }
+    return {
+            'train': self._generate_examples(dl_paths['train']),
+            'validation': self._generate_examples(dl_paths['validation']),
+            'test': self._generate_examples(dl_paths['test']),
+    }
+
     dl_manager.verify_ssl = False
+    print(_URLS['the_pile'])
     dl_paths = dl_manager.download(_URLS['the_pile'])
     print(dl_paths)
     return {
diff --git a/t5x/utils.py b/t5x/utils.py
index eb3f1aa..2162d05 100644
--- a/t5x/utils.py
+++ b/t5x/utils.py
@@ -1631,6 +1631,10 @@ def get_infer_fn(
         per_shard_batch_size,
     )
 
+    idx = 0
+    total_time = 0
+    total_step = 20
+    warmup_step = 10
     # Run inference for each replica set.
     batched_results, all_indices = [], []
     for index, infer_batch in sharded_ds.as_numpy_iterator():
@@ -1672,6 +1676,13 @@ def get_infer_fn(
             )
         )
       else:
+        import time
+        import os
+        if idx == warmup_step - 1 and os.environ["DEVICE_TYPE"] == "CUDA" and os.environ["IS_PROFILE"]=="True":
+            jax.profiler.start_trace(os.environ["PROFILE_DIR"])
+        if idx == warmup_step - 1 and os.environ["DEVICE_TYPE"] == "XPU" and os.environ["IS_PROFILE"]=="True":
+            os.environ["PTI_ENABLE_COLLECTION"] = "1"
+        start_time = time.time()
         batch_indices, batch_result = partitioned_infer_step(
             train_state.params,
             infer_batch,
@@ -1679,7 +1690,36 @@ def get_infer_fn(
             index,
             train_state.flax_mutables,
         )
-        logging.info('Inference of batch %s done.', index)
+        end_time = time.time()
+        if idx == warmup_step - 1 and os.environ["DEVICE_TYPE"] == "XPU" and os.environ["IS_PROFILE"]=="True":
+            os.environ["PTI_ENABLE_COLLECTION"] = "0"
+        if idx == warmup_step - 1 and os.environ["DEVICE_TYPE"] == "CUDA" and os.environ["IS_PROFILE"]=="True":
+            jax.profiler.stop_trace()
+        if True:
+            cur_time = (end_time - start_time)
+            throughput = batch_result.shape[0]/cur_time
+            info_str = ""
+            for k in infer_batch:
+                info_str = info_str + k + ":" + str(infer_batch[k].shape) + ","
+            info_str = info_str  + "batch_result:" + str(batch_result.shape) + ","
+            logging.info('%s', info_str)
+
+            info_str = "step:" + str(idx) + ","
+            info_str = info_str + "time:" + str(cur_time) + " s,"
+            info_str = info_str + "throughput:" + str(throughput) + " sentences/sencond,"
+            logging.info('%s', info_str)
+            if idx >= warmup_step:
+                total_time += cur_time
+            idx += 1
+            if idx >= total_step:
+              cur_time = total_time/(total_step - warmup_step)
+              throughput = batch_result.shape[0]/cur_time
+              info_str = "avg time:" + str(cur_time) + " s,"
+              info_str = info_str  + "avg throughput:" + str(throughput) + " sentences/sencond,"
+              logging.info('%s', info_str)
+              exit()
+
+        #logging.info('Inference of batch %s done. %s', index, info_str)
 
       def _copy_to_host_async(x):
         if hasattr(x, 'addressable_data'):
