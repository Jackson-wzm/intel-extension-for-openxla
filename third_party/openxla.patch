diff --git a/third_party/tsl/third_party/eigen3/eigen.patch b/third_party/tsl/third_party/eigen3/eigen.patch
new file mode 100644
index 000000000..8954b1d2e
--- /dev/null
+++ b/third_party/tsl/third_party/eigen3/eigen.patch
@@ -0,0 +1,13 @@
+diff --git a/Eigen/src/Core/util/Macros.h b/Eigen/src/Core/util/Macros.h
+index 69bbf2e73..251053b55 100644
+--- a/Eigen/src/Core/util/Macros.h
++++ b/Eigen/src/Core/util/Macros.h
+@@ -686,7 +686,7 @@
+ // For instance, if compiling with gcc and -std=c++17, then EIGEN_COMP_CXXVER
+ // is defined to 17.
+ #if EIGEN_CPLUSPLUS >= 202002L
+-  #define EIGEN_COMP_CXXVER 20
++  #define EIGEN_COMP_CXXVER 17
+ #elif EIGEN_CPLUSPLUS >= 201703L
+   #define EIGEN_COMP_CXXVER 17
+ #elif EIGEN_CPLUSPLUS >= 201402L
diff --git a/third_party/tsl/third_party/eigen3/workspace.bzl b/third_party/tsl/third_party/eigen3/workspace.bzl
index 027454e46..ca5489940 100644
--- a/third_party/tsl/third_party/eigen3/workspace.bzl
+++ b/third_party/tsl/third_party/eigen3/workspace.bzl
@@ -14,6 +14,7 @@ def repo():
     tf_http_archive(
         name = "eigen_archive",
         build_file = "//third_party/eigen3:eigen_archive.BUILD",
+        patch_file = ["//third_party/eigen3:eigen.patch"],
         sha256 = EIGEN_SHA256,
         strip_prefix = "eigen-{commit}".format(commit = EIGEN_COMMIT),
         urls = tf_mirror_urls("https://gitlab.com/libeigen/eigen/-/archive/{commit}/eigen-{commit}.tar.gz".format(commit = EIGEN_COMMIT)),
diff --git a/third_party/tsl/third_party/gpus/cuda_configure.bzl b/third_party/tsl/third_party/gpus/cuda_configure.bzl
index 166538843..e88f55019 100644
--- a/third_party/tsl/third_party/gpus/cuda_configure.bzl
+++ b/third_party/tsl/third_party/gpus/cuda_configure.bzl
@@ -26,7 +26,7 @@
   * `PYTHON_BIN_PATH`: The python binary path
 """
 
-load("//third_party/clang_toolchain:download_clang.bzl", "download_clang")
+load("@xla//third_party/tsl/third_party/clang_toolchain:download_clang.bzl", "download_clang")
 load(
     "@bazel_tools//tools/cpp:lib_cc_configure.bzl",
     "escape_string",
@@ -39,7 +39,7 @@ load(
     "setup_vc_env_vars",
 )
 load(
-    "//third_party/remote_config:common.bzl",
+    "@xla//third_party/tsl/third_party/remote_config:common.bzl",
     "config_repo_label",
     "err_out",
     "execute",
diff --git a/third_party/tsl/third_party/grpc/c_ares.patch b/third_party/tsl/third_party/grpc/c_ares.patch
new file mode 100644
index 000000000..752965f93
--- /dev/null
+++ b/third_party/tsl/third_party/grpc/c_ares.patch
@@ -0,0 +1,200 @@
+diff --git a/bazel/grpc_deps.bzl b/bazel/grpc_deps.bzl
+index c2f2f43820..72de3cea94 100644
+--- a/bazel/grpc_deps.bzl
++++ b/bazel/grpc_deps.bzl
+@@ -182,9 +182,9 @@ def grpc_deps():
+         http_archive(
+             name = "com_github_cares_cares",
+             build_file = "@com_github_grpc_grpc//third_party:cares/cares.BUILD",
+-            sha256 = "e8c2751ddc70fed9dc6f999acd92e232d5846f009ee1674f8aee81f19b2b915a",
+-            strip_prefix = "c-ares-e982924acee7f7313b4baa4ee5ec000c5e373c30",
+-            url = "https://github.com/c-ares/c-ares/archive/e982924acee7f7313b4baa4ee5ec000c5e373c30.tar.gz",
++            sha256 = "321700399b72ed0e037d0074c629e7741f6b2ec2dda92956abe3e9671d3e268e",
++            strip_prefix = "c-ares-1.19.1",
++            url = "https://github.com/c-ares/c-ares/releases/download/cares-1_19_1/c-ares-1.19.1.tar.gz",
+         )
+
+     if "com_google_absl" not in native.existing_rules():
+diff --git a/third_party/cares/cares.BUILD b/third_party/cares/cares.BUILD
+index 203712b182..2561b1a4bc 100644
+--- a/third_party/cares/cares.BUILD
++++ b/third_party/cares/cares.BUILD
+@@ -109,84 +109,95 @@ genrule(
+ cc_library(
+     name = "ares",
+     srcs = [
+-        "ares__close_sockets.c",
+-        "ares__get_hostent.c",
+-        "ares__read_line.c",
+-        "ares__timeval.c",
+-        "ares_cancel.c",
+-        "ares_create_query.c",
+-        "ares_data.c",
+-        "ares_destroy.c",
+-        "ares_expand_name.c",
+-        "ares_expand_string.c",
+-        "ares_fds.c",
+-        "ares_free_hostent.c",
+-        "ares_free_string.c",
+-        "ares_getenv.c",
+-        "ares_gethostbyaddr.c",
+-        "ares_gethostbyname.c",
+-        "ares_getnameinfo.c",
+-        "ares_getopt.c",
+-        "ares_getsock.c",
+-        "ares_init.c",
+-        "ares_library_init.c",
+-        "ares_llist.c",
+-        "ares_mkquery.c",
+-        "ares_nowarn.c",
+-        "ares_options.c",
+-        "ares_parse_a_reply.c",
+-        "ares_parse_aaaa_reply.c",
+-        "ares_parse_mx_reply.c",
+-        "ares_parse_naptr_reply.c",
+-        "ares_parse_ns_reply.c",
+-        "ares_parse_ptr_reply.c",
+-        "ares_parse_soa_reply.c",
+-        "ares_parse_srv_reply.c",
+-        "ares_parse_txt_reply.c",
+-        "ares_platform.c",
+-        "ares_process.c",
+-        "ares_query.c",
+-        "ares_search.c",
+-        "ares_send.c",
+-        "ares_strcasecmp.c",
+-        "ares_strdup.c",
+-        "ares_strsplit.c",
+-        "ares_strerror.c",
+-        "ares_timeout.c",
+-        "ares_version.c",
+-        "ares_writev.c",
+-        "bitncmp.c",
+-        "inet_net_pton.c",
+-        "inet_ntop.c",
+-        "windows_port.c",
++        "src/lib/ares__read_line.c",
++        "src/lib/ares__get_hostent.c",
++        "src/lib/ares__close_sockets.c",
++        "src/lib/ares__timeval.c",
++        "src/lib/ares_gethostbyaddr.c",
++        "src/lib/ares_getenv.c",
++        "src/lib/ares_free_string.c",
++        "src/lib/ares_free_hostent.c",
++        "src/lib/ares_fds.c",
++        "src/lib/ares_expand_string.c",
++        "src/lib/ares_create_query.c",
++        "src/lib/ares_cancel.c",
++        "src/lib/ares_android.c",
++        "src/lib/ares_parse_txt_reply.c",
++        "src/lib/ares_parse_srv_reply.c",
++        "src/lib/ares_parse_soa_reply.c",
++        "src/lib/ares_parse_ptr_reply.c",
++        "src/lib/ares_parse_ns_reply.c",
++        "src/lib/ares_parse_naptr_reply.c",
++        "src/lib/ares_parse_mx_reply.c",
++        "src/lib/ares_parse_caa_reply.c",
++        "src/lib/ares_options.c",
++        "src/lib/ares_nowarn.c",
++        "src/lib/ares_mkquery.c",
++        "src/lib/ares_llist.c",
++        "src/lib/ares_getsock.c",
++        "src/lib/ares_getnameinfo.c",
++        "src/lib/bitncmp.c",
++        "src/lib/ares_writev.c",
++        "src/lib/ares_version.c",
++        "src/lib/ares_timeout.c",
++        "src/lib/ares_strerror.c",
++        "src/lib/ares_strcasecmp.c",
++        "src/lib/ares_search.c",
++        "src/lib/ares_platform.c",
++        "src/lib/windows_port.c",
++        "src/lib/inet_ntop.c",
++        "src/lib/ares__sortaddrinfo.c",
++        "src/lib/ares__readaddrinfo.c",
++        "src/lib/ares_parse_uri_reply.c",
++        "src/lib/ares__parse_into_addrinfo.c",
++        "src/lib/ares_parse_a_reply.c",
++        "src/lib/ares_parse_aaaa_reply.c",
++        "src/lib/ares_library_init.c",
++        "src/lib/ares_init.c",
++        "src/lib/ares_gethostbyname.c",
++        "src/lib/ares_getaddrinfo.c",
++        "src/lib/ares_freeaddrinfo.c",
++        "src/lib/ares_expand_name.c",
++        "src/lib/ares_destroy.c",
++        "src/lib/ares_data.c",
++        "src/lib/ares__addrinfo_localhost.c",
++        "src/lib/ares__addrinfo2hostent.c",
++        "src/lib/inet_net_pton.c",
++        "src/lib/ares_strsplit.c",
++        "src/lib/ares_strdup.c",
++        "src/lib/ares_send.c",
++        "src/lib/ares_rand.c",
++        "src/lib/ares_query.c",
++        "src/lib/ares_process.c",
+     ],
+     hdrs = [
+-        "ares.h",
+         "ares_build.h",
+         "ares_config.h",
+-        "ares_data.h",
+-        "ares_dns.h",
+-        "ares_getenv.h",
+-        "ares_getopt.h",
+-        "ares_inet_net_pton.h",
+-        "ares_iphlpapi.h",
+-        "ares_ipv6.h",
+-        "ares_library_init.h",
+-        "ares_llist.h",
+-        "ares_nowarn.h",
+-        "ares_platform.h",
+-        "ares_private.h",
+-        "ares_rules.h",
+-        "ares_setup.h",
+-        "ares_strcasecmp.h",
+-        "ares_strdup.h",
+-        "ares_strsplit.h",
+-        "ares_version.h",
+-        "ares_writev.h",
+-        "bitncmp.h",
+-        "config-win32.h",
+-        "nameser.h",
+-        "setup_once.h",
++        "include/ares_version.h",
++        "include/ares.h",
++        "include/ares_rules.h",
++        "include/ares_dns.h",
++        "include/ares_nameser.h",
++        "src/tools/ares_getopt.h",
++        "src/lib/ares_strsplit.h",
++        "src/lib/ares_android.h",
++        "src/lib/ares_private.h",
++        "src/lib/ares_llist.h",
++        "src/lib/ares_platform.h",
++        "src/lib/ares_ipv6.h",
++        "src/lib/config-dos.h",
++        "src/lib/bitncmp.h",
++        "src/lib/ares_strcasecmp.h",
++        "src/lib/setup_once.h",
++        "src/lib/ares_inet_net_pton.h",
++        "src/lib/ares_data.h",
++        "src/lib/ares_getenv.h",
++        "src/lib/config-win32.h",
++        "src/lib/ares_strdup.h",
++        "src/lib/ares_iphlpapi.h",
++        "src/lib/ares_setup.h",
++        "src/lib/ares_writev.h",
++        "src/lib/ares_nowarn.h",
+     ],
+     copts = [
+         "-D_GNU_SOURCE",
+@@ -202,7 +213,7 @@ cc_library(
+         "//conditions:default": [],
+     }),
+     defines = ["CARES_STATICLIB"],
+-    includes = ["."],
++    includes = ["include", "."],
+     linkopts = select({
+         ":windows": ["-defaultlib:ws2_32.lib"],
+         "//conditions:default": [],
diff --git a/third_party/tsl/third_party/grpc/upb_platform_fix.patch b/third_party/tsl/third_party/grpc/upb_platform_fix.patch
index 6edd66067..022c9d155 100644
--- a/third_party/tsl/third_party/grpc/upb_platform_fix.patch
+++ b/third_party/tsl/third_party/grpc/upb_platform_fix.patch
@@ -11,3 +11,12 @@ index ad85b202..2311b2e4 100644
  )
 
  config_setting(
+@@ -24,7 +24,7 @@ exports_files([
+
+ CPPOPTS = [
+     # copybara:strip_for_google3_begin
+-    "-Werror",
++    # "-Werror",
+     "-Wno-long-long",
+     # copybara:strip_end
+ ]
diff --git a/third_party/tsl/third_party/llvm/build.patch b/third_party/tsl/third_party/llvm/build.patch
index 479e08cde..33f585b70 100644
--- a/third_party/tsl/third_party/llvm/build.patch
+++ b/third_party/tsl/third_party/llvm/build.patch
@@ -44,3 +44,24 @@ index 7770284e5543..0b45127495dc 100644
          "//conditions:default": [
              "BLAKE3_NO_AVX2",
              "BLAKE3_NO_AVX512",
+diff --git a/mlir/lib/Bytecode/Reader/BytecodeReader.cpp b/mlir/lib/Bytecode/Reader/BytecodeReader.cpp
+index 177372c68046..40d49dc13b2f 100644
+--- a/mlir/lib/Bytecode/Reader/BytecodeReader.cpp
++++ b/mlir/lib/Bytecode/Reader/BytecodeReader.cpp
+@@ -1549,6 +1549,8 @@ private:
+   const std::shared_ptr<llvm::SourceMgr> &bufferOwnerRef;
+ };
+
++// TODO: Disable clang opt since it crashes with clang-17 compiler.
++#pragma clang optimize off
+ LogicalResult BytecodeReader::Impl::read(
+     Block *block, llvm::function_ref<bool(Operation *)> lazyOpsCallback) {
+   EncodingReader reader(buffer.getBuffer(), fileLoc);
+@@ -1628,6 +1630,7 @@ LogicalResult BytecodeReader::Impl::read(
+   // Finally, process the IR section.
+   return parseIRSection(*sectionDatas[bytecode::Section::kIR], block);
+ }
++#pragma clang optimize on
+
+ LogicalResult BytecodeReader::Impl::parseVersion(EncodingReader &reader) {
+   if (failed(reader.parseVarInt(version)))
diff --git a/third_party/tsl/third_party/llvm/spirv.patch b/third_party/tsl/third_party/llvm/spirv.patch
new file mode 100644
index 000000000..8643e9181
--- /dev/null
+++ b/third_party/tsl/third_party/llvm/spirv.patch
@@ -0,0 +1,76 @@
+diff --git a/llvm/lib/Passes/PassBuilderPipelines.cpp b/llvm/lib/Passes/PassBuilderPipelines.cpp
+index 78e0e6353056..4f9f51164bfe 100644
+--- a/llvm/lib/Passes/PassBuilderPipelines.cpp
++++ b/llvm/lib/Passes/PassBuilderPipelines.cpp
+@@ -183,6 +183,10 @@ static cl::opt<bool> EnableGlobalAnalyses(
+     "enable-global-analyses", cl::init(true), cl::Hidden,
+     cl::desc("Enable inter-procedural analyses"));
+ 
++static cl::opt<bool>
++    SYCLOptimizationMode("sycl-opt", cl::init(false), cl::Hidden,
++                         cl::desc("Enable SYCL optimization mode."));
++
+ static cl::opt<bool>
+     RunPartialInlining("enable-partial-inlining", cl::init(false), cl::Hidden,
+                        cl::desc("Run Partial inlinining pass"));
+@@ -406,6 +410,7 @@ PassBuilder::buildO1FunctionSimplificationPipeline(OptimizationLevel Level,
+   // Form canonically associated expression trees, and simplify the trees using
+   // basic mathematical properties. For example, this will form (nearly)
+   // minimal multiplication trees.
++  if (!SYCLOptimizationMode) {
+   FPM.addPass(ReassociatePass());
+ 
+   // Add the primary loop simplification pipeline.
+@@ -477,7 +482,7 @@ PassBuilder::buildO1FunctionSimplificationPipeline(OptimizationLevel Level,
+   FPM.addPass(createFunctionToLoopPassAdaptor(std::move(LPM2),
+                                               /*UseMemorySSA=*/false,
+                                               /*UseBlockFrequencyInfo=*/false));
+-
++  }
+   // Delete small array after loop unroll.
+   FPM.addPass(SROAPass(SROAOptions::ModifyCFG));
+ 
+@@ -580,6 +585,7 @@ PassBuilder::buildFunctionSimplificationPipeline(OptimizationLevel Level,
+   // Form canonically associated expression trees, and simplify the trees using
+   // basic mathematical properties. For example, this will form (nearly)
+   // minimal multiplication trees.
++  if (!SYCLOptimizationMode) {
+   FPM.addPass(ReassociatePass());
+ 
+   if (EnableConstraintElimination)
+@@ -657,7 +663,7 @@ PassBuilder::buildFunctionSimplificationPipeline(OptimizationLevel Level,
+   FPM.addPass(createFunctionToLoopPassAdaptor(std::move(LPM2),
+                                               /*UseMemorySSA=*/false,
+                                               /*UseBlockFrequencyInfo=*/false));
+-
++  }
+   // Delete small array after loop unroll.
+   FPM.addPass(SROAPass(SROAOptions::ModifyCFG));
+ 
+@@ -715,6 +721,9 @@ PassBuilder::buildFunctionSimplificationPipeline(OptimizationLevel Level,
+ 
+   invokeScalarOptimizerLateEPCallbacks(FPM, Level);
+ 
++  if (SYCLOptimizationMode)
++    FPM.addPass(SimplifyCFGPass());
++  else
+   FPM.addPass(SimplifyCFGPass(SimplifyCFGOptions()
+                                   .convertSwitchRangeToICmp(true)
+                                   .hoistCommonInsts(true)
+@@ -1385,6 +1394,7 @@ PassBuilder::buildModuleOptimizationPipeline(OptimizationLevel Level,
+ 
+   invokeVectorizerStartEPCallbacks(OptimizePM, Level);
+ 
++  if (!SYCLOptimizationMode) {
+   LoopPassManager LPM;
+   // First rotate loops that may have been un-rotated by prior passes.
+   // Disable header duplication at -Oz.
+@@ -1408,7 +1418,7 @@ PassBuilder::buildModuleOptimizationPipeline(OptimizationLevel Level,
+   OptimizePM.addPass(InjectTLIMappings());
+ 
+   addVectorPasses(Level, OptimizePM, /* IsFullLTO */ false);
+-
++  }
+   // LoopSink pass sinks instructions hoisted by LICM, which serves as a
+   // canonicalization pass that enables other optimizations. As a result,
+   // LoopSink pass needs to be a very late IR pass to avoid undoing LICM
diff --git a/third_party/tsl/third_party/llvm/workspace.bzl b/third_party/tsl/third_party/llvm/workspace.bzl
index fa02d95b4..405cee4be 100644
--- a/third_party/tsl/third_party/llvm/workspace.bzl
+++ b/third_party/tsl/third_party/llvm/workspace.bzl
@@ -17,6 +17,7 @@ def repo(name):
         ],
         build_file = "//third_party/llvm:llvm.BUILD",
         patch_file = [
+            "//third_party/llvm:spirv.patch",
             "//third_party/llvm:generated.patch",  # Autogenerated, don't remove.
             "//third_party/llvm:build.patch",
             "//third_party/llvm:mathextras.patch",
diff --git a/third_party/tsl/tsl/framework/bfc_allocator.cc b/third_party/tsl/tsl/framework/bfc_allocator.cc
index e19c0018d..79a8ea7c8 100644
--- a/third_party/tsl/tsl/framework/bfc_allocator.cc
+++ b/third_party/tsl/tsl/framework/bfc_allocator.cc
@@ -135,24 +135,19 @@ bool BFCAllocator::Extend(size_t alignment, size_t rounded_bytes) {
   size_t bytes = std::min(curr_region_allocation_bytes_, available_bytes);
   size_t bytes_received;
   void* mem_addr = sub_allocator_->Alloc(alignment, bytes, &bytes_received);
-  if (mem_addr == nullptr && !started_backpedal_) {
-    // Only backpedal once.
-    started_backpedal_ = true;
-
+  if (mem_addr == nullptr) {
     static constexpr float kBackpedalFactor = 0.9;
 
     // Try allocating less memory.
     while (mem_addr == nullptr) {
       bytes = RoundedBytes(bytes * kBackpedalFactor);
-      if (bytes < rounded_bytes) break;
+      if (bytes < rounded_bytes) {
+        return false;
+      }
       mem_addr = sub_allocator_->Alloc(alignment, bytes, &bytes_received);
     }
   }
 
-  if (mem_addr == nullptr) {
-    return false;
-  }
-
   if (!increased_allocation) {
     // Increase the region size of the next required allocation.
     curr_region_allocation_bytes_ *= 2;
diff --git a/third_party/tsl/tsl/framework/bfc_allocator.h b/third_party/tsl/tsl/framework/bfc_allocator.h
index 47619856a..76921c5f0 100644
--- a/third_party/tsl/tsl/framework/bfc_allocator.h
+++ b/third_party/tsl/tsl/framework/bfc_allocator.h
@@ -579,10 +579,6 @@ class BFCAllocator : public Allocator {
   // The size of the current region allocation.
   size_t curr_region_allocation_bytes_;
 
-  // An indicator that expansion of a region has hit the limits
-  // of the available memory.
-  bool started_backpedal_ = false;
-
   // Whether the allocator will coalesce adjacent sub allocator provided
   // AllocationRegions. This may be disabled if discrete sub allocator
   // regions can't be treated as contiguous (e.g. if the allocation refers to
diff --git a/third_party/tsl/workspace2.bzl b/third_party/tsl/workspace2.bzl
index 5bfb6fd03..78e69ea51 100644
--- a/third_party/tsl/workspace2.bzl
+++ b/third_party/tsl/workspace2.bzl
@@ -370,6 +370,7 @@ def _tf_repositories():
         patch_file = [
             "//third_party/grpc:generate_cc_env_fix.patch",
             "//third_party/grpc:register_go_toolchain.patch",
+            "//third_party/grpc:c_ares.patch",
         ],
         system_link_files = {
             "//third_party/systemlibs:BUILD": "bazel/BUILD",
diff --git a/xla/pjrt/c/pjrt_c_api_gpu_internal.cc b/xla/pjrt/c/pjrt_c_api_gpu_internal.cc
index fa10334d5..5820c94ae 100644
--- a/xla/pjrt/c/pjrt_c_api_gpu_internal.cc
+++ b/xla/pjrt/c/pjrt_c_api_gpu_internal.cc
@@ -45,7 +45,7 @@ limitations under the License.
 namespace pjrt {
 namespace gpu_plugin {
 
-#define PJRT_GPU_PLUGIN_PLATFORM_NAME "CUDA"
+#define PJRT_GPU_PLUGIN_PLATFORM_NAME "SYCL"
 
 PJRT_Error* PJRT_Client_Create(PJRT_Client_Create_Args* args) {
   PJRT_RETURN_IF_ERROR(ActualStructSizeIsGreaterOrEqual(
diff --git a/xla/pjrt/event_pool.cc b/xla/pjrt/event_pool.cc
index 1db130633..d1accf7fa 100644
--- a/xla/pjrt/event_pool.cc
+++ b/xla/pjrt/event_pool.cc
@@ -53,8 +53,11 @@ StatusOr<EventPool::Handle> EventPool::AllocateEvent(
 }
 
 void EventPool::ThenRecordEvent(se::Stream* stream, EventPool::Handle& handle) {
-  absl::MutexLock lock(&mu_);
+  // We should not lock event pool mutex before submitting a sycl barrier to a
+  // stream, otherwise it may lead to a dead lock if there is a host task
+  // requiring this mutex in the same stream.
   stream->ThenRecordEvent(handle.event_.get());
+  absl::MutexLock lock(&mu_);
   handle.sequence_number_ = next_sequence_number_++;
 }
 
diff --git a/xla/pjrt/gpu/BUILD b/xla/pjrt/gpu/BUILD
index c98d792dd..8e146237e 100644
--- a/xla/pjrt/gpu/BUILD
+++ b/xla/pjrt/gpu/BUILD
@@ -40,6 +40,7 @@ cc_library(
     defines = if_cuda(["GOOGLE_CUDA=1"]) + if_rocm(["TENSORFLOW_USE_ROCM=1"]),
     visibility = ["//xla/pjrt:friends"],
     deps = [
+        "@intel_extension_for_openxla//xla/stream_executor/sycl:hw_info",
         ":gpu_helpers",
         ":gpu_metrics",
         ":gpu_topology",
diff --git a/xla/pjrt/gpu/gpu_helpers.cc b/xla/pjrt/gpu/gpu_helpers.cc
index d77ebf6bd..000ad13fb 100644
--- a/xla/pjrt/gpu/gpu_helpers.cc
+++ b/xla/pjrt/gpu/gpu_helpers.cc
@@ -37,11 +37,12 @@ namespace xla {
 StatusOr<LocalClient*> GetGpuXlaClient(
     const std::optional<std::string>& platform_name,
     const std::optional<std::set<int>>& allowed_devices) {
+  // SYCL: hardcode to xpu
   TF_ASSIGN_OR_RETURN(
       se::Platform * platform,
-      PlatformUtil::GetPlatform(platform_name ? *platform_name : "gpu"));
+      PlatformUtil::GetPlatform(platform_name ? *platform_name : "SYCL"));
   if (platform->VisibleDeviceCount() <= 0) {
-    return FailedPrecondition("No visible GPU devices.");
+    return FailedPrecondition("No visible XPU devices.");
   }
   LocalClientOptions options;
   options.set_platform(platform);
@@ -115,7 +116,8 @@ StatusOr<std::unique_ptr<tsl::BFCAllocator>> CreateBFCAllocator(
   opts.allow_growth = !preallocate;
   return std::make_unique<tsl::BFCAllocator>(
       std::move(sub_allocator), allocator_memory,
-      absl::StrCat("GPU_", device_ordinal, "_bfc"), opts);
+      // SYCL: hardcode to xpu
+      absl::StrCat("XPU_", device_ordinal, "_bfc"), opts);
 }
 
 // Builds a BFCAllocator for all local GPUs that uses collective memory.
@@ -171,7 +173,8 @@ std::unique_ptr<tsl::BFCAllocator> GetGpuHostAllocator(
   opts.allow_growth = true;
   return std::make_unique<tsl::BFCAllocator>(std::move(sub_allocator),
                                              kGpuHostMemoryLimitBytes,
-                                             /*name=*/"xla_gpu_host_bfc", opts);
+                                             // SYCL: hardcode to xpu
+                                             /*name=*/"xla_xpu_host_bfc", opts);
 }
 
 }  // namespace xla
diff --git a/xla/pjrt/gpu/se_gpu_pjrt_client.cc b/xla/pjrt/gpu/se_gpu_pjrt_client.cc
index 20ecb49c0..aadff0e29 100644
--- a/xla/pjrt/gpu/se_gpu_pjrt_client.cc
+++ b/xla/pjrt/gpu/se_gpu_pjrt_client.cc
@@ -81,6 +81,8 @@ limitations under the License.
 #include "tsl/platform/threadpool.h"
 #include "tsl/profiler/lib/connected_traceme.h"
 #include "tsl/profiler/lib/traceme.h"
+#include "xla/stream_executor/sycl/hw_info.h"
+#include "tsl/util/env_var.h"
 
 #if defined(GOOGLE_CUDA) || defined(TENSORFLOW_USE_ROCM)
 #include "xla/pjrt/compile_options.pb.h"
@@ -429,7 +431,7 @@ absl::string_view StreamExecutorGpuClient::platform_version() const {
 #elif GOOGLE_CUDA && defined(CUDART_VERSION)  // cuda
   return "cuda " STRINGIFY(CUDART_VERSION);
 #else
-  return "<unknown>";
+  return "sycl";
 #endif  // TENSORFLOW_USE_ROCM && defined(TF_ROCM_VERSION)
 }
 
@@ -951,6 +953,8 @@ StatusOr<std::unique_ptr<PjRtClient>> GetStreamExecutorGpuClient(
     const GpuClientOptions& options) {
 #if TENSORFLOW_USE_ROCM
   auto pjrt_platform_name = xla::RocmName();
+#elif TENSORFLOW_USE_SYCL
+  auto pjrt_platform_name = xla::XpuName();
 #else   // TENSORFLOW_USE_ROCM
   auto pjrt_platform_name = xla::CudaName();
 #endif  // TENSORFLOW_USE_ROCM
diff --git a/xla/pjrt/pjrt_compiler.h b/xla/pjrt/pjrt_compiler.h
index f94769093..e902a059e 100644
--- a/xla/pjrt/pjrt_compiler.h
+++ b/xla/pjrt/pjrt_compiler.h
@@ -48,6 +48,10 @@ inline const char* TpuName() {
   static constexpr char kTpuName[] = "tpu";
   return kTpuName;
 }
+inline const char* XpuName() {
+  static constexpr char kXpuName[] = "xpu";
+  return kXpuName;
+}
 inline PjRtPlatformId CpuId() {
   static const PjRtPlatformId kCpuId = tsl::Fingerprint64(CpuName());
   return kCpuId;
@@ -64,6 +68,10 @@ inline PjRtPlatformId TpuId() {
   static const PjRtPlatformId kTpuId = tsl::Fingerprint64(TpuName());
   return kTpuId;
 }
+inline PjRtPlatformId XpuId() {
+  static const PjRtPlatformId kXpuId = tsl::Fingerprint64(XpuName());
+  return kXpuId;
+}
 
 class PjRtCompiler;
 class PjRtClient;
diff --git a/xla/service/BUILD b/xla/service/BUILD
index b69e6a1e7..447a7dfff 100644
--- a/xla/service/BUILD
+++ b/xla/service/BUILD
@@ -20,6 +20,10 @@ load(
     "if_rocm",
     "if_rocm_is_configured",
 )
+load(
+    "@local_config_sycl//sycl:build_defs.bzl",
+    "if_sycl_is_configured",
+)
 load("@tsl//tsl:tsl.bzl", "if_google", "if_libtpu")
 load("@tsl//tsl:tsl.default.bzl", "filegroup", "get_compatible_with_portable", "internal_hlo_deps")
 load(
@@ -1338,6 +1342,9 @@ cc_library(
     ]) + if_rocm_is_configured([
         "//xla/service/gpu:amdgpu_compiler",
         "//xla/stream_executor/rocm:stream_executor_rocm",
+    ]) + if_sycl_is_configured([
+        "//xla/service/gpu:spir_compiler",
+        "@intel_extension_for_openxla//xla/stream_executor/sycl:stream_executor_sycl",
     ]),
 )
 
@@ -3824,6 +3831,7 @@ cc_library(
         "//xla/stream_executor/cuda:cuda_platform_id",
         "//xla/stream_executor/host:host_platform_id",
         "//xla/stream_executor/rocm:rocm_platform_id",
+        "@intel_extension_for_openxla//xla/stream_executor/sycl:sycl_platform_id",
         "@com_google_absl//absl/container:flat_hash_map",
         "@com_google_absl//absl/memory",
         "@com_google_absl//absl/strings",
diff --git a/xla/service/algebraic_simplifier.h b/xla/service/algebraic_simplifier.h
index 6152b8c05..bbd739d40 100644
--- a/xla/service/algebraic_simplifier.h
+++ b/xla/service/algebraic_simplifier.h
@@ -27,6 +27,7 @@ limitations under the License.
 #include <vector>
 
 #include "absl/container/inlined_vector.h"
+#include "tsl/util/env_var.h"
 #include "xla/hlo/ir/dfs_hlo_visitor_with_default.h"
 #include "xla/hlo/ir/hlo_instruction.h"
 #include "xla/hlo/ir/hlo_module.h"
@@ -439,7 +440,9 @@ class AlgebraicSimplifierVisitor : public DfsHloRewriteVisitor {
   virtual bool IsValidLayout(const Shape& shape) { return true; }
   // Allow backend targets to determine whether a layout is inefficient.
   virtual bool ShouldStrengthReduceDotToReduce(const HloInstruction* hlo) {
-    return true;
+    bool llm_flag = false;
+    tsl::ReadBoolFromEnvVar("LLM", false, &llm_flag);
+    return !llm_flag;
   }
 
  protected:
diff --git a/xla/service/computation_placer.cc b/xla/service/computation_placer.cc
index d0274f563..bc77594ad 100644
--- a/xla/service/computation_placer.cc
+++ b/xla/service/computation_placer.cc
@@ -31,6 +31,7 @@ limitations under the License.
 #include "xla/stream_executor/cuda/cuda_platform_id.h"
 #include "xla/stream_executor/host/host_platform_id.h"
 #include "xla/stream_executor/rocm/rocm_platform_id.h"
+#include "xla/stream_executor/sycl/sycl_platform_id.h"
 #include "xla/types.h"
 #include "xla/util.h"
 #include "tsl/platform/errors.h"
@@ -216,6 +217,8 @@ static bool InitModule() {
       stream_executor::cuda::kCudaPlatformId, &CreateComputationPlacer);
   xla::ComputationPlacer::RegisterComputationPlacer(
       stream_executor::rocm::kROCmPlatformId, &CreateComputationPlacer);
+  xla::ComputationPlacer::RegisterComputationPlacer(
+      stream_executor::sycl::kSyclPlatformId, &CreateComputationPlacer);
   return true;
 }
 static bool module_initialized = InitModule();
diff --git a/xla/service/dump.cc b/xla/service/dump.cc
index 493980cd1..7e4d4bf40 100644
--- a/xla/service/dump.cc
+++ b/xla/service/dump.cc
@@ -624,6 +624,8 @@ void DumpToFileInDirOrStdout(const HloModule& module, string_view file_prefix,
   if (opts.dumping_to_stdout()) return op->dump();
 
   mlir::OpPrintingFlags print_flags = mlir::OpPrintingFlags();
+  // Avoid printing large constant weight.
+  print_flags.elideLargeElementsAttrs(8);
   // Enable debug info so that it is easier to see the corresponding HLO node.
   if (file_prefix == "lmhlo") {
     print_flags.enableDebugInfo(/*enable=*/true,
diff --git a/xla/service/float8_fnuz_ir_emitter.cc b/xla/service/float8_fnuz_ir_emitter.cc
index 649729d87..873b1ee51 100644
--- a/xla/service/float8_fnuz_ir_emitter.cc
+++ b/xla/service/float8_fnuz_ir_emitter.cc
@@ -19,6 +19,7 @@ limitations under the License.
 
 #include "llvm/IR/Constants.h"
 #include "llvm/IR/Intrinsics.h"
+#include "llvm/TargetParser/Triple.h"
 #include "xla/primitive_util.h"
 #include "xla/status_macros.h"
 #include "xla/util.h"
@@ -610,13 +611,17 @@ StatusOr<llvm::Value*> EmitF8fnuzToFloating(PrimitiveType input_type,
         llvm::Constant* result_lut_array =
             llvm::ConstantArray::get(result_lut_array_type, result_lut);
 
+        int addrspace = llvm::Triple(module->getTargetTriple()).isSPIR() ? 1 : 0;
         return new llvm::GlobalVariable(
             /*M=*/*module,
             /*Ty=*/result_lut_array_type,
             /*isConstant=*/true,
             /*Linkage=*/llvm::GlobalValue::PrivateLinkage,
             /*Initializer=*/result_lut_array,
-            /*Name=*/lut_name);
+            /*Name=*/lut_name,
+            /*InsertBefore*/nullptr,
+            /*TLMode=*/llvm::GlobalValue::NotThreadLocal,
+            /*AddressSpace=*/addrspace);
       });
 
   // Check for NaN, since it's a special case.
diff --git a/xla/service/gpu/BUILD b/xla/service/gpu/BUILD
index 3812f8ece..c70787c09 100644
--- a/xla/service/gpu/BUILD
+++ b/xla/service/gpu/BUILD
@@ -36,6 +36,7 @@ load(
     "@tsl//tsl/platform/default:cuda_build_defs.bzl",
     "if_cuda_is_configured",
 )
+load("@local_config_sycl//sycl:build_defs.bzl", "if_sycl_is_configured")
 
 package(
     # copybara:uncomment default_applicable_licenses = ["//tensorflow:license"],
@@ -319,7 +320,8 @@ cc_library(
         ":launch_dimensions",
         ":matmul_utils",
         ":nccl_api",
-        ":nccl_collective_thunks",
+        # ":nccl_collective_thunks",
+        "@intel_extension_for_openxla//xla/service/gpu:ccl_collective_thunks",
         ":parallel_loop_emitter",
         ":reduction_utils",
         ":target_util",
@@ -352,9 +354,9 @@ cc_library(
         "//xla/service/gpu/fusions:tiling_util",
         "//xla/service/gpu/kernels:custom_kernel",
         "//xla/service/gpu/kernels:topk_custom_kernel",
-        "//xla/service/gpu/runtime3:command_buffer_cmd",
-        "//xla/service/gpu/runtime3:command_buffer_cmd_emitter",
-        "//xla/service/gpu/runtime3:command_buffer_thunk",
+        # "//xla/service/gpu/runtime3:command_buffer_cmd",
+        # "//xla/service/gpu/runtime3:command_buffer_cmd_emitter",
+        # "//xla/service/gpu/runtime3:command_buffer_thunk",
         "//xla/service/gpu/runtime3:conditional_thunk",
         "//xla/service/gpu/runtime3:convolution_thunk",
         "//xla/service/gpu/runtime3:copy_thunk",
@@ -364,8 +366,8 @@ cc_library(
         "//xla/service/gpu/runtime3:gemm_thunk",
         "//xla/service/gpu/runtime3:infeed_thunk",
         "//xla/service/gpu/runtime3:kernel_thunk",
-        "//xla/service/gpu/runtime3:nccl_all_gather_thunk",
-        "//xla/service/gpu/runtime3:nccl_all_reduce_thunk",
+        # "//xla/service/gpu/runtime3:nccl_all_gather_thunk",
+        # "//xla/service/gpu/runtime3:nccl_all_reduce_thunk",
         "//xla/service/gpu/runtime3:norm_thunk",
         "//xla/service/gpu/runtime3:outfeed_thunk",
         "//xla/service/gpu/runtime3:replica_id_thunk",
@@ -421,9 +423,9 @@ cc_library(
         "@tsl//tsl/platform:statusor",
         "@tsl//tsl/protobuf:dnn_proto_cc",
     ] + if_gpu_is_configured([
-        ":ir_emitter_triton",
+        # ":ir_emitter_triton",
         "//xla/service/gpu/runtime3:cholesky_thunk",
-        "//xla/service/gpu/runtime3:cub_sort_thunk",
+        # "//xla/service/gpu/runtime3:cub_sort_thunk",
         "//xla/service/gpu/runtime3:gpublas_lt_matmul_thunk",
         "//xla/service/gpu/runtime3:triangular_solve_thunk",
     ]),
@@ -925,55 +927,78 @@ cc_library(
 # :nccl_api target and all other targets should use this header to launch collective operations.
 # This allows to minimize the spreading of #ifdef all over the XLA code base.
 
-alias(
-    name = "nccl_api",
-    actual = if_nccl(":_nccl_api_impl", ":_nccl_api_stub"),
-)
+# alias(
+#     name = "nccl_api",
+#     actual = if_nccl(":_nccl_api_impl", ":_nccl_api_stub"),
+# )
+
+# cc_library(
+#     name = "_nccl_api_impl",
+#     srcs = if_cuda_is_configured(
+#         ["nccl_api.cc"],
+#         ["nccl_api_stub.cc"],
+#     ),
+#     hdrs = ["nccl_api.h"],
+#     compatible_with = get_compatible_with_portable(),
+#     defines = if_cuda_is_configured(["XLA_ENABLE_XCCL"]),  # TODO(ezhulenev): Remove!
+#     deps = [
+#         ":nccl_clique_key",
+#         "//xla:shape_util",
+#         "//xla:xla_data_proto_cc",
+#         "//xla/service:collective_ops_utils",
+#         "//xla/stream_executor",
+#         "@com_google_absl//absl/algorithm:container",
+#         "@com_google_absl//absl/hash",
+#         "@com_google_absl//absl/status",
+#         "@com_google_absl//absl/status:statusor",
+#         "@com_google_absl//absl/strings",
+#         "@com_google_absl//absl/strings:str_format",
+#         "@tsl//tsl/concurrency:ref_count",
+#         "@tsl//tsl/platform:logging",
+#         "@tsl//tsl/platform:statusor",
+#     ] + if_cuda_is_configured([
+#         "@local_config_nccl//:nccl",
+#         "//xla/stream_executor/cuda:cuda_driver",
+#         "//xla/stream_executor/cuda:cuda_executor",
+#     ]) + if_gpu_is_configured([
+#         "//xla/stream_executor/gpu:gpu_stream",
+#     ]),
+# )
+
+# cc_library(
+#     name = "_nccl_api_stub",
+#     srcs = ["nccl_api_stub.cc"],
+#     hdrs = ["nccl_api.h"],
+#     compatible_with = get_compatible_with_portable(),
+#     deps = [
+#         ":nccl_clique_key",
+#         "//xla:shape_util",
+#         "//xla:xla_data_proto_cc",
+#         "//xla/service:collective_ops_utils",
+#         "//xla/stream_executor",
+#         "@com_google_absl//absl/status",
+#         "@com_google_absl//absl/status:statusor",
+#         "@tsl//tsl/concurrency:ref_count",
+#         "@tsl//tsl/platform:logging",
+#     ],
+# )
 
 cc_library(
-    name = "_nccl_api_impl",
-    srcs = if_cuda_is_configured(
-        ["nccl_api.cc"],
-        ["nccl_api_stub.cc"],
-    ),
-    hdrs = ["nccl_api.h"],
+    name = "nccl_api",
+    srcs = ["ccl_api.cc"],
+    hdrs = [
+        "nccl_api.h",
+        "ccl_api.h",
+    ],
     compatible_with = get_compatible_with_portable(),
-    defines = if_cuda_is_configured(["XLA_ENABLE_XCCL"]),  # TODO(ezhulenev): Remove!
     deps = [
         ":nccl_clique_key",
         "//xla:shape_util",
         "//xla:xla_data_proto_cc",
         "//xla/service:collective_ops_utils",
         "//xla/stream_executor",
-        "@com_google_absl//absl/algorithm:container",
-        "@com_google_absl//absl/hash",
-        "@com_google_absl//absl/status",
-        "@com_google_absl//absl/status:statusor",
-        "@com_google_absl//absl/strings",
-        "@com_google_absl//absl/strings:str_format",
-        "@tsl//tsl/concurrency:ref_count",
-        "@tsl//tsl/platform:logging",
-        "@tsl//tsl/platform:statusor",
-    ] + if_cuda_is_configured([
-        "@local_config_nccl//:nccl",
-        "//xla/stream_executor/cuda:cuda_driver",
-        "//xla/stream_executor/cuda:cuda_executor",
-    ]) + if_gpu_is_configured([
         "//xla/stream_executor/gpu:gpu_stream",
-    ]),
-)
-
-cc_library(
-    name = "_nccl_api_stub",
-    srcs = ["nccl_api_stub.cc"],
-    hdrs = ["nccl_api.h"],
-    compatible_with = get_compatible_with_portable(),
-    deps = [
-        ":nccl_clique_key",
-        "//xla:shape_util",
-        "//xla:xla_data_proto_cc",
-        "//xla/service:collective_ops_utils",
-        "//xla/stream_executor",
+        "@intel_extension_for_openxla//xla/service/gpu:ccl_ops",
         "@com_google_absl//absl/status",
         "@com_google_absl//absl/status:statusor",
         "@tsl//tsl/concurrency:ref_count",
@@ -987,6 +1012,7 @@ cc_library(
     hdrs = ["nccl_clique_key.h"],
     compatible_with = get_compatible_with_portable(),
     deps = [
+        "//xla:executable_run_options",
         "//xla/service:global_device_id",
         "@com_google_absl//absl/algorithm:container",
         "@com_google_absl//absl/status",
@@ -1232,10 +1258,10 @@ cc_library(
         "//xla:status_macros",
         "//xla:util",
         "//xla/hlo/ir:hlo",
-        "//xla/mlir/runtime/ir:rt",
-        "//xla/mlir/runtime/transforms:compilation_pipeline_gpu",
-        "//xla/mlir/runtime/transforms:type_converter",
-        "//xla/runtime:executable",
+        # "//xla/mlir/runtime/ir:rt",
+        # "//xla/mlir/runtime/transforms:compilation_pipeline_gpu",
+        # "//xla/mlir/runtime/transforms:type_converter",
+        # "//xla/runtime:executable",
         "//xla/service:buffer_assignment",
         "//xla/service:executable",
         "//xla/service:hlo_execution_profile",
@@ -1244,7 +1270,8 @@ cc_library(
         "//xla/service:shaped_buffer",
         "//xla/service:stream_pool",
         "//xla/service:xla_debug_info_manager",
-        "//xla/service/gpu/runtime:executable",
+        # "//xla/service/gpu/runtime:executable",
+        # "//xla/service/gpu/runtime:support",
         "//xla/service/gpu/runtime:tracing",
         "//xla/stream_executor",
         "//xla/stream_executor:device_description",
@@ -1279,6 +1306,8 @@ cc_library(
         "@tsl//tsl/platform:statusor",
         "@tsl//tsl/profiler/lib:scoped_annotation",
         "@tsl//tsl/profiler/lib:traceme",
+        "@intel_extension_for_openxla//xla/service/gpu:xetla_gpu_fused_mha_runner",
+        "@intel_extension_for_openxla//xla/service/gpu:onednn_gpu_conv_runner",
     ] + if_gpu_is_configured([
         ":make_batch_pointers",
     ]) + if_cuda_is_configured([
@@ -2363,6 +2392,8 @@ cc_library(
         "//xla/stream_executor/rocm:rocblas_wrapper",
         "//xla/stream_executor/rocm:rocsolver_wrapper",
         "//xla/stream_executor/rocm:hipsolver_wrapper",
+    ]) + if_sycl_is_configured([
+        "@local_config_sycl//sycl:sycl_headers",
     ]),
 )
 
@@ -3037,6 +3068,7 @@ cc_library(
         "//xla/stream_executor/cuda:cuda_platform_id",
         "//xla/stream_executor/host:host_platform_id",
         "//xla/stream_executor/rocm:rocm_platform_id",
+        "@intel_extension_for_openxla//xla/stream_executor/sycl:sycl_platform_id",
         "@com_google_absl//absl/cleanup",
         "@com_google_absl//absl/status",
         "@llvm-project//llvm:Core",
@@ -3123,7 +3155,7 @@ cc_library(
         "//xla/service:hlo_ordering",
         "//xla/service:hlo_proto_cc",
         "//xla/service:logical_buffer",
-        "//xla/service/gpu/runtime:executable",
+        # "//xla/service/gpu/runtime:executable",
         "//xla/service/gpu/runtime3:conditional_thunk",
         "//xla/service/gpu/runtime3:sequential_thunk",
         "//xla/service/gpu/runtime3:while_thunk",
@@ -3344,6 +3376,7 @@ cc_library(
         "TENSORFLOW_USE_ROCM=1",
     ]),
     deps = if_gpu_is_configured([
+        "@intel_extension_for_openxla//xla/service/gpu:dot_expand_dims",
         ":address_computation_fusion_rewriter",
         ":alias_passthrough_params",
         ":all_reduce_blueconnect",
@@ -3422,9 +3455,9 @@ cc_library(
         "//xla:xla_proto_cc",
         "//xla/hlo/ir:hlo",
         "//xla/hlo/transforms:hlo_constant_splitter",
-        "//xla/mlir/backends/gpu/transforms:passes",
-        "//xla/mlir/runtime/transforms:compilation_pipeline_gpu",
-        "//xla/runtime:jit_executable",
+        # "//xla/mlir/backends/gpu/transforms:passes",
+        # "//xla/mlir/runtime/transforms:compilation_pipeline_gpu",
+        # "//xla/runtime:jit_executable",
         "//xla/service:algebraic_simplifier",
         "//xla/service:all_gather_broadcast_reorder",
         "//xla/service:all_gather_combiner",
@@ -3553,7 +3586,7 @@ cc_library(
         "//xla/service:hlo_ordering",
         "//xla/service:layout_assignment",
         "//xla/service:logical_buffer",
-        "//xla/service/gpu/runtime:executable",
+        # "//xla/service/gpu/runtime:executable",
         "//xla/stream_executor/rocm:rocm_platform_id",
         "@tsl//tsl/platform:numbers",
     ]) + xla_export_hlo_deps() + [
@@ -3759,6 +3792,61 @@ xla_cc_test(
     ],
 )
 
+cc_library(
+    name = "spir_compiler_impl",
+    srcs = [
+        "spir_compiler.cc",
+    ],
+    hdrs = [
+        "spir_compiler.h",
+    ],
+    deps = [
+        "@intel_extension_for_openxla//xla/service/gpu:gemm_impl_picker",
+        "@intel_extension_for_openxla//xla/service/gpu:redundant_convert_mover",
+        "@intel_extension_for_openxla//xla/stream_executor/sycl:hw_info",
+        "@intel_extension_for_openxla//xla/stream_executor/sycl:sycl_platform_id",
+        "@com_google_absl//absl/base",
+        "@com_google_absl//absl/container:node_hash_map",
+        "@com_google_absl//absl/types:optional",
+        "@llvm-project//llvm:IRReader",
+        "@llvm-project//llvm:Support",
+        "//xla/service:dot_dimension_merger",
+        "//xla/service:float_normalization",
+        "//xla/service:float_support",
+        "//xla/service:hlo_constant_folding",
+        "//xla/service:hlo_cse",
+        "//xla/service:hlo_dce",
+        "//xla/service:hlo_pass",
+        "//xla/service:hlo_pass_pipeline",
+        "//xla/service:hlo_proto_cc",
+        "//xla/service:hlo_verifier",
+        "//xla/service:llvm_compiler",
+        "//xla/service:reshape_mover",
+        "//xla/service:tuple_simplifier",
+        "//xla/service/gpu:cudnn_fused_conv_rewriter",
+        "//xla/service/gpu:cudnn_fused_mha_rewriter",
+        "//xla/service/gpu:cusolver_rewriter",
+        "//xla/service/gpu:gpu_compiler",
+        "//xla/service/gpu:gpu_conv_padding_legalization",
+        "//xla/service/gpu:target_constants",
+        "//xla/service/gpu:triangular_solve_rewriter",
+        "//xla/service/gpu/llvm_gpu_backend",
+    ],
+)
+
+cc_library(
+    name = "spir_compiler",
+    srcs = [
+        "spir_compiler_registration.cc",
+    ],
+    deps = [
+        ":spir_compiler_impl",
+        "@intel_extension_for_openxla//xla/stream_executor/sycl:sycl_platform_id",
+        "@tsl//tsl/platform:path",
+    ],
+    alwayslink = True,  # Contains compiler registration
+)
+
 xla_cc_test(
     name = "gpu_aot_compilation_test",
     srcs = if_gpu_is_configured([
diff --git a/xla/service/gpu/buffer_sharing.cc b/xla/service/gpu/buffer_sharing.cc
index abfcefdd3..4041bdd1c 100644
--- a/xla/service/gpu/buffer_sharing.cc
+++ b/xla/service/gpu/buffer_sharing.cc
@@ -215,6 +215,15 @@ std::optional<bool> CanShareBufferHint(const HloInstruction* user,
                 ->gemm_backend_config();
         return (config.beta() != 0.) && user->operand(2) == operand;
       }
+      // SYCL: inplace sum for onednn conv with side input.
+      if (user->custom_call_target() ==
+          kCudnnConvBiasActivationForwardCallTarget) {
+        CudnnConvBackendConfig config =
+            std::move(user->backend_config<GpuBackendConfig>())
+                ->cudnn_conv_backend_config();
+        return (config.side_input_scale() != 0.) &&
+               (user->operand(user->operand_count() - 1) == operand);
+      }
       // The operand of cholesky can be shared with the first output.
       if (user->custom_call_target() == kCusolverCholeskyCallTarget) {
         return user_index.size() == 1 && user_index[0] == 0;
diff --git a/xla/service/gpu/ccl_api.cc b/xla/service/gpu/ccl_api.cc
new file mode 100644
index 000000000..f978e9b74
--- /dev/null
+++ b/xla/service/gpu/ccl_api.cc
@@ -0,0 +1,281 @@
+/* Copyright (c) 2024 Intel Corporation
+
+Copyright 2019 The TensorFlow Authors. All Rights Reserved.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#include "xla/service/gpu/ccl_api.h"
+
+#include <cstddef>
+#include <cstdint>
+
+#include "absl/status/status.h"
+#include "absl/status/statusor.h"
+#include "tsl/concurrency/ref_count.h"
+#include "xla/service/collective_ops_utils.h"
+#include "xla/service/gpu/ccl_ops.h"
+#include "xla/service/gpu/nccl_api.h"
+#include "xla/service/gpu/nccl_clique_key.h"
+#include "xla/stream_executor/device_memory.h"
+#include "xla/stream_executor/gpu/gpu_stream.h"
+#include "xla/stream_executor/stream.h"
+
+namespace xla::gpu {
+//==-----------------------------------------------------------------------===//
+// NcclApi::PersistentPlanAllocator
+//==-----------------------------------------------------------------------===//
+
+using PersistentPlanAllocator = NcclApi::PersistentPlanAllocator;
+using ScopedPersistentPlanAllocator = NcclApi::ScopedPersistentPlanAllocator;
+
+PersistentPlanAllocator::PersistentPlanAllocator(int64_t,
+                                                 se::DeviceMemoryAllocator*,
+                                                 se::Stream*) {
+  // Suppress clang unused private field warnings.
+  (void)device_ordinal_;
+  (void)allocator_;
+  (void)stream_;
+}
+
+PersistentPlanAllocator::~PersistentPlanAllocator() = default;
+
+absl::StatusOr<se::DeviceMemoryBase>
+PersistentPlanAllocator::AllocateAndInitialize(void*, size_t) {
+  return absl::UnimplementedError("XLA compiled without NCCL support");
+}
+
+absl::Status PersistentPlanAllocator::Deallocate(se::DeviceMemoryBase mem) {
+  return absl::UnimplementedError("XLA compiled without NCCL support");
+}
+
+ScopedPersistentPlanAllocator::ScopedPersistentPlanAllocator(
+    NcclCommHandle, tsl::RCReference<PersistentPlanAllocator>) {
+  // Suppress clang unused private field warnings.
+  (void)comm_;
+  (void)recover_;
+  (void)allocator_;
+}
+
+ScopedPersistentPlanAllocator::~ScopedPersistentPlanAllocator() = default;
+
+//===----------------------------------------------------------------------===//
+// CclApi
+//===----------------------------------------------------------------------===//
+
+static absl::Status UnimplementedError(std::string mes = "") {
+  return absl::UnimplementedError("XLA compiled without CCL support: " + mes);
+}
+
+CclApi::CclApi() {}
+
+absl::StatusOr<NcclCliqueId> CclApi::GetUniqueId() { return NcclCliqueId(); }
+
+absl::StatusOr<NcclCliqueId> CclApi::GetId(const NcclCliqueKey& key,
+                                           const RunId& id) {
+  std::string new_id =
+      id.ToString() + "=" + GlobalDeviceIdsToString(key.devices());
+  TF_RET_CHECK(new_id.size() < NcclCliqueId::kSize)
+      << "Run ID length must < kSize(" << NcclCliqueId::kSize << ").";
+  new_id.resize(NcclCliqueId::kSize);
+
+  return NcclCliqueId().FromString(new_id);
+}
+
+absl::StatusOr<CclApi::OwnedNcclComm> CclApi::CommInitRank(
+    int32_t nranks, const NcclCliqueId& clique_id, int32_t rank) {
+  VLOG(1) << "Initialize NCCL communicator for rank #" << rank << " of "
+          << nranks << "; hash(id)=" << absl::HashOf(clique_id.data());
+
+  if (rank < 0 || rank >= nranks)
+    return absl::InvalidArgumentError(absl::StrFormat(
+        "Invalid rank %d, it must be in [0, %d) range", rank, nranks));
+
+  NcclCommHandle comm = reinterpret_cast<NcclCommHandle>(
+      new ccl::communicator(nranks, rank, clique_id.ToString()));
+
+  return CclApi::OwnedNcclComm(comm, NcclCommDeleter{this});
+}
+
+absl::Status CclApi::CommAbort(NcclCommHandle) {
+  // Don't need now
+  return UnimplementedError("CommAbort");
+}
+
+absl::Status CclApi::CommFinalize(NcclCommHandle) {
+  // Don't need now
+  return UnimplementedError("CommFinalize");
+}
+
+absl::Status CclApi::CommDestroy(NcclCommHandle comm) {
+  delete reinterpret_cast<ncclComm_t>(comm);
+  return absl::OkStatus();
+}
+
+absl::StatusOr<int32_t> CclApi::CommCount(NcclCommHandle) {
+  // Don't need now
+  return UnimplementedError("CommCount");
+}
+
+absl::Status CclApi::CommGetAsyncError(NcclCommHandle comm) {
+  return absl::OkStatus();
+}
+
+absl::Status CclApi::GroupStart() {
+  // Don't need now
+  return UnimplementedError("GroupStart");
+}
+
+absl::Status CclApi::GroupEnd() {
+  // Don't need now
+  return UnimplementedError("GroupEnd");
+}
+
+absl::Status CclApi::AllReduce(se::DeviceMemoryBase send_buffer,
+                               se::DeviceMemoryBase recv_buffer,
+                               PrimitiveType dtype, size_t count,
+                               ReductionKind reduction_kind,
+                               NcclCommHandle comm, se::Stream* stream) {
+  se::gpu::GpuStreamHandle gpu_stream = se::gpu::AsGpuStreamValue(stream);
+
+  const void* send_buffer_ = send_buffer.opaque();
+  void* recv_buffer_ = recv_buffer.opaque();
+  int element_count = count * (primitive_util::IsComplexType(dtype) ? 2 : 1);
+
+  auto comm_ = reinterpret_cast<ncclComm_t>(comm);
+
+  sycl_allreduce(send_buffer_, recv_buffer_, element_count, dtype,
+                 reduction_kind, gpu_stream, comm_);
+  return absl::OkStatus();
+}
+
+absl::Status CclApi::ReduceScatter(se::DeviceMemoryBase send_buffer,
+                                   se::DeviceMemoryBase recv_buffer,
+                                   PrimitiveType dtype, size_t count,
+                                   ReductionKind reduction_kind,
+                                   NcclCommHandle comm, se::Stream* stream) {
+  se::gpu::GpuStreamHandle gpu_stream = se::gpu::AsGpuStreamValue(stream);
+
+  const void* send_buffer_ = send_buffer.opaque();
+  void* recv_buffer_ = recv_buffer.opaque();
+  int element_count = count * (primitive_util::IsComplexType(dtype) ? 2 : 1);
+
+  auto comm_ = reinterpret_cast<ncclComm_t>(comm);
+
+  int num_participants = comm_->nranks;
+  TF_RET_CHECK(element_count % num_participants == 0)
+      << "Source buffer was not an exact multiple of the number of "
+         "participants.";
+  int64_t recv_count = element_count / num_participants;
+  VLOG(3) << absl::StreamFormat(
+      "Calling ncclReduceScatter(send_buffer=%p, recv_buffer=%p, "
+      "recvcount=%d, "
+      "comm=%p, stream=%p)",
+      send_buffer_, recv_buffer_, recv_count, static_cast<const void*>(comm_),
+      gpu_stream);
+
+  sycl_reduce_scatter(send_buffer_, recv_buffer_, recv_count, dtype,
+                      reduction_kind, gpu_stream, comm_);
+  return absl::OkStatus();
+}
+
+absl::Status CclApi::AllGather(se::DeviceMemoryBase send_buffer,
+                               se::DeviceMemoryBase recv_buffer,
+                               PrimitiveType dtype, size_t count,
+                               NcclCommHandle comm, se::Stream* stream) {
+  se::gpu::GpuStreamHandle gpu_stream = se::gpu::AsGpuStreamValue(stream);
+
+  const void* send_buffer_ = send_buffer.opaque();
+  void* recv_buffer_ = recv_buffer.opaque();
+  int element_count = count * (primitive_util::IsComplexType(dtype) ? 2 : 1);
+
+  auto comm_ = reinterpret_cast<ncclComm_t>(comm);
+
+  sycl_allgather(send_buffer_, recv_buffer_, element_count, dtype, gpu_stream,
+                 comm_);
+  return absl::OkStatus();
+}
+
+absl::Status CclApi::AllToAll(bool has_split_dimension,
+                              std::vector<const void*>& send_buffers,
+                              std::vector<void*>& recv_buffers,
+                              int element_count, PrimitiveType element_type,
+                              NcclCommHandle comm, se::Stream* stream) {
+  auto comm_ = reinterpret_cast<ncclComm_t>(comm);
+
+  se::gpu::GpuStreamHandle gpu_stream = se::gpu::AsGpuStreamValue(stream);
+
+  if (has_split_dimension) {
+    sycl_alltoall_split(send_buffers, recv_buffers, element_count, element_type,
+                        gpu_stream, comm_);
+  } else {
+    sycl_alltoall(send_buffers, recv_buffers, element_count, element_type,
+                  gpu_stream, comm_);
+  }
+
+  return absl::OkStatus();
+}
+
+absl::Status CclApi::CollectivePermute(se::DeviceMemoryBase src_addr,
+                                       se::DeviceMemoryBase dest_addr,
+                                       int element_count,
+                                       PrimitiveType element_type,
+                                       const std::optional<int64_t> source_id,
+                                       const std::optional<int64_t> target_id,
+                                       NcclCommHandle comm,
+                                       se::Stream* stream) {
+  auto comm_ = reinterpret_cast<ncclComm_t>(comm);
+
+  se::gpu::GpuStreamHandle gpu_stream = se::gpu::AsGpuStreamValue(stream);
+
+  sycl_collective_permute(src_addr.opaque(), dest_addr.opaque(), element_count,
+                          element_type, source_id, target_id, gpu_stream,
+                          comm_);
+
+  return absl::OkStatus();
+}
+
+absl::Status CclApi::Send(se::DeviceMemoryBase, PrimitiveType, size_t, int32_t,
+                          NcclCommHandle, se::Stream*) {
+  // Don't need now
+  return UnimplementedError("Send");
+}
+
+absl::Status CclApi::Recv(se::DeviceMemoryBase, PrimitiveType, size_t, int32_t,
+                          NcclCommHandle, se::Stream*) {
+  // Don't need now
+  return UnimplementedError("Recv");
+}
+
+absl::StatusOr<CclApi::NcclRegisteredBufferHandle> CclApi::RegisterBuffer(
+    NcclCommHandle, se::DeviceMemoryBase) {
+  // Don't need now
+  return UnimplementedError("RegisterBuffer");
+}
+
+absl::StatusOr<CclApi::NcclRegisteredBufferHandle> CclApi::DeregisterBuffer(
+    NcclCommHandle, CclApi::NcclRegisteredBufferHandle) {
+  // Don't need now
+  return UnimplementedError("DeregisterBuffer");
+}
+
+ncclComm_t CastCCLComm(CclApi::NcclCommHandle comm) {
+  return reinterpret_cast<ncclComm_t>(comm);
+}
+
+NcclApi* NcclApi::Default() {
+  static auto* ccl_api = new CclApi();
+  return ccl_api;
+}
+
+}  // namespace xla::gpu
diff --git a/xla/service/gpu/ccl_api.h b/xla/service/gpu/ccl_api.h
new file mode 100644
index 000000000..ed898746f
--- /dev/null
+++ b/xla/service/gpu/ccl_api.h
@@ -0,0 +1,106 @@
+/* Copyright (c) 2024 Intel Corporation
+
+Copyright 2019 The TensorFlow Authors. All Rights Reserved.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+#ifndef XLA_SERVICE_GPU_CCL_API_H_
+#define XLA_SERVICE_GPU_CCL_API_H_
+
+#include <cstddef>
+#include <cstdint>
+
+#include "absl/status/status.h"
+#include "absl/status/statusor.h"
+#include "tsl/concurrency/ref_count.h"
+#include "xla/service/collective_ops_utils.h"
+#include "xla/service/gpu/ccl_ops.h"
+#include "xla/service/gpu/nccl_api.h"
+#include "xla/service/gpu/nccl_clique_key.h"
+#include "xla/stream_executor/device_memory.h"
+#include "xla/stream_executor/gpu/gpu_stream.h"
+#include "xla/stream_executor/stream.h"
+
+namespace xla::gpu {
+//===----------------------------------------------------------------------===//
+// CclApi
+//===----------------------------------------------------------------------===//
+
+class CclApi final : public NcclApi {
+ public:
+  CclApi();
+
+  absl::StatusOr<NcclCliqueId> GetUniqueId() final;
+
+  absl::StatusOr<NcclCliqueId> GetId(const NcclCliqueKey& key,
+                                     const RunId& id) final;
+
+  absl::StatusOr<OwnedNcclComm> CommInitRank(int32_t nranks,
+                                             const NcclCliqueId& clique_id,
+                                             int32_t rank) final;
+
+  absl::Status CommAbort(NcclCommHandle) final;
+  absl::Status CommFinalize(NcclCommHandle) final;
+  absl::Status CommDestroy(NcclCommHandle comm) final;
+  absl::StatusOr<int32_t> CommCount(NcclCommHandle) final;
+  absl::Status CommGetAsyncError(NcclCommHandle comm) final;
+
+  absl::Status GroupStart() final;
+
+  absl::Status GroupEnd() final;
+
+  absl::Status AllReduce(se::DeviceMemoryBase send_buffer,
+                         se::DeviceMemoryBase recv_buffer, PrimitiveType dtype,
+                         size_t count, ReductionKind reduction_kind,
+                         NcclCommHandle comm, se::Stream* stream) final;
+
+  absl::Status ReduceScatter(se::DeviceMemoryBase send_buffer,
+                             se::DeviceMemoryBase recv_buffer,
+                             PrimitiveType dtype, size_t count,
+                             ReductionKind reduction_kind, NcclCommHandle comm,
+                             se::Stream* stream) final;
+
+  absl::Status AllGather(se::DeviceMemoryBase send_buffer,
+                         se::DeviceMemoryBase recv_buffer, PrimitiveType dtype,
+                         size_t count, NcclCommHandle comm,
+                         se::Stream* stream) final;
+
+  absl::Status AllToAll(bool has_split_dimension,
+                        std::vector<const void*>& send_buffers,
+                        std::vector<void*>& recv_buffers, int element_count,
+                        PrimitiveType element_type, NcclCommHandle comm,
+                        se::Stream* stream);
+
+  absl::Status CollectivePermute(se::DeviceMemoryBase src_addr,
+                                 se::DeviceMemoryBase dest_addr,
+                                 int element_count, PrimitiveType element_type,
+                                 const std::optional<int64_t> source_id,
+                                 const std::optional<int64_t> target_id,
+                                 NcclCommHandle comm, se::Stream* stream);
+
+  absl::Status Send(se::DeviceMemoryBase, PrimitiveType, size_t, int32_t,
+                    NcclCommHandle, se::Stream*) final;
+  absl::Status Recv(se::DeviceMemoryBase, PrimitiveType, size_t, int32_t,
+                    NcclCommHandle, se::Stream*) final;
+
+  absl::StatusOr<NcclRegisteredBufferHandle> RegisterBuffer(
+      NcclCommHandle, se::DeviceMemoryBase) final;
+
+  absl::StatusOr<NcclRegisteredBufferHandle> DeregisterBuffer(
+      NcclCommHandle, NcclRegisteredBufferHandle) final;
+};
+
+ncclComm_t CastCCLComm(CclApi::NcclCommHandle);
+
+}  // namespace xla::gpu
+#endif  // XLA_SERVICE_GPU_CCL_API_H_
diff --git a/xla/service/gpu/compile_module_to_llvm_ir.cc b/xla/service/gpu/compile_module_to_llvm_ir.cc
index 80f74bb9f..82d4f1a13 100644
--- a/xla/service/gpu/compile_module_to_llvm_ir.cc
+++ b/xla/service/gpu/compile_module_to_llvm_ir.cc
@@ -69,7 +69,7 @@ limitations under the License.
 #include "xla/service/gpu/ir_emitter_context.h"
 #include "xla/service/gpu/ir_emitter_unnested.h"
 #include "xla/service/gpu/metrics.h"
-#include "xla/service/gpu/runtime/executable.h"
+// #include "xla/service/gpu/runtime/executable.h"
 #include "xla/service/gpu/runtime3/conditional_thunk.h"
 #include "xla/service/gpu/runtime3/sequential_thunk.h"
 #include "xla/service/gpu/runtime3/while_thunk.h"
@@ -147,6 +147,7 @@ class DumpAfterPassIfEnabled : public mlir::PassInstrumentation {
   int pass_counter_ = 0;
 };
 
+#if 0
 // Lowers MLIR module to the XLA Gpu runtime custom calls.
 static absl::Status LowerToXlaGpuRuntime(
     mlir::ModuleOp module, llvm::StringRef entry_function_name,
@@ -193,7 +194,7 @@ static absl::Status LowerToXlaGpuRuntime(
 
   return absl::OkStatus();
 }
-
+#endif
 }  // namespace
 
 void ForAllThunks(const std::function<void(Thunk*)>& fn,
@@ -228,6 +229,7 @@ static void ForwardCollectiveAttrs(mlir::ModuleOp module,
   func->setAttr("num_partitions", b.getI64IntegerAttr(config.num_partitions()));
 }
 
+#if 0
 absl::StatusOr<GpuExecutable::OwnedGpuRuntimeProgram> LowerToJitRt(
     mlir::ModuleOp mlir_module, llvm::StringRef entry_function_name,
     llvm::ArrayRef<int64_t> buffer_sizes,
@@ -257,6 +259,7 @@ absl::StatusOr<GpuExecutable::OwnedGpuRuntimeProgram> LowerToJitRt(
       entry_function_name.str(), std::move(module_str), buffer_sizes.vec(),
       std::move(allocation_indices), module_config.debug_options());
 }
+#endif
 
 // Analyze the function signature to reconstruct a vector of BufferAllocation
 // objects, as well as other output information.
@@ -451,12 +454,13 @@ absl::StatusOr<CompileModuleResults> CompileModuleToLlvmIr(
     llvm::transform(
         results.allocations, std::back_inserter(buffer_sizes),
         [](const BufferAllocation& allocation) { return allocation.size(); });
-
+#if 0
     TF_ASSIGN_OR_RETURN(
         results.executable,
         LowerToJitRt(*mlir_module, entry_function.getName(), buffer_sizes,
                      ir_emitter->ConsumeThunkSequence(), hlo_module,
                      gpu_device_info.gpu_compute_capability()));
+#endif
   } else {
     auto thunk_sequence = ir_emitter->ConsumeThunkSequence();
     ForAllThunks([](Thunk* thunk) { thunk->ClearCompileTimeInfo(); },
diff --git a/xla/service/gpu/cudnn_fused_conv_rewriter.cc b/xla/service/gpu/cudnn_fused_conv_rewriter.cc
index dca7a07a8..58b2cbf7a 100644
--- a/xla/service/gpu/cudnn_fused_conv_rewriter.cc
+++ b/xla/service/gpu/cudnn_fused_conv_rewriter.cc
@@ -846,6 +846,13 @@ absl::StatusOr<bool> FuseBiasOrSideInput(HloComputation* comp) {
         conv->CloneWithNewOperands(conv->shape(), new_operands));
     comp->parent()->SetAndUniquifyInstrName(new_conv, conv->name());
     TF_RETURN_IF_ERROR(new_conv->set_backend_config(gpu_config));
+#if TENSORFLOW_USE_SYCL
+    if (can_accept_side_input) {
+      xla::Cast<HloCustomCallInstruction>(new_conv)
+          ->set_output_to_operand_aliasing(
+              {{{}, {static_cast<long>(new_operands.size()) - 1, {}}}});
+    }
+#endif
     TF_ASSIGN_OR_RETURN(HloInstruction * new_instr,
                         MakeGetTupleElementHlo(new_conv, 0));
     TF_RETURN_IF_ERROR(comp->ReplaceInstruction(instr, new_instr));
diff --git a/xla/service/gpu/cudnn_fused_mha_rewriter.cc b/xla/service/gpu/cudnn_fused_mha_rewriter.cc
index 295f9ca78..c64e97820 100644
--- a/xla/service/gpu/cudnn_fused_mha_rewriter.cc
+++ b/xla/service/gpu/cudnn_fused_mha_rewriter.cc
@@ -328,16 +328,24 @@ bool IsSupportedPrimitiveType(const HloInstruction* bmm) {
 }
 
 bool IsContractingDimSupported(absl::Span<const int64_t> contracting_dims) {
+#if TENSORFLOW_USE_SYCL
+  return true;
+#else
   return absl::c_all_of(contracting_dims,
                         [](int64_t dim) { return dim == 64; });
+#endif
 }
 
 bool IsNonContractingDimSupported(
     const std::vector<int64_t>& non_contracting_dims, bool is_training) {
+#if TENSORFLOW_USE_SYCL
+  return true;
+#else
   // For training, cuDNN require non_contracting_dim to be Divisible by 64
   return absl::c_all_of(non_contracting_dims, [&](int64_t dim) {
     return dim <= 512 && (!is_training || dim % 64 == 0);
   });
+#endif
 }
 
 std::vector<int64_t> GetDimensionVector(absl::Span<const int64_t> dimensions,
@@ -426,6 +434,7 @@ absl::StatusOr<bool> IsSupportedBMM2(const HloInstruction* bmm_2,
   std::vector<int64_t> non_contracting_dims_bmm2 =
       GetDimensionVector(bmm_2->operand(operand_index)->shape().dimensions(),
                          non_contracting_dim_nums_bmm2);
+#if !TENSORFLOW_USE_SYCL
   // The non contracting dimension for BMM2 needs to be 64 for the input matrix.
   // The input matrix is the second argument to BMM2 i.e, rhs.
   if (!absl::c_all_of(non_contracting_dims_bmm2,
@@ -437,6 +446,7 @@ absl::StatusOr<bool> IsSupportedBMM2(const HloInstruction* bmm_2,
     }
     return false;
   }
+#endif
   return true;
 }
 
@@ -670,6 +680,7 @@ MatchFwdResult MatchBmm1UnfusedBiasSoftmaxBmm2(MatchFwdResult previous_result,
                 first_bmm_pattern, unfused_scaled_bmm_subpattern))))) {
     // bmm1 - (scale) - softmax
     match_result.matched_bmm_1 = bmm_1;
+    match_result.matched_scale = scale;
     match_result.matched_custom_call_name =
         has_dropout ? kCudnnfMHASoftmaxDropoutCallTarget
                     : kCudnnfMHASoftmaxCallTarget;
@@ -1240,16 +1251,6 @@ absl::StatusOr<bool> IsMHABlockSupported(
     return false;
   }
 
-  if (bmm_1->shape().rank() != 4 || bmm_2->shape().rank() != 4) {
-    if (VLOG_IS_ON(2)) {
-      VLOG(2) << "Unsupported bmm rank for cuDNN MHA fusion:\n"
-              << bmm_1->ToString() << "\nOR\n"
-              << bmm_2->ToString() << "\n"
-              << "Only bmm with rank 4 is supported.";
-    }
-    return false;
-  }
-
   // check if matched attention block is supported by cuDNN flash attention.
   TF_ASSIGN_OR_RETURN(is_flash_attention,
                       IsFlashAttention(bmm_1, is_causal_mask, custom_call_name,
@@ -1857,15 +1861,15 @@ absl::StatusOr<bool> CudnnFusedMHARewriter::Run(
         comp->parent()->config().debug_options();
     const auto cudnn_version =
         GetRealCuDNNVersion(cudnn_version_, stream_executor_);
-#if CUDA_VERSION < 12000
-    return false;
-#endif
+
+#if !TENSORFLOW_USE_SYCL
     if (!debug_options.xla_gpu_enable_cudnn_fmha() ||
         !IsComputeCapabilityAndCudnnSupported(
             compute_capability_, cudnn_version,
             stream_executor::dnn::VersionInfo(8, 8, 0))) {
       return false;
     }
+#endif  // !TENSORFLOW_USE_SYCL
     for (HloInstruction* instr : comp->MakeInstructionPostOrder()) {
       bool v_transposed = false;
       bool changed = false;
@@ -1936,6 +1940,8 @@ absl::StatusOr<bool> CudnnFusedMHARewriter::Run(
               matched_result.is_causal_mask,
               matched_result.is_flash_attention));
       any_changed |= changed;
+
+#if !TENSORFLOW_USE_SYCL
       if (matched_result.is_training) {
         MatchBwdResult matched_bwd_result =
             MatchBwdMHAPatternsForCanonicalization(
@@ -2019,6 +2025,7 @@ absl::StatusOr<bool> CudnnFusedMHARewriter::Run(
                 matched_bwd_result.matched_custom_call_name));
         any_changed |= changed;
       }
+#endif  // !TENSORFLOW_USE_SYCL
     }
   }
 
diff --git a/xla/service/gpu/cusolver_context.cc b/xla/service/gpu/cusolver_context.cc
index d63f7a7cc..51023e44c 100644
--- a/xla/service/gpu/cusolver_context.cc
+++ b/xla/service/gpu/cusolver_context.cc
@@ -40,6 +40,7 @@ limitations under the License.
 namespace xla {
 namespace gpu {
 
+#if !TENSORFLOW_USE_SYCL
 namespace {
 
 // Type traits to get CUDA complex types from std::complex<T>.
@@ -427,6 +428,19 @@ absl::Status GpuSolverContext::PotrfBatched(
 #endif
       ToDevicePointer(lapack_info), batch_size));
 }
+#else // !TENSORFLOW_USE_SYCL
 
+StatusOr<GpuSolverContext> GpuSolverContext::Create() {
+  return GpuSolverContext();
+}
+
+Status GpuSolverContext::SetStream(se::Stream* stream) {
+  gpu_stream_ = stream_executor::gpu::AsGpuStreamValue(stream);
+  return OkStatus();
+}
+
+GpuSolverContext::GpuSolverContext() {}
+
+#endif // !TENSORFLOW_USE_SYCL
 }  // namespace gpu
 }  // namespace xla
diff --git a/xla/service/gpu/cusolver_context.h b/xla/service/gpu/cusolver_context.h
index d17a570ef..5602df2b3 100644
--- a/xla/service/gpu/cusolver_context.h
+++ b/xla/service/gpu/cusolver_context.h
@@ -26,6 +26,12 @@ limitations under the License.
 #define TENSORFLOW_USE_CUSOLVER_OR_HIPSOLVER \
   (!TENSORFLOW_USE_ROCM || TENSORFLOW_USE_HIPSOLVER)
 
+#if TENSORFLOW_USE_SYCL
+#include "oneapi/mkl/blas.hpp"
+#include "oneapi/mkl/lapack.hpp"
+#include "oneapi/mkl/dfti.hpp"
+#include "oneapi/mkl/exceptions.hpp"
+#else // TENSORFLOW_USE_SYCL
 #if !TENSORFLOW_USE_ROCM
 #include "third_party/gpus/cuda/include/cusolverDn.h"
 using gpusolverHandle_t = cusolverDnHandle_t;
@@ -41,16 +47,18 @@ using gpusolverHandle_t = hipsolverHandle_t;
 using gpusolverHandle_t = rocblas_handle;
 #endif  // TF_ROCM_VERSION >= 40500
 #endif  // TENSORFLOW_USE_ROCM
+#endif  // TENSORFLOW_USE_SYCL
 
 #include "xla/statusor.h"
 #include "xla/stream_executor/blas.h"
 #include "xla/stream_executor/stream_executor.h"
 #include "xla/xla_data.pb.h"
+#include "xla/stream_executor/gpu/gpu_types.h"
 
 namespace xla {
 namespace gpu {
-
 namespace se = ::stream_executor;
+#if !TENSORFLOW_USE_SYCL
 
 class GpuSolverContext {
  public:
@@ -104,6 +112,59 @@ class GpuSolverContext {
   std::unique_ptr<std::remove_pointer_t<gpusolverHandle_t>, Deleter> handle_;
 };
 
+#else // !TENSORFLOW_USE_SYCL
+class GpuSolverContext {
+ public:
+  static StatusOr<GpuSolverContext> Create();
+  Status SetStream(se::Stream* stream);
+
+  template <typename T>
+  Status PotrfBatched(se::blas::UpperLower uplo, int n, se::DeviceMemory<T*> as,
+                      int lda, se::DeviceMemory<int> lapack_info,
+                      int batch_size, T* a_base) {
+    T* scratch_data = static_cast<T*>(as.opaque());
+    int64_t scratchpad_size = as.size() / sizeof(T);
+
+    const int64_t stride_a = n * n;
+
+    oneapi::mkl::uplo params_uplo;
+    switch (uplo) {
+      case se::blas::UpperLower::kLower:
+        params_uplo = oneapi::mkl::uplo::L;
+        break;
+      case se::blas::UpperLower::kUpper:
+        params_uplo = oneapi::mkl::uplo::U;
+        break;
+      default:
+        params_uplo = static_cast<oneapi::mkl::uplo>(uplo);
+    }
+
+    try {
+      oneapi::mkl::lapack::potrf_batch(*gpu_stream_, params_uplo, n, a_base,
+                                       lda, stride_a, batch_size, scratch_data,
+                                       scratchpad_size);
+    } catch (oneapi::mkl::lapack::batch_error const& be) {
+      int i = 0;
+      auto& ids = be.ids();
+      for (auto const& e : be.exceptions()) {
+        try {
+          std::rethrow_exception(e);
+        } catch (oneapi::mkl::lapack::exception& e) {
+          LOG(ERROR) << "Exception " << ids[i++]
+                     << " in a batch says: " << e.what()
+                     << " (info code: " << e.info() << ")";
+        }
+      }
+    }
+    return OkStatus();
+  }
+
+ private:
+  explicit GpuSolverContext();
+  stream_executor::gpu::GpuStreamHandle gpu_stream_;
+};
+
+#endif // !TENSORFLOW_USE_SYCL
 }  // namespace gpu
 }  // namespace xla
 
diff --git a/xla/service/gpu/cusolver_rewriter.cc b/xla/service/gpu/cusolver_rewriter.cc
index fbf01f2f9..12a35d92e 100644
--- a/xla/service/gpu/cusolver_rewriter.cc
+++ b/xla/service/gpu/cusolver_rewriter.cc
@@ -70,6 +70,7 @@ absl::StatusOr<HloInstruction*> CreateCholesky(GpuSolverContext* context,
   absl::c_iota(batch_dim_ids, 0);
   int64_t batch_size = absl::c_accumulate(batch_dims, 1, std::multiplies<>{});
 
+#if !TENSORFLOW_USE_SYCL
   // Find the workspace size.
   se::blas::UpperLower uplo = options.lower() ? se::blas::UpperLower::kLower
                                               : se::blas::UpperLower::kUpper;
@@ -77,7 +78,36 @@ absl::StatusOr<HloInstruction*> CreateCholesky(GpuSolverContext* context,
   TF_ASSIGN_OR_RETURN(
       workspace_size,
       context->PotrfBufferSize(a_shape.element_type(), uplo, n, n, batch_size));
-
+#else
+  // Find the workspace size.
+  int64_t workspace_size = 0;
+  oneapi::mkl::uplo uplo =
+      options.lower() ? oneapi::mkl::uplo::L : oneapi::mkl::uplo::U;
+  sycl::property_list propList{sycl::property::queue::in_order()};
+  sycl::queue queue(sycl::gpu_selector{}, propList);
+  switch (a_shape.element_type()) {
+    case F32:
+      workspace_size = oneapi::mkl::lapack::potrf_batch_scratchpad_size<float>(
+          queue, uplo, n, n, n * n, batch_size);
+      break;
+    case F64:
+      workspace_size = oneapi::mkl::lapack::potrf_batch_scratchpad_size<double>(
+          queue, uplo, n, n, n * n, batch_size);
+      break;
+    case C64:
+      workspace_size =
+          oneapi::mkl::lapack::potrf_batch_scratchpad_size<std::complex<float>>(
+              queue, uplo, n, n, n * n, batch_size);
+      break;
+    case C128:
+      workspace_size = oneapi::mkl::lapack::potrf_batch_scratchpad_size<
+          std::complex<double>>(queue, uplo, n, n, n * n, batch_size);
+      break;
+    default:
+      return InvalidArgument("Invalid type for cholesky %s",
+                             PrimitiveType_Name(a_shape.element_type()));
+  }
+#endif
   // TODO(phawkins): Ideally we would relax this constraint. What we actually
   // want is that:
   // a) the batch dimensions are major, in no particular order.
@@ -104,6 +134,9 @@ absl::StatusOr<HloInstruction*> CreateCholesky(GpuSolverContext* context,
   TF_RETURN_IF_ERROR(custom_call->set_backend_config(options));
   HloInstruction* out = computation->AddInstruction(
       HloInstruction::CreateGetTupleElement(a_shape, custom_call, 0));
+#if TENSORFLOW_USE_SYCL
+  return out;
+#else
   HloInstruction* info = computation->AddInstruction(
       HloInstruction::CreateGetTupleElement(info_shape, custom_call, 2));
 
@@ -133,6 +166,7 @@ absl::StatusOr<HloInstruction*> CreateCholesky(GpuSolverContext* context,
       computation->AddInstruction(HloInstruction::CreateTernary(
           a_shape, HloOpcode::kSelect, ok, out, nans));
   return select;
+#endif
 }
 
 // Tries to rewrite a single convolution into a call to cudnn.
diff --git a/xla/service/gpu/elemental_ir_emitter.cc b/xla/service/gpu/elemental_ir_emitter.cc
index 60422d296..00c8bcee6 100644
--- a/xla/service/gpu/elemental_ir_emitter.cc
+++ b/xla/service/gpu/elemental_ir_emitter.cc
@@ -374,6 +374,7 @@ llvm::Value* GpuElementalIrEmitter::EmitThreadId() {
 
 absl::StatusOr<llvm::Value*> GpuElementalIrEmitter::EmitF32ToBF16(
     llvm::Value* f32_value) {
+#if 0
   // sm_80 and up has an instruction to convert f32 into bf16.
   if (ir_emitter_context_.cuda_compute_capability().IsAtLeast(
           se::CudaComputeCapability::AMPERE)) {
@@ -381,6 +382,7 @@ absl::StatusOr<llvm::Value*> GpuElementalIrEmitter::EmitF32ToBF16(
         FPTrunc(BitCast(f32_value, b()->getFloatTy()), b()->getBFloatTy()),
         b()->getInt16Ty());
   }
+#endif
   return ElementalIrEmitter::EmitF32ToBF16(f32_value);
 }
 
diff --git a/xla/service/gpu/fusions/copy.cc b/xla/service/gpu/fusions/copy.cc
index eeb934c82..ce405ab11 100644
--- a/xla/service/gpu/fusions/copy.cc
+++ b/xla/service/gpu/fusions/copy.cc
@@ -24,6 +24,10 @@ limitations under the License.
 #include "xla/service/gpu/thunk.h"
 #include "xla/statusor.h"
 
+#if TENSORFLOW_USE_SYCL
+#include "xla/service/gpu/ir_emission_utils.h"
+#endif
+
 namespace xla {
 namespace gpu {
 
diff --git a/xla/service/gpu/fusions/fusion_emitter.cc b/xla/service/gpu/fusions/fusion_emitter.cc
index f4ddb497d..ad95a1921 100644
--- a/xla/service/gpu/fusions/fusion_emitter.cc
+++ b/xla/service/gpu/fusions/fusion_emitter.cc
@@ -61,6 +61,10 @@ limitations under the License.
 #include "tsl/platform/errors.h"
 #include "tsl/platform/statusor.h"
 
+#if TENSORFLOW_USE_SYCL
+#include "xla/service/gpu/ir_emission_utils.h"
+#endif
+
 namespace xla {
 namespace gpu {
 namespace {
@@ -217,9 +221,13 @@ BuildKernelPrototype(IrEmitterContext& ir_emitter_context,
   // Create the kernel and add it to the module.
   auto* llvm_module = ir_emitter_context.llvm_module();
   llvm::LLVMContext& context = llvm_module->getContext();
+
+  // SYCL: Hardcode to global device addrspace.
+  llvm::Type* arg_type =
+      IsSPIR(llvm_module)? builder->getPtrTy(1) : builder->getPtrTy();
   llvm::FunctionType* kernel_type = llvm::FunctionType::get(
       /*Result=*/llvm::Type::getVoidTy(context),
-      std::vector<llvm::Type*>(kNumLlvmArgs, builder->getPtrTy()),
+      std::vector<llvm::Type*>(kNumLlvmArgs, arg_type),
       /*isVarArg=*/false);
   llvm::Function* kernel =
       llvm::Function::Create(kernel_type, llvm::GlobalValue::ExternalLinkage,
@@ -230,6 +238,17 @@ BuildKernelPrototype(IrEmitterContext& ir_emitter_context,
       ir_emitter_context.gpu_device_info(), launch_dimensions, kernel_name,
       llvm_module));
 
+  // SYCL: Set function metadata
+  if (IsSPIR(llvm_module)) {
+    llvm::LLVMContext& context = llvm_module->getContext();
+    llvm::IntegerType* i32 = llvm::Type::getInt32Ty(context);
+    kernel->setMetadata(
+        "intel_reqd_sub_group_size",
+        llvm::MDNode::get(context,
+                          {llvm::ConstantAsMetadata::get(
+                              llvm::ConstantInt::get(i32, WarpSize()))}));
+  }
+
   // TODO(b/65380986): Investigate if adding fast math flags for generated
   // kernels makes sense.
 
diff --git a/xla/service/gpu/fusions/reduction.cc b/xla/service/gpu/fusions/reduction.cc
index 55519f58e..7e70e802a 100644
--- a/xla/service/gpu/fusions/reduction.cc
+++ b/xla/service/gpu/fusions/reduction.cc
@@ -105,6 +105,8 @@ constexpr int kColMajorKeptDimension = 0;
 constexpr int kColReducedDimension = 1;
 constexpr int kColMinorKeptDimension = 2;
 
+constexpr int kVectorizedDimension = 3;
+
 using TypedPointer = std::pair<llvm::Value* const, llvm::Type* const>;
 
 // Fusion root -> array of indexes, one per reduction output.
@@ -249,18 +251,24 @@ std::vector<std::vector<const HloInstruction*>> GroupDisjointReductions(
 int GetVectorSize(const HloFusionAnalysis& analysis,
                   const ReductionDimensions& reduction_dimensions,
                   int num_threads, Vector3 reduction_tiling) {
-  if (!reduction_dimensions.is_row_reduction) {
+  if (MayPreventVectorization(analysis.fusion())) {
     return 1;
   }
 
-  if (reduction_dimensions.dimensions[kRowMinorReducedDimension] % 2 != 0 ||
-      MayPreventVectorization(analysis.fusion())) {
+  if (!reduction_dimensions.is_row_reduction) {
+    // Check if the last dimension is divisible by (vector_size * num_threads).
+    auto num_kept_minor =
+        reduction_dimensions.dimensions[kColMinorKeptDimension];
+    return num_kept_minor % (2 * num_threads) == 0 ? 2 : 1;
+  }
+
+  if (reduction_dimensions.dimensions[kRowMinorReducedDimension] % 2 != 0) {
     return 1;
   }
 
-  // Enabling vectorization if number of threads is <= warpsize leads to half or
-  // more of the threads not doing any work.
-  if (num_threads <= WarpSize()) {
+  // Enabling vectorization if (number_threads * vector_size) is <= minor_reduced_dimension
+  // otherwise exist threads not doing any work.
+  if (num_threads * 2 > reduction_dimensions.dimensions[kRowMinorReducedDimension]) {
     return 1;
   }
 
@@ -280,6 +288,60 @@ int GetVectorSize(const HloFusionAnalysis& analysis,
   return 1;
 }
 
+std::tuple<Vector3, int, bool> AdjustColReductionTilingConfig(
+    const HloFusionAnalysis& analysis, Vector3 reduction_dimensions,
+    Vector3 reduction_tiling, int64_t num_threads_y, int64_t num_threads_x,
+    int vector_size) {
+  // Compute active core number by assuming each block occupy one sm core. It is
+  // a conservative but easy approach otherwise we should consider shared memory
+  // size. Due to column reduction always use 1024 as block size, the computed
+  // active core num has not much difference from the actual situation.
+  auto active_core_num = [&](int64_t tile_size) {
+    int64_t blocks_x = CeilOfRatio(reduction_dimensions[kColMinorKeptDimension],
+                                   num_threads_x * vector_size);
+    int64_t block_tile_y = num_threads_y * tile_size;
+    int64_t blocks_y =
+        CeilOfRatio(reduction_dimensions[kColReducedDimension], block_tile_y);
+    int64_t blocks =
+        reduction_dimensions[kColMajorKeptDimension] * blocks_x * blocks_y;
+    return blocks;
+  };
+
+  auto core_count = analysis.device_info().core_count();
+  constexpr int minimum_tile_size = 8;
+
+  // Early return if device occupancy is already high.
+  if (active_core_num(reduction_tiling[kColReducedDimension]) >= core_count) {
+    return {reduction_tiling, vector_size, false};
+  }
+
+  auto roots = analysis.fusion().GetRoots();
+  for (auto [root, hero] : llvm::zip(roots, analysis.fusion_heroes())) {
+    // Only adjust tile_y if hero is reduction and output element type is F32.
+    // F32 atomic is fast so that we can ignore the extra atomic overhead
+    // by adjusting tile_y to increase the parallelism of the kernel.
+    if (hero->opcode() == HloOpcode::kReduce) {
+      if (hero != (&root.instruction()) ||
+          hero->shape().element_type() != F32) {
+        // If we can not adjust tile_y but active core number is small, reset
+        // vector size as 1.
+        return {reduction_tiling, 1, false};
+      }
+    }
+  }
+
+  auto actual_tile_size =
+      CeilOfRatio(reduction_dimensions[kColReducedDimension], num_threads_y);
+  auto current_tile_size = actual_tile_size;
+  while (current_tile_size >= minimum_tile_size * 2) {
+    if (active_core_num(current_tile_size) >= core_count) break;
+    current_tile_size = current_tile_size / 2;
+  }
+  bool tile_size_decreased = current_tile_size != actual_tile_size;
+  reduction_tiling[kColReducedDimension] = current_tile_size;
+  return {reduction_tiling, tile_size_decreased ? vector_size : 1, tile_size_decreased};
+}
+
 llvm::Value* CastSharedToGlobal(llvm::IRBuilder<>* builder, llvm::Value* input,
                                 llvm::Type* element_type, llvm::Twine name) {
   return builder->CreateAddrSpaceCast(
@@ -448,20 +510,12 @@ ReductionFusion::ReductionGroupEmitter::ReductionGroupEmitter(
            << reduction_emitter_.fusion_.ToString();
 
   auto* builder = reduction_emitter_.builder_;
+  const Tiling& tiling = reduction_info.GetTiling();
   for (const HloReduceInstruction* reduce_hlo : reduce_instr_index_group) {
     for (int op_result_idx = 0;
          op_result_idx < GetNumOutputs(reduce_hlo->shape()); op_result_idx++) {
       Shape result_shape = OutputShape(reduce_hlo->shape(), op_result_idx);
 
-      llvm::Type* element_type = llvm_ir::PrimitiveTypeToIrType(
-          result_shape.element_type(), builder->GetInsertBlock()->getModule());
-      llvm::AllocaInst* reduction_input_address =
-          llvm_ir::EmitAllocaAtFunctionEntry(
-              element_type, "reduction_input_address", builder);
-
-      llvm::AllocaInst* result_address = llvm_ir::EmitAllocaAtFunctionEntry(
-          element_type, "partial_reduction_result", builder);
-
       const HloInstruction* init_value =
           reduce_hlo->init_values()[op_result_idx];
 
@@ -469,8 +523,33 @@ ReductionFusion::ReductionGroupEmitter::ReductionGroupEmitter(
       llvm::Value* init_ir_value = (*fused_emitter.GetGenerator(
           *init_value))(llvm_ir::IrArray::Index(builder->getInt32Ty()))
                                        .value();
-
-      builder->CreateStore(init_ir_value, result_address);
+      
+      llvm::Type* element_type = llvm_ir::PrimitiveTypeToIrType(
+          result_shape.element_type(), builder->GetInsertBlock()->getModule());
+      llvm::AllocaInst *reduction_input_address, *result_address;
+      if (reduction_info.IsRowReduction()) {
+        reduction_input_address = llvm_ir::EmitAllocaAtFunctionEntry(
+            element_type, "reduction_input_address", builder);
+        result_address = llvm_ir::EmitAllocaAtFunctionEntry(
+            element_type, "partial_reduction_result", builder);
+        builder->CreateStore(init_ir_value, result_address);
+      } else {
+        auto vectorize_size = tiling.GetThreadTileSize()[kVectorizedDimension];
+        reduction_input_address = llvm_ir::EmitAllocaAtFunctionEntryWithCount(
+            element_type,
+            llvm::ConstantInt::get(builder->getInt32Ty(), vectorize_size),
+            "reduction_input_address", builder);
+        result_address = llvm_ir::EmitAllocaAtFunctionEntryWithCount(
+            element_type,
+            llvm::ConstantInt::get(builder->getInt32Ty(), vectorize_size),
+            "partial_reduction_result", builder);
+        for (int id = 0; id < vectorize_size; id++) {
+          auto slot = builder->CreateInBoundsGEP(
+              element_type,
+              result_address, {builder->getInt32(id)});
+          builder->CreateStore(init_ir_value, slot);
+        }
+      }
       const Tiling& tiling = reduction_info.GetTiling();
       auto shared_cache = [&]() -> std::optional<llvm_ir::SharedMemoryTile> {
         auto* module = reduction_emitter.ir_emitter_context_.llvm_module();
@@ -774,7 +853,9 @@ ReductionFusion::ReductionGroupEmitter::GetOutputIndexForReduction(
     auto* minor_idx = builder->CreateAdd(offset[kColMinorKeptDimension],
                                          thread_ids[kColReducedDimension]);
     return {{major_idx, minor_idx},
-            ShapeUtil::DeleteDimension(kColReducedDimension, shape),
+            {shape.dimensions(kColMajorKeptDimension),
+             shape.dimensions(kColMinorKeptDimension) *
+                 shape.dimensions(kVectorizedDimension)},
             index_ty};
   }();
 
@@ -956,8 +1037,11 @@ void ReductionFusion::ReductionGroupEmitter::
   auto* builder = reduction_emitter_.builder_;
   KernelSupportLibrary ksl(builder);
   const HloComputation* reducer = reduction->to_apply();
-  const auto& thread_id_info = tiling_kernel_info.thread_id_info;
-  const auto& thread_ids = thread_id_info.thread_ids;
+  TilingKernelInfo reduction_tiling_info = tiling_kernel_info;
+  auto& tile_origin = reduction_tiling_info.tile_origin;
+  auto& output_tile_bounds = reduction_tiling_info.output_tile_bounds;
+  auto& thread_id_info = reduction_tiling_info.thread_id_info;
+  auto& thread_ids = thread_id_info.thread_ids;
 
   auto constant = [&](uint64_t c) -> llvm::Constant* {
     return llvm::ConstantInt::get(reduction_emitter_.index_ty_, c);
@@ -967,53 +1051,75 @@ void ReductionFusion::ReductionGroupEmitter::
   };
   const auto& reduction_info = reduction_emitter_.reduction_codegen_info_;
   const Tiling& tiling = reduction_info.GetTiling();
+  const auto& tile_size = tiling.GetThreadTileSize();
   int num_outputs = reducer->num_parameters() / 2;
 
   auto* kept_index = thread_ids[kColMinorKeptDimension];
   auto* reduced_index = thread_ids[kColReducedDimension];
 
-  // Store the transpose in shared memory.
-  for (int output_idx = 0; output_idx < num_outputs; output_idx++) {
-    const auto& state = GetCalculationStateFor(reduction, output_idx);
-    auto* current_output_value =
-        builder->CreateLoad(state.partial_result_address->getAllocatedType(),
-                            state.partial_result_address);
-    state.shared_cache->Store(current_output_value, {kept_index, reduced_index},
-                              builder);
+  // Some warps in the block are completely outside of the bound of the
+  // tensor, so they should not write any output at all.
+  llvm::Value* has_output = builder->CreateAnd(
+      builder->CreateICmpULT(reduced_index,
+                             output_tile_bounds[kColMinorKeptDimension]),
+      builder->CreateICmpULT(kept_index,
+                             output_tile_bounds[kColReducedDimension]));
+
+  if (tile_size[kVectorizedDimension] > 1) {
+    std::vector<llvm::Value*> tile_index = tile_origin.multidim();
+    tile_index[kColMinorKeptDimension] =
+        builder->CreateMul(tile_index[kColMinorKeptDimension],
+                           output_tile_bounds[kVectorizedDimension]);
+    tile_origin = llvm_ir::IrArray::Index(tile_index, tiling.GetShape(),
+                                          reduction_emitter_.index_ty_);
   }
 
-  reduction_emitter_.EmitSyncThreads();
-
-  // Get transposed element from shared memory.
-  absl::InlinedVector<TypedPointer, 2> shmem_transposed_addrs;
-  for (int output_idx = 0; output_idx < num_outputs; output_idx++) {
-    const auto& state = GetCalculationStateFor(reduction, output_idx);
-    auto* shmem_transposed_addr =
-        state.shared_cache->Address({reduced_index, kept_index}, builder);
-    shmem_transposed_addrs.push_back(
-        {shmem_transposed_addr, state.shared_cache->GetElementType()});
-  }
+  for (int vec_dim = 0; vec_dim < tile_size[kVectorizedDimension]; vec_dim++) {
+    // Store the transpose in shared memory.
+    for (int output_idx = 0; output_idx < num_outputs; output_idx++) {
+      const auto& state = GetCalculationStateFor(reduction, output_idx);
+      auto* partial_result_address = builder->CreateInBoundsGEP(
+          state.partial_result_address->getAllocatedType(),
+          state.partial_result_address, {builder->getInt32(vec_dim)});
+      auto* current_output_value =
+          builder->CreateLoad(state.partial_result_address->getAllocatedType(),
+                              partial_result_address);
+      state.shared_cache->Store(current_output_value,
+                                {kept_index, reduced_index}, builder);
+    }
 
-  EmitFullWarpShuffleDownLoopForReduce(reducer,
-                                       absl::MakeSpan(shmem_transposed_addrs),
-                                       tiling.GetNumThreadsPerBlock(),
-                                       /*num_results_per_warp=*/1);
+    reduction_emitter_.EmitSyncThreads();
 
-  // Some warps in the block are completely outside of the bound of the
-  // tensor, so they should not write any output at all.
-  llvm::Value* has_output = builder->CreateAnd(
-      builder->CreateICmpULT(
-          reduced_index,
-          tiling_kernel_info.output_tile_bounds[kColMinorKeptDimension]),
-      builder->CreateICmpULT(
-          kept_index,
-          tiling_kernel_info.output_tile_bounds[kColReducedDimension]));
-
-  ksl.If("reduction_write_output",
-         builder->CreateAnd(has_output, is_zero(thread_id_info.lane_id)), [&] {
-           WriteReductionOutput(tiling_kernel_info, reduction, roots,
-                                shmem_transposed_addrs);
-         });
+    // Get transposed element from shared memory.
+    absl::InlinedVector<TypedPointer, 2> shmem_transposed_addrs;
+    for (int output_idx = 0; output_idx < num_outputs; output_idx++) {
+      const auto& state = GetCalculationStateFor(reduction, output_idx);
+      auto* shmem_transposed_addr =
+          state.shared_cache->Address({reduced_index, kept_index}, builder);
+      shmem_transposed_addrs.push_back(
+          {shmem_transposed_addr, state.shared_cache->GetElementType()});
+    }
+    EmitFullWarpShuffleDownLoopForReduce(reducer,
+                                         absl::MakeSpan(shmem_transposed_addrs),
+                                         tiling.GetNumThreadsPerBlock(),
+                                         /*num_results_per_warp=*/1);
+
+    thread_ids[kColReducedDimension] = builder->CreateAdd(
+        builder->CreateMul(reduced_index,
+                           output_tile_bounds[kVectorizedDimension]),
+        llvm::ConstantInt::get(reduction_emitter_.index_ty_, vec_dim));
+
+    ksl.If("reduction_write_output",
+           builder->CreateAnd(has_output, is_zero(thread_id_info.lane_id)),
+           [&] {
+             WriteReductionOutput(reduction_tiling_info, reduction, roots,
+                                  shmem_transposed_addrs);
+           });
+
+    if (tile_size[kVectorizedDimension] > 1) {
+      reduction_emitter_.EmitSyncThreads();
+    }
+  }
 }
 
 // Generate a single element of the tile (update the accumulator state) for a
@@ -1023,6 +1129,8 @@ void ReductionFusion::ReductionGroupEmitter::GenerateElementForReducer(
     const llvm_ir::IrArray::Index& index) const {
   HloComputation* reducer = reduction->to_apply();
   auto* builder = reduction_emitter_.builder_;
+  const ReductionCodegenInfo& reduction_info =
+      reduction_emitter_.reduction_codegen_info_;
   CHECK_EQ(reducer->num_parameters() % 2, 0);
 
   absl::InlinedVector<llvm::Value*, 2> reduction_accumulators;
@@ -1030,12 +1138,21 @@ void ReductionFusion::ReductionGroupEmitter::GenerateElementForReducer(
   for (int red_idx = 0; red_idx < reducer->num_parameters() / 2; red_idx++) {
     const auto& state = GetCalculationStateFor(reduction, red_idx);
 
-    llvm::AllocaInst* input_address = state.input_address;
+    llvm::Value* input_address = state.input_address;
+    llvm::Value* partial_result_address = state.partial_result_address;
+    if (!reduction_info.IsRowReduction()) {
+      input_address = builder->CreateInBoundsGEP(
+          state.input_address->getAllocatedType(), input_address,
+          {index[kVectorizedDimension]});
+      partial_result_address = builder->CreateInBoundsGEP(
+          state.partial_result_address->getAllocatedType(),
+          partial_result_address, {index[kVectorizedDimension]});
+    }
     auto input_index =
         index.SourceIndexOfBitcast(reduction->operand(0)->shape(), builder);
     llvm::Value* const input_ir_value = *state.input_gen(input_index);
     builder->CreateStore(input_ir_value, input_address);
-    reduction_accumulators.push_back(state.partial_result_address);
+    reduction_accumulators.push_back(partial_result_address);
     reduction_input_value.push_back(input_address);
   }
 
@@ -1329,7 +1446,8 @@ ReductionFusion::ComputeReductionCodegenInfo(
   // parallelizing the z dimension (major reduced dimensions). The general
   // recommendation is to use between 128 and 512 threads, so we just go for
   // 256. See https://forums.developer.nvidia.com/t/55529
-  constexpr int64_t kThreadsPerBlockTarget = 256;
+  // SYCL: Use 32 as WA for intel platform to avoid reduce hang.
+  constexpr int64_t kThreadsPerBlockTarget = 32;
   if (reduction_dimensions.is_row_reduction &&
       num_threads_x * 2 <= kThreadsPerBlockTarget) {
     int64_t kept_size = reduction_dimensions.dimensions[kRowKeptDimension];
@@ -1348,14 +1466,21 @@ ReductionFusion::ComputeReductionCodegenInfo(
 
   int vector_size = GetVectorSize(analysis, reduction_dimensions, num_threads_x,
                                   reduction_tiling);
-
+  bool tile_size_decreased = false;
+  if (!reduction_dimensions.is_row_reduction) {
+    // Adjust tile_y and vector size for column reduction if device occupancy is low.
+    std::tie(reduction_tiling, vector_size, tile_size_decreased) =
+        AdjustColReductionTilingConfig(analysis, shape, reduction_tiling,
+                                       num_threads_y, num_threads_x, vector_size);
+  }
   absl::InlinedVector<int64_t, 4> num_threads{1, num_threads_y, num_threads_x};
   absl::InlinedVector<int64_t, 4> tiled_shape{shape[0], shape[1],
                                               shape[2] / vector_size};
   absl::InlinedVector<int64_t, 4> tile_per_thread{
       reduction_tiling[0], reduction_tiling[1],
-      reduction_tiling[2] / vector_size};
-  if (vector_size != 1) {
+      reduction_dimensions.is_row_reduction ? reduction_tiling[2] / vector_size
+                                            : reduction_tiling[2]};
+  if (!reduction_dimensions.is_row_reduction || vector_size != 1) {
     num_threads.push_back(1);  // The vector dimension is a loop.
     tiled_shape.push_back(vector_size);
     tile_per_thread.push_back(vector_size);
@@ -1365,6 +1490,8 @@ ReductionFusion::ComputeReductionCodegenInfo(
                 /*loops_to_unroll=*/{false, false, true, false});
   bool reduction_is_race_free = ReductionIsRaceFree(
       hero_reduction->GetModule()->config(), reduction_dimensions);
+  // If tile_y is decreased, reduction is not race free.
+  reduction_is_race_free = reduction_is_race_free && !tile_size_decreased;
   return ReductionCodegenInfo(
       tiling, reduction_dimensions.is_row_reduction, reduction_is_race_free,
       GroupDisjointReductions(analysis), hero_reduction);
diff --git a/xla/service/gpu/gemm_rewriter.cc b/xla/service/gpu/gemm_rewriter.cc
index 8a3919a75..a69632e96 100644
--- a/xla/service/gpu/gemm_rewriter.cc
+++ b/xla/service/gpu/gemm_rewriter.cc
@@ -566,6 +566,7 @@ class GemmRewriterVisitor : public DfsHloRewriteVisitor {
 
   absl::Status HandleMultiply(HloInstruction *instr) override {
     HloInstruction *alpha, *existing_gemm;
+#if !TENSORFLOW_USE_SYCL
     if (Match(instr,
               m::MultiplyAnyOrder(
                   GemmOrCublasLtMatmulMaybeF8(&existing_gemm).WithOneUser(),
@@ -589,11 +590,13 @@ class GemmRewriterVisitor : public DfsHloRewriteVisitor {
         return ReplaceInstruction(instr, existing_gemm);
       }
     }
+#endif  // !TENSORFLOW_USE_SYCL
 
     // Attempt to match approximate GELU activation
     // (https://arxiv.org/abs/1606.08415), where:
     // approx_gelu(x) = x * cdf(x)
     // cdf(x) = 0.5 * (1 + tanh(sqrt(2 / pi) * (x + 0.044715 * x**3))
+    // SYCL: OptionalBitcast?
     HloInstruction *cdf, *slice_or_bitcast = nullptr;
     if (Match(instr, m::MultiplyAnyOrder(
                          m::AnyOf<HloInstruction>(
@@ -633,6 +636,14 @@ class GemmRewriterVisitor : public DfsHloRewriteVisitor {
                                   .WithOneUser())
                               .WithOneUser())
                           .WithOneUser())))) {
+      // SYCL
+      // gemm - bitcast - gelu - bitcast
+      if (instr->user_count() == 1) {
+        auto bitcast = instr->users()[0];
+        if (bitcast->opcode() == HloOpcode::kBitcast &&
+            ShapeUtil::Compatible(bitcast->shape(), existing_gemm->shape()))
+          return FuseGeluActivation(bitcast, existing_gemm);
+      }
       return FuseGeluActivation(instr, existing_gemm, slice_or_bitcast);
     }
     return absl::OkStatus();
@@ -1277,8 +1288,8 @@ class GemmRewriterVisitor : public DfsHloRewriteVisitor {
       return in_out_alias_config.ParameterHasAlias(bias->parameter_number(),
                                                    /*param_index=*/{});
     }();
-    bool want_to_fuse_bias = IsCublasLtMatmulF8(*gemm) ||
-                             IsCublasLtMatmul(*gemm) || can_overwrite_bias;
+    // SYCL: cannot always fuse bias.
+    bool want_to_fuse_bias = can_overwrite_bias;
 
     auto gpu_config = gemm->backend_config<GpuBackendConfig>().value();
     GemmBackendConfig &config = *gpu_config.mutable_gemm_backend_config();
@@ -1576,13 +1587,14 @@ class GemmRewriterVisitor : public DfsHloRewriteVisitor {
   absl::StatusOr<absl::string_view> GetNonFp8GemmCustomCallTarget(
       const HloInstruction &instr,
       const GemmBackendConfig &gemm_backend_config) const {
-    if (!instr.GetModule()
-             ->config()
-             .debug_options()
-             .xla_gpu_enable_cublaslt()) {
-      // cublasLt is not enabled.
-      return absl::string_view(kGemmCallTarget);
-    }
+    // SYCL: disable fallback
+    // if (!instr.GetModule()
+    //          ->config()
+    //          .debug_options()
+    //          .xla_gpu_enable_cublaslt()) {
+    //   // cublasLt is not enabled.
+    //   return absl::string_view(kGemmCallTarget);
+    // }
 
     // cublasLt is enabled, check if other internal conditions are met.
     const HloInstruction *lhs = instr.operand(0);
@@ -1879,11 +1891,6 @@ class GemmRewriterVisitor : public DfsHloRewriteVisitor {
     for (auto batch_dimension : batch_dimensions) {
       batch_count *= lhs->shape().dimensions(batch_dimension);
     }
-    if (batch_count > kMaxBatchCount) {
-      // This is not supported by cublasLt.
-      return false;
-    }
-
     TF_ASSIGN_OR_RETURN(bool output_is_column_major,
                         MatrixIsColumnMajor(instr, gemm_backend_config));
 
diff --git a/xla/service/gpu/gpu_compiler.cc b/xla/service/gpu/gpu_compiler.cc
index 43c390e84..37d13b997 100644
--- a/xla/service/gpu/gpu_compiler.cc
+++ b/xla/service/gpu/gpu_compiler.cc
@@ -71,12 +71,12 @@ limitations under the License.
 #include "xla/hlo/ir/hlo_opcode.h"
 #include "xla/hlo/ir/hlo_schedule.h"
 #include "xla/hlo/transforms/hlo_constant_splitter.h"
-#include "xla/mlir/backends/gpu/transforms/passes.h"
-#include "xla/mlir/runtime/transforms/compilation_pipeline_gpu.h"
-#include "xla/mlir/runtime/transforms/compilation_pipeline_options.h"
-#include "xla/runtime/compiler.h"
-#include "xla/runtime/executable.h"
-#include "xla/runtime/jit_executable.h"
+// #include "xla/mlir/backends/gpu/transforms/passes.h"
+// #include "xla/mlir/runtime/transforms/compilation_pipeline_gpu.h"
+// #include "xla/mlir/runtime/transforms/compilation_pipeline_options.h"
+// #include "xla/runtime/compiler.h"
+// #include "xla/runtime/executable.h"
+// #include "xla/runtime/jit_executable.h"
 #include "xla/service/algebraic_simplifier.h"
 #include "xla/service/all_gather_broadcast_reorder.h"
 #include "xla/service/all_gather_combiner.h"
@@ -166,7 +166,7 @@ limitations under the License.
 #include "xla/service/gpu/reduction_splitter.h"
 #include "xla/service/gpu/reduction_utils.h"
 #include "xla/service/gpu/rename_fusions.h"
-#include "xla/service/gpu/runtime/executable.h"
+// #include "xla/service/gpu/runtime/executable.h"
 #include "xla/service/gpu/runtime_intrinsics.h"
 #include "xla/service/gpu/scatter_slice_simplifier.h"
 #include "xla/service/gpu/softmax_rewriter_triton.h"
@@ -265,6 +265,8 @@ limitations under the License.
 #include "xla/hlo/experimental/auto_sharding/auto_sharding.h"
 #endif  // PLATFORM_GOOGLE
 
+#include "xla/service/gpu/dot_expand_dims.h"
+
 namespace xla {
 namespace gpu {
 namespace {
@@ -474,6 +476,7 @@ class GpuThunkAotCompilationResult : public AotCompilationResult {
 absl::StatusOr<std::unique_ptr<Executable>>
 GpuAotCompilationResult::LoadExecutable(
     Compiler* compiler, const se::StreamExecutor* executor) const {
+#if 0
   XlaRuntimeExecutableProto xla_runtime_executable =
       xla_runtime_gpu_executable_.xla_runtime_executable();
   TF_ASSIGN_OR_RETURN(HloModuleConfig hlo_module_config,
@@ -500,6 +503,7 @@ GpuAotCompilationResult::LoadExecutable(
       xla_runtime_gpu_executable_.gpu_asm_text(),
       xla_runtime_gpu_executable_.gpu_binary(), std::move(constants),
       GetGpuVersion(executor));
+#endif
 }
 
 absl::StatusOr<std::unique_ptr<Executable>>
@@ -680,7 +684,9 @@ absl::Status GpuCompiler::OptimizeHloModule(
   layout_insensitive_algsimp_opts
       .set_unconditionally_simplify_reduce_of_transpose_or_reshape(true);
 
-  if (gpu_target_config.platform_name == "ROCM") {
+  // SYCL: Conv swap has accuracy issue in some cases.
+  if (gpu_target_config.platform_name == "ROCM" ||
+      gpu_target_config.platform_name == "SYCL") {
     layout_insensitive_algsimp_opts.set_enable_conv_operand_swap(false);
   }
   layout_insensitive_algsimp_opts
@@ -734,9 +740,10 @@ absl::Status GpuCompiler::OptimizeHloModule(
     spmd_simplify.AddPass<ScatterSimplifier>();
     spmd_simplify.AddPass<ScatterExpander>(
         ScatterExpander::kEliminateSimpleScatters);
-    spmd_simplify.AddPass<GatherSimplifier>();
-    spmd_simplify.AddPass<GatherExpander>(
-        GatherExpander::kEliminateSimpleGathers);
+    // FIXME(intel): Reopen below pass when fix sharding error.
+    // spmd_simplify.AddPass<GatherSimplifier>();
+    // spmd_simplify.AddPass<GatherExpander>(
+    //     GatherExpander::kEliminateSimpleGathers);
     spmd_simplify.AddPass<WhileLoopConstantSinking>();
     spmd_simplify.AddPass<WhileLoopSimplifier>();
 
@@ -1064,6 +1071,8 @@ absl::Status GpuCompiler::OptimizeHloModule(
   se::GpuComputeCapability gpu_version =
       gpu_target_config.device_description.gpu_compute_capability();
   se::dnn::VersionInfo dnn_version = gpu_target_config.dnn_version_info;
+  // SYCL: do not check dnn version
+#if 0
   if (stream_exec != nullptr) {
     gpu_version = GetGpuVersion(stream_exec);
     se::dnn::DnnSupport* dnn = stream_exec->AsDnn();
@@ -1074,7 +1083,7 @@ absl::Status GpuCompiler::OptimizeHloModule(
     }
     TF_ASSIGN_OR_RETURN(dnn_version, dnn->GetVersion());
   }
-
+#endif
   TF_RETURN_IF_ERROR(OptimizeHloConvolutionCanonicalization(
       hlo_module, gpu_version, dnn_version, options.device_allocator));
 
@@ -1088,6 +1097,10 @@ absl::Status GpuCompiler::OptimizeHloModule(
     HloPassPipeline pipeline("layout assignment");
     // Layout assignment uses alias analysis, which requires the call graph to
     // be flattened.
+    // SYCL: LLM passes.
+    bool llm_flag = false;
+    tsl::ReadBoolFromEnvVar("LLM", false, &llm_flag);
+    if (llm_flag) pipeline.AddPass<DotExpandDims>();
     pipeline.AddPass<FlattenCallGraph>();
     ChannelLayoutConstraints layout_constraints;
     pipeline.AddPass<GpuLayoutAssignment>(
@@ -1388,7 +1401,8 @@ absl::Status GpuCompiler::OptimizeHloPostLayoutAssignment(
     const auto* cuda_cc = std::get_if<se::CudaComputeCapability>(&gpu_version);
     if (debug_options.xla_gpu_enable_triton_gemm() && cuda_cc != nullptr &&
         cuda_cc->IsAtLeast(se::CudaComputeCapability::VOLTA)) {
-      pipeline.AddPass<GemmRewriterTriton>(gpu_version);
+      // SYCL: disable triton gemm
+      // pipeline.AddPass<GemmRewriterTriton>(gpu_version);
     }
     pipeline.AddPass<GemmRewriter>(gpu_version);
 
@@ -1410,8 +1424,9 @@ absl::Status GpuCompiler::OptimizeHloPostLayoutAssignment(
     if (debug_options.xla_gpu_enable_triton_softmax_fusion() &&
         cuda_cc != nullptr &&
         cuda_cc->IsAtLeast(se::CudaComputeCapability::VOLTA)) {
-      pipeline.AddPass<HloPassFix<AlgebraicSimplifier>>(simplifier_options);
-      pipeline.AddPass<SoftmaxRewriterTriton>(gpu_version);
+      // SYCL: disable triton
+      // pipeline.AddPass<HloPassFix<AlgebraicSimplifier>>(simplifier_options);
+      // pipeline.AddPass<SoftmaxRewriterTriton>(gpu_version);
     }
 
     pipeline.AddPass<ReductionDimensionGrouper>();
@@ -1742,6 +1757,11 @@ GpuCompiler::CompileSingleModule(const HloModuleConfig& module_config,
 
   // Write PTX to IR dump directory, if IR dumping was requested.
   if (should_dump) {
+    // SYCL: dump spv
+    auto spir_vector = result.binary;
+    std::string spir(spir_vector.begin(), spir_vector.end());
+    DumpToFileInDirOrStdout(*debug_module, "", "spv", spir);
+
     absl::string_view ptx = result.asm_text;
     if (debug_module) {
       DumpToFileInDirOrStdout(*debug_module, "",
@@ -2055,6 +2075,7 @@ absl::StatusOr<std::unique_ptr<Executable>> GpuCompiler::RunBackend(
 absl::StatusOr<std::vector<std::unique_ptr<AotCompilationResult>>>
 GpuCompiler::CompileAheadOfTime(std::unique_ptr<HloModuleGroup> module_group,
                                 const AotCompilationOptions& options) {
+#if 0
 #if GOOGLE_CUDA
   CHECK(options.PlatformId() == se::cuda::kCudaPlatformId);
 #elif TENSORFLOW_USE_ROCM
@@ -2146,6 +2167,7 @@ GpuCompiler::CompileAheadOfTime(std::unique_ptr<HloModuleGroup> module_group,
         res.compile_module_results.constants));
   }
   return std::move(results);
+#endif
 }
 
 HloCostAnalysis::ShapeSizeFunction GpuCompiler::ShapeSizeBytesFunction() const {
@@ -2157,6 +2179,7 @@ HloCostAnalysis::ShapeSizeFunction GpuCompiler::ShapeSizeBytesFunction() const {
 
 absl::StatusOr<std::unique_ptr<AotCompilationResult>> GpuCompiler::Export(
     Executable* executable) const {
+#if 0
   auto* gpu_executable = tensorflow::down_cast<GpuExecutable*>(executable);
   if (!gpu_executable) return Internal("GpuExecutable is null");
 
@@ -2168,6 +2191,8 @@ absl::StatusOr<std::unique_ptr<AotCompilationResult>> GpuCompiler::Export(
   return GpuThunkAotCompilationResult::FromModule(
       &gpu_executable->module(), gpu_executable->buffer_assignment(),
       gpu_executable->text(), gpu_executable->binary());
+#endif
+  LOG(FATAL) << "GpuCompiler::Export is not implemented";
 }
 
 absl::Status GpuCompiler::RunPostSchedulingPipelines(
@@ -2233,12 +2258,16 @@ absl::Status GpuCompiler::RunPostSchedulingPipelines(
     auto driver_version = se::gpu::GpuDriver::GetDriverVersion();
 #if GOOGLE_CUDA
     constexpr int toolkit_version = CUDA_VERSION;
-#else
+#elif TENSORFLOW_USE_ROCM
     constexpr int toolkit_version = TF_ROCM_VERSION;
+#else
+    constexpr int toolkit_version = -1;
 #endif
+#if 0
     pipeline.AddPass<CommandBufferScheduling>(
         gpu_device_info.gpu_compute_capability(), toolkit_version,
         driver_version.value_or(toolkit_version));
+#endif
     TF_RETURN_IF_ERROR(pipeline.Run(module).status());
   }
 
diff --git a/xla/service/gpu/gpu_executable.cc b/xla/service/gpu/gpu_executable.cc
index 70052ceee..380364fa1 100644
--- a/xla/service/gpu/gpu_executable.cc
+++ b/xla/service/gpu/gpu_executable.cc
@@ -41,18 +41,19 @@ limitations under the License.
 #include "xla/executable_run_options.h"
 #include "xla/hlo/ir/hlo_instruction.h"
 #include "xla/map_util.h"
-#include "xla/mlir/runtime/ir/rt_ops.h"
-#include "xla/mlir/runtime/transforms/compilation_pipeline_gpu.h"
-#include "xla/mlir/runtime/transforms/type_converter.h"
-#include "xla/runtime/executable.h"
+// #include "xla/mlir/runtime/ir/rt_ops.h"
+// #include "xla/mlir/runtime/transforms/compilation_pipeline_gpu.h"
+// #include "xla/mlir/runtime/transforms/type_converter.h"
+// #include "xla/runtime/executable.h"
 #include "xla/service/buffer_assignment.h"
+#include "xla/service/gpu/backend_configs.pb.h"
 #include "xla/service/gpu/buffer_allocations.h"
 #include "xla/service/gpu/gpu_constants.h"
 #include "xla/service/gpu/gpu_executable_run_options.h"
 #include "xla/service/gpu/nccl_clique.h"
 #include "xla/service/gpu/nccl_clique_key.h"
 #include "xla/service/gpu/non_atomically_upgradeable_rw_lock.h"
-#include "xla/service/gpu/runtime/executable.h"
+// #include "xla/service/gpu/runtime/executable.h"
 #include "xla/service/gpu/runtime/tracing.h"
 #include "xla/service/gpu/stream_executor_util.h"
 #include "xla/service/gpu/thunk.h"
@@ -137,7 +138,7 @@ absl::StatusOr<std::unique_ptr<GpuExecutable>> GpuExecutable::Create(
     result->thunks_ = std::move(std::get<OwnedThunkSequence>(executable));
     return result;
   }
-
+#if !TENSORFLOW_USE_SYCL
   if (std::holds_alternative<OwnedGpuRuntimeProgram>(executable)) {
     auto& program = std::get<OwnedGpuRuntimeProgram>(executable);
     TF_ASSIGN_OR_RETURN(
@@ -145,6 +146,7 @@ absl::StatusOr<std::unique_ptr<GpuExecutable>> GpuExecutable::Create(
         GpuRuntimeExecutable::Create(result->module_name_, std::move(program)));
     return result;
   }
+#endif
 
   return Internal("No XLA gpu executable was provided");
 }
@@ -207,7 +209,9 @@ absl::Status GpuExecutable::CheckCompatibilityWithServiceExecutableRunOptions(
         << "}, but was {" << std::get<se::CudaComputeCapability>(cc).ToString()
         << "}";
   } else {
+#if !TENSORFLOW_USE_SYCL
     return Internal("Unknown platform");
+#endif
   }
 
   return absl::OkStatus();
@@ -335,6 +339,9 @@ absl::Status ExecuteThunks(
   se::StreamExecutor* executor = main_stream->parent();
   stream_executor::StreamPriority stream_priority =
       stream_executor::StreamPriority::Default;
+#if TENSORFLOW_USE_SYCL
+  use_highest_priority_for_async_stream = false;
+#endif
   if (use_highest_priority_for_async_stream) {
     stream_priority = stream_executor::StreamPriority::Highest;
   }
@@ -589,10 +596,16 @@ GpuExecutable::ResolveConstantGlobals(se::Stream* stream) {
   // The CUDA driver isn't able to load a PTX and a binary which are both empty.
   // It's okay if we skip loading in this case; if the module isn't loaded, all
   // symbol lookups will fail, just as they should for an empty module.
+#if !TENSORFLOW_USE_SYCL
   if (!(executor->platform()->id() == stream_executor::cuda::kCudaPlatformId &&
         binary().empty() && text().empty())) {
     TF_RETURN_IF_ERROR(executor->LoadModule(module_spec, &module_handle));
   }
+#else
+  if (module_spec.has_cuda_cubin_in_memory()) {
+    TF_RETURN_IF_ERROR(executor->LoadModule(module_spec, &module_handle));
+  }
+#endif
 
   // A flag signalling if constant initialization submitted memcpy operations
   // to the `stream`.
@@ -621,6 +634,26 @@ GpuExecutable::ResolveConstantGlobals(se::Stream* stream) {
         submitted_mem_copies = true;
       }
     } else {
+#if TENSORFLOW_USE_SYCL
+      // SYCL: content may be empty
+      if (info.content.span().empty()) {
+        // LLVM module contains the const variable, but it still fails to look
+        // up symbol. So allocate an empty buffer here.
+        void* opaque = nullptr;
+        size_t bytes = 0;
+        global = se::DeviceMemoryBase(opaque, bytes);
+      } else {
+        TF_ASSIGN_OR_RETURN(
+            auto shared, executor->CreateOrShareConstant(stream, info.content.span()));
+        global = *shared;
+        VLOG(3) << "Allocated (or shared) global " << info.symbol_name << " at "
+                << global.opaque();
+        // XLA will continue to own this global at least until this executable
+        // is destroyed (longer if another, longer-lived executable shares the
+        // same constant).
+        shared_constants_.push_back(std::move(shared));
+      }
+#else
       // The constant was not defined in the PTX and therefore must be both
       // allocated and initialized by XLA here.
       CHECK(!info.content.span().empty());
@@ -634,6 +667,7 @@ GpuExecutable::ResolveConstantGlobals(se::Stream* stream) {
       // destroyed (longer if another, longer-lived executable shares the same
       // constant).
       shared_constants_.push_back(std::move(shared));
+#endif
     }
 
     if (info.allocation_index != -1) {
@@ -771,6 +805,7 @@ absl::StatusOr<ScopedShapedBuffer> GpuExecutable::ExecuteAsyncOnStream(
   return out.ConsumeResult();
 }
 
+#if !TENSORFLOW_USE_SYCL
 static absl::Status ExecuteXlaRuntime(
     const std::string& module_name, ModuleIdentifier module_id,
     GpuRuntimeExecutable& gpu_runtime_executable,
@@ -801,6 +836,7 @@ static absl::Status ExecuteXlaRuntime(
       run_options, std::move(execution_timer),
       block_host_until_done ? run_options->stream() : nullptr);
 }
+#endif
 
 absl::StatusOr<ExecutionOutput> GpuExecutable::ExecuteAsyncOnStreamImpl(
     const ServiceExecutableRunOptions* run_options,
@@ -1022,13 +1058,13 @@ absl::Status GpuExecutable::ExecuteThunksOrXlaRuntime(
       if (temp_buffer == nullptr) temp_buffer = &alloc;
     }
   }
-
+#if !TENSORFLOW_USE_SYCL
   if (gpu_runtime_executable_) {
     return ExecuteXlaRuntime(module_name_, unique_id, *gpu_runtime_executable_,
                              run_options, text_, binary_, buffer_allocations,
                              temp_buffer, block_host_until_done, gpu_lock);
   }
-
+#endif
   return FailedPrecondition("Expected XLA gpu executable is not supplied.");
 }
 
@@ -1163,7 +1199,7 @@ GetOutputInfo(const HloModule& hlo_module, const BufferAssignment& assignment) {
       }));
   return output;
 }
-
+#if !TENSORFLOW_USE_SYCL
 GpuExecutable::GpuExecutable(
     std::shared_ptr<HloModule> hlo_module, std::string asm_text,
     std::vector<uint8_t> binary, std::vector<ConstantInfo> constants,
@@ -1368,6 +1404,6 @@ absl::StatusOr<std::string_view> GpuExecutable::GetMlirModule() const {
     return Internal("gpu_runtime_executable is null");
   return gpu_runtime_executable_->GetMlirModule();
 }
-
+#endif
 }  // namespace gpu
 }  // namespace xla
diff --git a/xla/service/gpu/gpu_executable.h b/xla/service/gpu/gpu_executable.h
index 5a4ce3f0e..9386f11a8 100644
--- a/xla/service/gpu/gpu_executable.h
+++ b/xla/service/gpu/gpu_executable.h
@@ -40,7 +40,7 @@ limitations under the License.
 #include "xla/service/gpu/ir_emission_utils.h"
 #include "xla/service/gpu/non_atomically_upgradeable_rw_lock.h"
 #include "xla/service/gpu/runtime/annotation.h"
-#include "xla/service/gpu/runtime/executable.h"
+// #include "xla/service/gpu/runtime/executable.h"
 #include "xla/service/gpu/thunk.h"
 #include "xla/service/hlo_execution_profile.h"
 #include "xla/service/rendezvous.h"
@@ -56,6 +56,25 @@ namespace gpu {
 // Returns whether GpuExecutable runs with Xla Runtime.
 bool IsXlaRuntimeExecutableEnabled(const HloModuleConfig& config);
 
+// SYCL: dummy GpuRuntimeProgram for compilation
+#if TENSORFLOW_USE_SYCL
+struct GpuRuntimeProgram {
+  GpuRuntimeProgram(std::string entry_point, std::string module,
+                    std::vector<int64_t> buffer_sizes,
+                    DebugOptions debug_options)
+      : entry_point(std::move(entry_point)),
+        module(std::move(module)),
+        buffer_sizes(std::move(buffer_sizes)),
+        debug_options(std::move(debug_options)) {}
+
+  std::string entry_point;
+  std::string module;
+  std::vector<int64_t> buffer_sizes;
+  DebugOptions debug_options;
+};
+class GpuRuntimeExecutable {};
+#endif
+
 // GPU-targeting implementation of the XLA Executable interface.
 //
 // Launches the given GPU kernel via the StreamExecutor.
@@ -113,7 +132,7 @@ class GpuExecutable : public Executable {
       std::vector<BufferAllocation>* allocations,
       absl::flat_hash_map<ShapeIndex, OutputInfo>* output_info,
       Shape* output_shape);
-
+#if !TENSORFLOW_USE_SYCL
   // Returns an Executable that is loaded from an object file (XLA program
   // compiled to a native function using the XLA Runtime stack).
   static absl::StatusOr<std::unique_ptr<Executable>> LoadFromObjFile(
@@ -122,7 +141,7 @@ class GpuExecutable : public Executable {
       absl::string_view asm_text, absl::string_view binary,
       std::vector<ConstantInfo> constants,
       se::GpuComputeCapability gpu_version);
-
+#endif
   // Constructor to use when loading a GpuExecutable from an object file (native
   // function compiled for XLA Runtime). Omits setting class members that aren't
   // used in XLA Runtime execution mode.
diff --git a/xla/service/gpu/gpu_fused_mha_runner.cc b/xla/service/gpu/gpu_fused_mha_runner.cc
index 889d2e5bc..a59e092d2 100644
--- a/xla/service/gpu/gpu_fused_mha_runner.cc
+++ b/xla/service/gpu/gpu_fused_mha_runner.cc
@@ -155,6 +155,8 @@ void AssignScale(GpufMHAConfig &config,
   double fmha_scale = 0.0;
 
   switch (config.kind) {
+    // SYCL: supports bias + softmax
+    case CudnnfMHAKind::kSoftmax:
     case CudnnfMHAKind::kScaleBiasMaskSoftmax:
     case CudnnfMHAKind::kScaleBiasMaskSoftmaxDropout:
     case CudnnfMHAKind::kScaleMaskSoftmax:
diff --git a/xla/service/gpu/gpu_fusible.cc b/xla/service/gpu/gpu_fusible.cc
index 2b880f0ab..fefde6520 100644
--- a/xla/service/gpu/gpu_fusible.cc
+++ b/xla/service/gpu/gpu_fusible.cc
@@ -440,6 +440,13 @@ FusionDecision IsProducerConsumerFusible(const HloInstruction& producer,
   }
 
   if (IsInputFusibleReduction(producer)) {
+#if TENSORFLOW_USE_SYCL
+    // TODO: Check on latest XLA. Reduction epilogue fusion may cause
+    // regression for row reductions cases with large dim.
+    // It changes fusion kind from kInput to kLoop, and
+    // will fail to enter the row vectorization pass.
+    return "Reduction epilogue fusion is not enabled.";
+#endif
     if (!producer.GetModule()
              ->config()
              .debug_options()
diff --git a/xla/service/gpu/gpu_layout_assignment.cc b/xla/service/gpu/gpu_layout_assignment.cc
index c69d218c6..a0c242f94 100644
--- a/xla/service/gpu/gpu_layout_assignment.cc
+++ b/xla/service/gpu/gpu_layout_assignment.cc
@@ -86,7 +86,7 @@ HeuristicLayoutAssignment(const HloInstruction* instr,
       std::make_tuple(DataLayout::kBatchDepthYX4, FilterLayout::kOutputInputYX4,
                       DataLayout::kBatchDepthYX4);
   constexpr auto kAllNHWC =
-      std::make_tuple(DataLayout::kBatchYXDepth, FilterLayout::kOutputYXInput,
+      std::make_tuple(DataLayout::kBatchYXDepth, FilterLayout::kYXInputOutput,
                       DataLayout::kBatchYXDepth);
 
   // Integer convolution must use NHWC or NCHW_VECT_C.
diff --git a/xla/service/gpu/gpu_sanitize_constant_names.cc b/xla/service/gpu/gpu_sanitize_constant_names.cc
index c8ef2d60b..69fdfd096 100644
--- a/xla/service/gpu/gpu_sanitize_constant_names.cc
+++ b/xla/service/gpu/gpu_sanitize_constant_names.cc
@@ -17,11 +17,11 @@ limitations under the License.
 
 #include <string>
 
+#include "tsl/platform/logging.h"
+#include "tsl/platform/status.h"
 #include "xla/hlo/ir/hlo_instruction.h"
 #include "xla/hlo/ir/hlo_opcode.h"
 #include "xla/service/llvm_ir/buffer_assignment_util.h"
-#include "tsl/platform/logging.h"
-#include "tsl/platform/status.h"
 
 namespace xla {
 
@@ -39,8 +39,9 @@ absl::StatusOr<bool> GpuSanitizeConstantNames::Run(
       if (instr->opcode() == HloOpcode::kConstant) {
         continue;
       }
-
-      instr->UniquifyName(&instr_name_uniquer);
+      // Record the non-constant HLO instruction name in uniquer, and keep
+      // original instruction name unchanged.
+      instr_name_uniquer.GetUniqueName(instr->name());
     }
   }
 
diff --git a/xla/service/gpu/gpu_transfer_manager.cc b/xla/service/gpu/gpu_transfer_manager.cc
index 4ef4ece8d..1ca8d7352 100644
--- a/xla/service/gpu/gpu_transfer_manager.cc
+++ b/xla/service/gpu/gpu_transfer_manager.cc
@@ -36,6 +36,7 @@ limitations under the License.
 #include "xla/stream_executor/host/host_platform_id.h"
 #include "xla/stream_executor/multi_platform_manager.h"
 #include "xla/stream_executor/rocm/rocm_platform_id.h"
+#include "xla/stream_executor/sycl/sycl_platform_id.h"
 #include "xla/stream_executor/stream_executor.h"
 #include "xla/types.h"
 #include "xla/util.h"
@@ -208,11 +209,20 @@ static std::unique_ptr<xla::TransferManager> CreateAMDGPUTransferManager() {
           .getPointerSize(0 /* default address space */));
 }
 
+static std::unique_ptr<xla::TransferManager> CreateSYCLTransferManager() {
+  return std::make_unique<xla::gpu::GpuTransferManager>(
+      /*id=*/stream_executor::sycl::kSyclPlatformId,
+      /*pointer_size=*/llvm::DataLayout(xla::gpu::spir::DataLayout())
+          .getPointerSize(0 /* default address space */));
+}
+
 static bool InitModule() {
   xla::TransferManager::RegisterTransferManager(
       stream_executor::cuda::kCudaPlatformId, &CreateNVPTXTransferManager);
   xla::TransferManager::RegisterTransferManager(
       stream_executor::rocm::kROCmPlatformId, &CreateAMDGPUTransferManager);
+  xla::TransferManager::RegisterTransferManager(
+      stream_executor::sycl::kSyclPlatformId, &CreateSYCLTransferManager);
   return true;
 }
 static bool module_initialized = InitModule();
diff --git a/xla/service/gpu/ir_emission_utils.cc b/xla/service/gpu/ir_emission_utils.cc
index ca299d71d..079518a9f 100644
--- a/xla/service/gpu/ir_emission_utils.cc
+++ b/xla/service/gpu/ir_emission_utils.cc
@@ -130,11 +130,11 @@ bool IsMatrixMultiplication(const HloInstruction& dot) {
   const DotDimensionNumbers& dim_numbers = dot.dot_dimension_numbers();
 
   PrimitiveType output_primitive_type = dot.shape().element_type();
+  // Disable F64, C64, C128
   bool type_is_allowed =
       (output_primitive_type == F8E4M3FN || output_primitive_type == F8E5M2 ||
        output_primitive_type == F16 || output_primitive_type == BF16 ||
-       output_primitive_type == F32 || output_primitive_type == F64 ||
-       output_primitive_type == C64 || output_primitive_type == C128) ||
+       output_primitive_type == F32) ||
       (output_primitive_type == S32 && lhs_shape.element_type() == S8 &&
        rhs_shape.element_type() == S8);
   bool shapes_are_valid =
diff --git a/xla/service/gpu/ir_emitter_context.cc b/xla/service/gpu/ir_emitter_context.cc
index f09520eae..762a4ca0c 100644
--- a/xla/service/gpu/ir_emitter_context.cc
+++ b/xla/service/gpu/ir_emitter_context.cc
@@ -22,6 +22,7 @@ limitations under the License.
 
 #include "absl/algorithm/container.h"
 #include "llvm/ADT/ArrayRef.h"
+#include "llvm/TargetParser/Triple.h"
 #include "xla/service/gpu/gpu_constants.h"
 #include "xla/service/gpu/ir_emission_utils.h"
 
@@ -57,6 +58,8 @@ void IrEmitterContext::emit_constant(int64_t num_elements,
       return llvm::ConstantAggregateZero::get(global_type);
     }
 
+    // SYCL: always set info.content.
+    info.content = content;
     std::vector<uint8_t> padded(kMinConstAllocationInBytes, 0);
     absl::c_copy(content.span(), padded.begin());
     return llvm::ConstantDataArray::get<uint8_t>(
@@ -74,14 +77,38 @@ void IrEmitterContext::emit_constant(int64_t num_elements,
   //
   // We may have to be more clever here in the future if we notice that we're
   // keeping around too many globals because of their linkage.
+  // SYCL: Hardcode to global addrspace
+  bool is_spir = llvm::Triple(llvm_module_->getTargetTriple()).isSPIR();
+  int addrspace = is_spir ? 1 : 0;
   llvm::GlobalVariable* global_for_const = new llvm::GlobalVariable(
       global_type, /*isConstant=*/should_emit_initializer,
       llvm::GlobalValue::ExternalLinkage,
       /*Initializer=*/initializer, symbol_name,
       /*TLMode=*/llvm::GlobalValue::NotThreadLocal,
-      /*AddressSpace=*/0,
+      /*AddressSpace=*/addrspace,
       /*isExternallyInitialized=*/false);
   global_for_const->setAlignment(llvm::Align(kConstantBufferAlignBytes));
+
+  if (is_spir) {
+    // SYCL: Add spirv.Decorations for global variable. See document about the
+    // annotation:
+    // https://github.com/intel/llvm/blob/sycl/sycl/doc/design/spirv-extensions/SPV_INTEL_global_variable_decorations.asciidoc
+    llvm::LLVMContext& context = llvm_module_->getContext();
+    llvm::SmallVector<llvm::Metadata*, 4> metadatas;
+    std::vector<llvm::Metadata*> ops;
+
+    auto* kind = llvm::ConstantAsMetadata::get(llvm::ConstantInt::get(
+        llvm::Type::getInt32Ty(context), /*IDecHostAccessINTEL*/ 6147));
+    ops.push_back(kind);
+    auto* const acc_mode = llvm::ConstantAsMetadata::get(llvm::ConstantInt::get(
+        llvm::Type::getInt32Ty(context), /*AccessMode*/ 2));
+    ops.push_back(acc_mode);
+    ops.push_back(llvm::MDString::get(context, symbol_name));
+    metadatas.push_back(llvm::MDNode::get(context, ops));
+
+    llvm::MDNode* md_list = llvm::MDNode::get(context, metadatas);
+    global_for_const->setMetadata("spirv.Decorations", md_list);
+  }
   llvm_module_->insertGlobalVariable(global_for_const);
 
   info.symbol_name.assign(symbol_name);
diff --git a/xla/service/gpu/ir_emitter_nested.cc b/xla/service/gpu/ir_emitter_nested.cc
index 3a54197f2..98a8e39e1 100644
--- a/xla/service/gpu/ir_emitter_nested.cc
+++ b/xla/service/gpu/ir_emitter_nested.cc
@@ -37,6 +37,11 @@ limitations under the License.
 #include "xla/service/llvm_ir/llvm_util.h"
 #include "xla/service/llvm_ir/tuple_ops.h"
 
+#if TENSORFLOW_USE_SYCL
+#include "xla/service/gpu/ir_emission_utils.h"
+#include "llvm/TargetParser/Triple.h"
+#endif
+
 namespace xla {
 namespace gpu {
 namespace {
@@ -299,7 +304,8 @@ void EmitAMDGPUAtomicAdd(llvm::IRBuilder<>* builder,
 
   builder->CreateAtomicRMW(
       llvm::AtomicRMWInst::FAdd, output_ptr, source, llvm::MaybeAlign(),
-      llvm::AtomicOrdering::SequentiallyConsistent,
+      // SYCL: set Monotonic.
+      llvm::AtomicOrdering::Monotonic,
       builder->getContext().getOrInsertSyncScopeID("agent"));
 }
 
@@ -367,7 +373,7 @@ bool MaybeEmitDirectAtomicOperation(llvm::IRBuilder<>* builder,
       if (atomic_add_supported) {
         builder->CreateAtomicRMW(llvm::AtomicRMWInst::FAdd, output_address,
                                  source, llvm::MaybeAlign(),
-                                 llvm::AtomicOrdering::SequentiallyConsistent);
+                                 llvm::AtomicOrdering::Monotonic);
         return true;
       }
     }
@@ -381,11 +387,19 @@ bool MaybeEmitDirectAtomicOperation(llvm::IRBuilder<>* builder,
       return true;
     }
 
+    if (target_triple.isSPIR() &&
+        element_type == F32) {
+      builder->CreateAtomicRMW(llvm::AtomicRMWInst::FAdd, output_address,
+                               source, llvm::MaybeAlign(),
+                               llvm::AtomicOrdering::Monotonic);
+      return true;
+    }
+
     if (is_atomic_integral) {
       // integral + integral
       builder->CreateAtomicRMW(
           llvm::AtomicRMWInst::Add, output_address, source, llvm::MaybeAlign(),
-          llvm::AtomicOrdering::SequentiallyConsistent, sync_scope);
+          llvm::AtomicOrdering::Monotonic, sync_scope);
       return true;
     }
   }
@@ -402,7 +416,7 @@ bool MaybeEmitDirectAtomicOperation(llvm::IRBuilder<>* builder,
                         : llvm::AtomicRMWInst::UMax;
       builder->CreateAtomicRMW(
           opcode, output_address, source, llvm::MaybeAlign(),
-          llvm::AtomicOrdering::SequentiallyConsistent, sync_scope);
+          llvm::AtomicOrdering::Monotonic, sync_scope);
       return true;
     } else if (element_type == F32) {
       // max(float, float) via AtomicMax and AtomicMin on int
@@ -455,7 +469,7 @@ bool MaybeEmitDirectAtomicOperation(llvm::IRBuilder<>* builder,
                     builder->CreateAtomicRMW(
                         llvm::AtomicRMWInst::Max, output_address,
                         source_float_as_int, llvm::MaybeAlign(),
-                        llvm::AtomicOrdering::SequentiallyConsistent,
+                        llvm::AtomicOrdering::Monotonic,
                         sync_scope);
                   },
                   [&]() {
@@ -463,7 +477,7 @@ bool MaybeEmitDirectAtomicOperation(llvm::IRBuilder<>* builder,
                     builder->CreateAtomicRMW(
                         llvm::AtomicRMWInst::UMin, output_address,
                         source_float_as_int, llvm::MaybeAlign(),
-                        llvm::AtomicOrdering::SequentiallyConsistent,
+                        llvm::AtomicOrdering::Monotonic,
                         sync_scope);
                   });
             });
@@ -479,7 +493,7 @@ bool MaybeEmitDirectAtomicOperation(llvm::IRBuilder<>* builder,
                       ? llvm::AtomicRMWInst::Min
                       : llvm::AtomicRMWInst::UMin;
     builder->CreateAtomicRMW(opcode, output_address, source, llvm::MaybeAlign(),
-                             llvm::AtomicOrdering::SequentiallyConsistent,
+                             llvm::AtomicOrdering::Monotonic,
                              sync_scope);
     return true;
   }
@@ -648,8 +662,8 @@ absl::Status EmitAtomicOperationUsingCAS(llvm::IRBuilder<>* builder,
   //                                       cas_new_output);
   llvm::Value* ret_value = builder->CreateAtomicCmpXchg(
       atomic_memory_address, cas_old_output, cas_new_output, llvm::MaybeAlign(),
-      llvm::AtomicOrdering::SequentiallyConsistent,
-      llvm::AtomicOrdering::SequentiallyConsistent, DetermineSyncScope(module));
+      llvm::AtomicOrdering::Monotonic,
+      llvm::AtomicOrdering::Monotonic, DetermineSyncScope(module));
 
   // Extract the memory value returned from atomicCAS and store it as
   // cas_old_output.
diff --git a/xla/service/gpu/ir_emitter_unnested.cc b/xla/service/gpu/ir_emitter_unnested.cc
index f3015e286..23ae84bd3 100644
--- a/xla/service/gpu/ir_emitter_unnested.cc
+++ b/xla/service/gpu/ir_emitter_unnested.cc
@@ -1,4 +1,6 @@
-/*Copyright 2022 The OpenXLA Authors.
+/* Copyright (c) 2024 Intel Corporation
+
+Copyright 2019 The TensorFlow Authors. All Rights Reserved.
 
 Licensed under the Apache License, Version 2.0 (the "License");
 you may not use this file except in compliance with the License.
@@ -116,16 +118,26 @@ limitations under the License.
 #include "xla/service/gpu/kernels/topk_custom_kernel.h"
 #include "xla/service/gpu/launch_dimensions.h"
 #include "xla/service/gpu/matmul_utils.h"
-#include "xla/service/gpu/nccl_all_to_all_thunk.h"
-#include "xla/service/gpu/nccl_api.h"
-#include "xla/service/gpu/nccl_collective_permute_thunk.h"
-#include "xla/service/gpu/nccl_collective_thunk.h"
-#include "xla/service/gpu/nccl_recv_thunk.h"
-#include "xla/service/gpu/nccl_send_thunk.h"
+// #include "xla/service/gpu/nccl_all_to_all_thunk.h"
+// #include "xla/service/gpu/nccl_api.h"
+// #include "xla/service/gpu/nccl_collective_permute_thunk.h"
+// #include "xla/service/gpu/nccl_collective_thunk.h"
+// #include "xla/service/gpu/runtime3/nccl_all_gather_thunk.h"
+// #include "xla/service/gpu/runtime3/nccl_all_reduce_thunk.h"
+// #include "xla/service/gpu/nccl_recv_thunk.h"
+// #include "xla/service/gpu/nccl_send_thunk.h"
+#include "xla/service/gpu/ccl_all_to_all_thunk.h"
+// #include "xla/service/gpu/nccl_api.h"
+#include "xla/service/gpu/ccl_collective_permute_thunk.h"
+#include "xla/service/gpu/ccl_collective_thunk.h"
+#include "xla/service/gpu/ccl_all_gather_thunk.h"
+#include "xla/service/gpu/ccl_all_reduce_thunk.h"
+// #include "xla/service/gpu/ccl_recv_thunk.h"
+// #include "xla/service/gpu/ccl_send_thunk.h"
 #include "xla/service/gpu/parallel_loop_emitter.h"
-#include "xla/service/gpu/runtime3/command_buffer_cmd.h"
-#include "xla/service/gpu/runtime3/command_buffer_cmd_emitter.h"
-#include "xla/service/gpu/runtime3/command_buffer_thunk.h"
+// #include "xla/service/gpu/runtime3/command_buffer_cmd.h"
+// #include "xla/service/gpu/runtime3/command_buffer_cmd_emitter.h"
+// #include "xla/service/gpu/runtime3/command_buffer_thunk.h"
 #include "xla/service/gpu/runtime3/conditional_thunk.h"
 #include "xla/service/gpu/runtime3/convolution_thunk.h"
 #include "xla/service/gpu/runtime3/copy_thunk.h"
@@ -135,8 +147,6 @@ limitations under the License.
 #include "xla/service/gpu/runtime3/gemm_thunk.h"
 #include "xla/service/gpu/runtime3/infeed_thunk.h"
 #include "xla/service/gpu/runtime3/kernel_thunk.h"
-#include "xla/service/gpu/runtime3/nccl_all_gather_thunk.h"
-#include "xla/service/gpu/runtime3/nccl_all_reduce_thunk.h"
 #include "xla/service/gpu/runtime3/norm_thunk.h"
 #include "xla/service/gpu/runtime3/outfeed_thunk.h"
 #include "xla/service/gpu/runtime3/replica_id_thunk.h"
@@ -172,16 +182,16 @@ limitations under the License.
 #include "tsl/platform/statusor.h"
 #include "tsl/protobuf/dnn.pb.h"
 
-#if GOOGLE_CUDA || TF_HIPBLASLT
+#if GOOGLE_CUDA || TF_HIPBLASLT || TENSORFLOW_USE_SYCL
 #include "xla/service/gpu/runtime3/gpublas_lt_matmul_thunk.h"
-#endif  // GOOGLE_CUDA || TF_HIPBLASLT
+#endif  // GOOGLE_CUDA || TF_HIPBLASLT || TENSORFLOW_USE_SYCL
 
-#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM
-#include "xla/service/gpu/ir_emitter_triton.h"
+#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM || TENSORFLOW_USE_SYCL
+// #include "xla/service/gpu/ir_emitter_triton.h"
 #include "xla/service/gpu/runtime3/cholesky_thunk.h"
-#include "xla/service/gpu/runtime3/cub_sort_thunk.h"
+// #include "xla/service/gpu/runtime3/cub_sort_thunk.h"
 #include "xla/service/gpu/runtime3/triangular_solve_thunk.h"
-#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM
+#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM || TENSORFLOW_USE_SYCL
 
 namespace xla {
 namespace gpu {
@@ -753,16 +763,19 @@ absl::Status IrEmitterUnnested::EmitCommandBufferThunk(
   // computation. Then convert emitted thunks to a sequence of CommandBufferCmd.
   // The resulting thunk added to the thunk sequence is a CommandBufferThunk.
   // Thunks emitted from the command buffer computation are discarded.
+  /*
   DCHECK_EQ(instr->called_computations().size(), 1);
   const HloComputation* command_buffer = instr->called_computations().front();
   auto ir_emitter = IrEmitterUnnested::Create(ir_emitter_context_);
   TF_RETURN_IF_ERROR(ir_emitter->EmitHloComputation(command_buffer));
   std::unique_ptr<ThunkSequence> thunk_sequence =
       ir_emitter->ConsumeThunkSequence();
+  */
 
   // Linearize all commands in a sequence by forcing barriers between all
   // recorded commands. This guarantees that we execute all device operations
   // in the exact same order as a thunk sequence.
+  /*
   bool force_barriers = !ir_emitter_context_->debug_options()
                              .xla_gpu_graph_enable_concurrent_region();
 
@@ -771,7 +784,7 @@ absl::Status IrEmitterUnnested::EmitCommandBufferThunk(
   AddThunkToThunkSequence(std::make_unique<CommandBufferThunk>(
       std::move(cmd_sequence), Thunk::ThunkInfo::WithProfileAnnotation(instr),
       std::move(*thunk_sequence)));
-
+  */
   return absl::OkStatus();
 }
 
@@ -813,10 +826,32 @@ absl::Status IrEmitterUnnested::EmitConvolutionThunk(
                                   instr->convolution_dimension_numbers(),
                                   instr->feature_group_count()};
 
-  TF_ASSIGN_OR_RETURN(GpuConvConfig config, GetGpuConvConfig(descriptor, ""));
-  AddThunkToThunkSequence(std::make_unique<ConvolutionThunk>(
-      Thunk::ThunkInfo::WithProfileAnnotation(instr), std::move(config),
+  ThunkSequence thunks;
+  if (kind == CudnnConvKind::kForwardActivation) {
+    // SYCL: OneDNN requires inplace sum
+    if (operand_slices.size() > 3 && operand_slices[3] != result_slices[0]) {
+      thunks.push_back(std::make_unique<DeviceToDeviceCopyThunk>(
+          Thunk::ThunkInfo::WithProfileAnnotation(instr),
+          /*side_input_buffer=*/operand_slices[3],
+          /*destination_buffer=*/result_slices[0],
+          /*mem_size=*/ShapeUtil::ByteSizeOf(instr->operand(3)->shape()),
+          /*source_value=*/nullptr,
+          /*destination_value=*/nullptr));
+    }
+  }
+
+  // SYCL: use descriptor for sycl conv
+  // TF_ASSIGN_OR_RETURN(GpuConvConfig config, GetGpuConvConfig(descriptor, ""));
+  thunks.push_back(std::make_unique<ConvolutionThunk>(
+      Thunk::ThunkInfo::WithProfileAnnotation(instr), std::move(descriptor),
       std::move(operand_slices), std::move(result_slices), scratch_slice));
+  if (thunks.size() == 1) {
+    AddThunkToThunkSequence(std::move(thunks[0]));
+  } else {
+    AddThunkToThunkSequence(std::make_unique<SequentialThunk>(
+        Thunk::ThunkInfo::WithProfileAnnotation(instr), std::move(thunks)));
+  }
+
   return OkStatus();
 }
 
@@ -930,6 +965,7 @@ absl::Status IrEmitterUnnested::EmitConvolutionThunk(mlir::Operation* op) {
     return absl::OkStatus();
   };
 
+  ThunkSequence thunks;
   if (auto conv = dyn_cast<ConvForwardOp>(op)) {
     descriptor.kind = CudnnConvKind::kForward;
     fill_conv_descriptor(conv);
@@ -956,13 +992,31 @@ absl::Status IrEmitterUnnested::EmitConvolutionThunk(mlir::Operation* op) {
     TF_RETURN_IF_ERROR(set_activation_mode(conv));
     descriptor.backend_config.set_side_input_scale(
         conv.getSideInputScale().convertToDouble());
+
+    // SYCL: OneDNN requires inplace sum
+    if (operand_slices[3] != result_slices[0]) {
+      thunks.push_back(std::make_unique<DeviceToDeviceCopyThunk>(
+          Thunk::ThunkInfo::WithProfileAnnotation(op),
+          /*side_input_buffer=*/operand_slices[3],
+          /*destination_buffer=*/result_slices[0],
+          /*mem_size=*/ShapeUtil::ByteSizeOf(GetShape(op->getOperand(3))),
+          /*source_value=*/op->getOperand(3),
+          /*destination_value=*/op->getOperand(4)));
+    }
   } else {
     return Internal("EmitConvolutionThunk: Unexpected operation");
   }
-  TF_ASSIGN_OR_RETURN(GpuConvConfig config, GetGpuConvConfig(descriptor, ""));
-  AddThunkToThunkSequence(std::make_unique<ConvolutionThunk>(
-      Thunk::ThunkInfo::WithProfileAnnotation(op), std::move(config),
+  // SYCL: use descriptor for sycl conv
+  // TF_ASSIGN_OR_RETURN(GpuConvConfig config, GetGpuConvConfig(descriptor, ""));
+  thunks.push_back(std::make_unique<ConvolutionThunk>(
+      Thunk::ThunkInfo::WithProfileAnnotation(op), std::move(descriptor),
       std::move(operand_slices), std::move(result_slices), scratch_slice));
+  if (thunks.size() == 1) {
+    AddThunkToThunkSequence(std::move(thunks[0]));
+  } else {
+    AddThunkToThunkSequence(std::make_unique<SequentialThunk>(
+        Thunk::ThunkInfo::WithProfileAnnotation(op), std::move(thunks)));
+  }
   return absl::OkStatus();
 }
 
@@ -1018,7 +1072,7 @@ absl::Status IrEmitterUnnested::EmitGemmThunk(
   return absl::OkStatus();
 }
 
-#if GOOGLE_CUDA || TF_HIPBLASLT
+#if GOOGLE_CUDA || TF_HIPBLASLT || TENSORFLOW_USE_SYCL
 
 absl::Status IrEmitterUnnested::EmitCublasLtMatmulThunk(
     const HloCustomCallInstruction* instr) {
@@ -1115,8 +1169,9 @@ absl::Status IrEmitterUnnested::EmitCublasLtMatmulThunk(mlir::Operation* op) {
   AddThunkToThunkSequence(std::move(thunk));
   return absl::OkStatus();
 }
-#endif  // GOOGLE_CUDA || TF_HIPBLASLT
+#endif  // GOOGLE_CUDA || TF_HIPBLASLT || TENSORFLOW_USE_SYCL
 
+#if GOOGLE_CUDA || TENSORFLOW_USE_SYCL
 #if GOOGLE_CUDA
 
 absl::Status IrEmitterUnnested::EmitCublasLtMatmulThunkF8(
@@ -1437,6 +1492,7 @@ absl::Status IrEmitterUnnested::EmitNormThunk(mlir::Operation* op) {
   return absl::OkStatus();
 }
 
+#endif  // GOOGLE_CUDA
 absl::Status IrEmitterUnnested::EmitFusedMHAThunk(mlir::Operation* op) {
   using mlir::dyn_cast;
   using mlir::lmhlo_gpu::fusedMHAOp;
@@ -1565,6 +1621,7 @@ absl::Status IrEmitterUnnested::EmitFusedMHAThunk(mlir::Operation* op) {
   return absl::OkStatus();
 }
 
+#if GOOGLE_CUDA
 absl::Status IrEmitterUnnested::EmitFusedMHABackwardThunk(mlir::Operation* op) {
   using mlir::dyn_cast;
   using mlir::lmhlo_gpu::fusedMHABackwardOp;
@@ -1778,6 +1835,7 @@ absl::Status IrEmitterUnnested::EmitFusedMHABackwardThunk(mlir::Operation* op) {
   return absl::OkStatus();
 }
 #endif  // GOOGLE_CUDA
+#endif  // GOOGLE_CUDA || TENSORFLOW_USE_SYCL
 
 absl::StatusOr<BufferAllocation::Slice>
 IrEmitterUnnested::GetAllocationSliceForHlo(const HloInstruction* instr,
@@ -1786,8 +1844,8 @@ IrEmitterUnnested::GetAllocationSliceForHlo(const HloInstruction* instr,
                                       instr, index);
 }
 
-#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM
-
+#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM || TENSORFLOW_USE_SYCL
+#if 0
 absl::Status IrEmitterUnnested::EmitCubDeviceRadixSort(
     const HloCustomCallInstruction* instr) {
   if (instr->operand_count() != 1 && instr->operand_count() != 2) {
@@ -1858,7 +1916,7 @@ absl::Status IrEmitterUnnested::EmitCubDeviceRadixSort(mlir::Operation* op) {
   AddThunkToThunkSequence(std::move(thunk));
   return absl::OkStatus();
 }
-
+#endif
 absl::Status IrEmitterUnnested::EmitCholeskyThunk(mlir::Operation* op) {
   auto cholesky_op = mlir::cast<mlir::lmhlo_gpu::CholeskyOp>(op);
 
@@ -1960,7 +2018,7 @@ absl::Status IrEmitterUnnested::EmitCholeskyThunk(const HloInstruction* instr) {
 
   return absl::OkStatus();
 }
-#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM
+#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM || TENSORFLOW_USE_SYCL
 
 // Converts MLIR dictionary attribute attached to a custom call operation to a
 // custom call thunk attributes that are forwarded to the FFI handler.
@@ -2398,7 +2456,7 @@ absl::Status IrEmitterUnnested::EmitFftThunk(const HloFftInstruction* instr) {
   return absl::OkStatus();
 }
 
-#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM
+#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM || TENSORFLOW_USE_SYCL
 absl::Status IrEmitterUnnested::EmitTriangularSolveCustomCall(
     mlir::Operation* op) {
   auto custom_call = mlir::cast<mlir::lmhlo::CustomCallOp>(op);
@@ -2568,7 +2626,7 @@ absl::Status IrEmitterUnnested::EmitTriangularSolveCustomCall(
   }
   return absl::OkStatus();
 }
-#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM
+#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM || TENSORFLOW_USE_SYCL
 
 absl::Status IrEmitterUnnested::EmitTopKCustomCall(
     const HloCustomCallInstruction* instr) {
@@ -4108,33 +4166,33 @@ static std::optional<GlobalDeviceId> DeviceConstraint(
 }
 
 absl::Status IrEmitterUnnested::EmitSendThunk(const HloSendInstruction* instr) {
-  if (!instr->channel_id().has_value())
-    return absl::InternalError("Unknown send instruction channel id");
-
-  const HloInstruction* src = instr->operand(0);
-  TF_ASSIGN_OR_RETURN(BufferAllocation::Slice buffer,
-                      GetAllocationSliceForHlo(src, {}));
-  if (!instr->is_host_transfer()) {
-    const auto& hlo_config = ir_emitter_context_->hlo_module().config();
-    const int64_t replica_count = hlo_config.replica_count();
-    const int64_t partition_count = hlo_config.num_partitions();
-    const NcclCollectiveThunk::Buffer nccl_buffer = {
-        /*element_count=*/ShapeUtil::ElementsIn(src->shape()),
-        /*source_buffer=*/buffer,
-        /*destination_buffer=*/buffer};
-    auto thunk = std::make_unique<NcclSendThunk>(
-        Thunk::ThunkInfo::WithProfileAnnotation(instr), NcclApi::Default(),
-        instr, replica_count, partition_count, nccl_buffer);
-    collectives_async_events_.try_emplace(instr, thunk->async_events());
-    AddThunkToThunkSequence(std::move(thunk));
-    return absl::OkStatus();
-  }
+  // if (!instr->channel_id().has_value())
+  //   return absl::InternalError("Unknown send instruction channel id");
+
+  // const HloInstruction* src = instr->operand(0);
+  // TF_ASSIGN_OR_RETURN(BufferAllocation::Slice buffer,
+  //                     GetAllocationSliceForHlo(src, {}));
+  // if (!instr->is_host_transfer()) {
+  //   const auto& hlo_config = ir_emitter_context_->hlo_module().config();
+  //   const int64_t replica_count = hlo_config.replica_count();
+  //   const int64_t partition_count = hlo_config.num_partitions();
+  //   const NcclCollectiveThunk::Buffer nccl_buffer = {
+  //       /*element_count=*/ShapeUtil::ElementsIn(src->shape()),
+  //       /*source_buffer=*/buffer,
+  //       /*destination_buffer=*/buffer};
+  //   auto thunk = std::make_unique<NcclSendThunk>(
+  //       Thunk::ThunkInfo::WithProfileAnnotation(instr), NcclApi::Default(),
+  //       instr, replica_count, partition_count, nccl_buffer);
+  //   collectives_async_events_.try_emplace(instr, thunk->async_events());
+  //   AddThunkToThunkSequence(std::move(thunk));
+  //   return absl::OkStatus();
+  // }
 
-  AddThunkToThunkSequence(std::make_unique<SendThunk>(
-      Thunk::ThunkInfo::WithProfileAnnotation(instr), src->shape(), buffer,
-      *instr->channel_id(), send_recv_events_,
-      ConvertFrontendAttributes(instr->frontend_attributes()),
-      DeviceConstraint(instr)));
+  // AddThunkToThunkSequence(std::make_unique<SendThunk>(
+  //     Thunk::ThunkInfo::WithProfileAnnotation(instr), src->shape(), buffer,
+  //     *instr->channel_id(), send_recv_events_,
+  //     ConvertFrontendAttributes(instr->frontend_attributes()),
+  //     DeviceConstraint(instr)));
 
   return absl::OkStatus();
 }
@@ -4156,33 +4214,33 @@ absl::Status IrEmitterUnnested::EmitSendDoneThunk(
 }
 
 absl::Status IrEmitterUnnested::EmitRecvThunk(const HloRecvInstruction* instr) {
-  if (!instr->channel_id().has_value())
-    return absl::InternalError("Unknown recv instruction channel id");
-  TF_RET_CHECK(instr->shape().IsTuple());
-  TF_ASSIGN_OR_RETURN(BufferAllocation::Slice buffer,
-                      GetAllocationSliceForHlo(instr, {0}));
-  if (!instr->is_host_transfer()) {
-    const auto& hlo_config = ir_emitter_context_->hlo_module().config();
-    const int64_t replica_count = hlo_config.replica_count();
-    const int64_t partition_count = hlo_config.num_partitions();
-    const NcclCollectiveThunk::Buffer nccl_buffer = {
-        /*element_count=*/ShapeUtil::ElementsIn(instr->shape().tuple_shapes(0)),
-        /*source_buffer=*/buffer,
-        /*destination_buffer=*/buffer};
-    auto thunk = std::make_unique<NcclRecvThunk>(
-        Thunk::ThunkInfo::WithProfileAnnotation(instr), NcclApi::Default(),
-        instr, replica_count, partition_count, nccl_buffer);
-    collectives_async_events_.try_emplace(instr, thunk->async_events());
-    AddThunkToThunkSequence(std::move(thunk));
-    return absl::OkStatus();
-  }
+  // if (!instr->channel_id().has_value())
+  //   return absl::InternalError("Unknown recv instruction channel id");
+  // TF_RET_CHECK(instr->shape().IsTuple());
+  // TF_ASSIGN_OR_RETURN(BufferAllocation::Slice buffer,
+  //                     GetAllocationSliceForHlo(instr, {0}));
+  // if (!instr->is_host_transfer()) {
+  //   const auto& hlo_config = ir_emitter_context_->hlo_module().config();
+  //   const int64_t replica_count = hlo_config.replica_count();
+  //   const int64_t partition_count = hlo_config.num_partitions();
+  //   const NcclCollectiveThunk::Buffer nccl_buffer = {
+  //       /*element_count=*/ShapeUtil::ElementsIn(instr->shape().tuple_shapes(0)),
+  //       /*source_buffer=*/buffer,
+  //       /*destination_buffer=*/buffer};
+  //   auto thunk = std::make_unique<NcclRecvThunk>(
+  //       Thunk::ThunkInfo::WithProfileAnnotation(instr), NcclApi::Default(),
+  //       instr, replica_count, partition_count, nccl_buffer);
+  //   collectives_async_events_.try_emplace(instr, thunk->async_events());
+  //   AddThunkToThunkSequence(std::move(thunk));
+  //   return absl::OkStatus();
+  // }
 
-  AddThunkToThunkSequence(std::make_unique<RecvThunk>(
-      Thunk::ThunkInfo::WithProfileAnnotation(instr),
-      instr->shape().tuple_shapes()[0], buffer, *instr->channel_id(),
-      send_recv_events_,
-      ConvertFrontendAttributes(instr->frontend_attributes()),
-      DeviceConstraint(instr)));
+  // AddThunkToThunkSequence(std::make_unique<RecvThunk>(
+  //     Thunk::ThunkInfo::WithProfileAnnotation(instr),
+  //     instr->shape().tuple_shapes()[0], buffer, *instr->channel_id(),
+  //     send_recv_events_,
+  //     ConvertFrontendAttributes(instr->frontend_attributes()),
+  //     DeviceConstraint(instr)));
 
   return absl::OkStatus();
 }
@@ -4233,12 +4291,12 @@ absl::Status IrEmitterUnnested::EmitOp(
       return EmitSliceToDynamic(op);
     }
     const llvm::StringRef call_target = call.getCallTargetName();
-#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM
+#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM || TENSORFLOW_USE_SYCL
     if (absl::string_view(call_target.data(), call_target.size()) ==
         kTriangularSolveCallTarget) {
       return EmitTriangularSolveCustomCall(op);
     }
-#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM
+#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM || TENSORFLOW_USE_SYCL
 
     if (!is_gpu_runtime && call.getCallTargetName() == "__gpu$TopK") {
       return EmitTopKCustomCall(
@@ -4258,7 +4316,7 @@ absl::Status IrEmitterUnnested::EmitOp(
     return EmitGemmThunk(op);
   }
 
-#if GOOGLE_CUDA || TF_HIPBLASLT
+#if GOOGLE_CUDA || TF_HIPBLASLT || TENSORFLOW_USE_SYCL
   if (mlir::isa<mlir::lmhlo_gpu::CublasLtMatmulOp>(op)) {
     if (ir_emitter_context_->emit_ir_from_hlo()) {
       const auto* instr = Cast<HloCustomCallInstruction>(hlo_for_lmhlo.at(op));
@@ -4266,7 +4324,7 @@ absl::Status IrEmitterUnnested::EmitOp(
     }
     return EmitCublasLtMatmulThunk(op);
   }
-#endif  // GOOGLE_CUDA || TF_HIPBLASLT
+#endif  // GOOGLE_CUDA || TF_HIPBLASLT || TENSORFLOW_USE_SYCL
 #if GOOGLE_CUDA
   if (mlir::isa<mlir::lmhlo_gpu::CublasLtMatmulF8Op>(op)) {
     if (ir_emitter_context_->emit_ir_from_hlo()) {
@@ -4290,13 +4348,13 @@ absl::Status IrEmitterUnnested::EmitOp(
     }
     return EmitNormThunk(op);
   }
-  if (mlir::isa<mlir::lmhlo_gpu::fusedMHAOp>(op)) {
-    return EmitFusedMHAThunk(op);
-  }
   if (mlir::isa<mlir::lmhlo_gpu::fusedMHABackwardOp>(op)) {
     return EmitFusedMHABackwardThunk(op);
   }
 #endif  // GOOGLE_CUDA
+  if (mlir::isa<mlir::lmhlo_gpu::fusedMHAOp>(op)) {
+    return EmitFusedMHAThunk(op);
+  }
 
   if (mlir::isa<mlir::lmhlo_gpu::ConvForwardOp,
                 mlir::lmhlo_gpu::ConvForwardGraphOp,
@@ -4311,7 +4369,8 @@ absl::Status IrEmitterUnnested::EmitOp(
     return EmitConvolutionThunk(op);
   }
 
-#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM
+#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM || TENSORFLOW_USE_SYCL
+#if 0
   if (mlir::isa<mlir::lmhlo_gpu::RadixSortOp>(op)) {
     if (ir_emitter_context_->emit_ir_from_hlo()) {
       auto* instr = Cast<HloCustomCallInstruction>(hlo_for_lmhlo.at(op));
@@ -4319,6 +4378,7 @@ absl::Status IrEmitterUnnested::EmitOp(
     }
     return EmitCubDeviceRadixSort(op);
   }
+#endif
   if (mlir::isa<mlir::lmhlo_gpu::CholeskyOp>(op)) {
     if (ir_emitter_context_->emit_ir_from_hlo()) {
       return EmitCholeskyThunk(hlo_for_lmhlo.at(op));
@@ -4326,7 +4386,7 @@ absl::Status IrEmitterUnnested::EmitOp(
       return EmitCholeskyThunk(op);
     }
   }
-#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM
+#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM || TENSORFLOW_USE_SYCL
 
   if (mlir::isa<mlir::lmhlo::FftOp>(op)) {
     if (ir_emitter_context_->emit_ir_from_hlo()) {
@@ -4668,11 +4728,11 @@ absl::Status IrEmitterUnnested::EmitHloInstruction(
       if (IsLegacyCublasMatmul(*instr)) {
         return EmitGemmThunk(custom_call);
       }
-#if GOOGLE_CUDA || TF_HIPBLASLT
+#if GOOGLE_CUDA || TF_HIPBLASLT || TENSORFLOW_USE_SYCL
       if (IsCublasLtMatmul(*instr)) {
         return EmitCublasLtMatmulThunk(custom_call);
       }
-#endif  // GOOGLE_CUDA || TF_HIPBLASLT
+#endif  // GOOGLE_CUDA || TF_HIPBLASLT || TENSORFLOW_USE_SYCL
 #if GOOGLE_CUDA
       if (IsCublasLtMatmulF8(*instr)) {
         return EmitCublasLtMatmulThunkF8(custom_call);
@@ -4690,17 +4750,19 @@ absl::Status IrEmitterUnnested::EmitHloInstruction(
       if (IsCustomCallToDnnConvolution(*instr)) {
         return EmitConvolutionThunk(custom_call);
       }
-#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM
+#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM || TENSORFLOW_USE_SYCL
       if (IsCustomCallToCusolver(*instr)) {
         return EmitCholeskyThunk(instr);
       }
       if (IsTriangularSolve(*instr)) {
         return EmitTriangularSolveCustomCall(instr);
       }
+#if 0
       if (IsCubDeviceRadixSort(*instr)) {
         return EmitCubDeviceRadixSort(custom_call);
       }
-#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM
+#endif
+#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM || TENSORFLOW_USE_SYCL
       if (custom_call->custom_call_target() == "PadToStatic") {
         return EmitPadToStatic(custom_call);
       }
diff --git a/xla/service/gpu/ir_emitter_unnested.h b/xla/service/gpu/ir_emitter_unnested.h
index 5fa23a514..31da6809a 100644
--- a/xla/service/gpu/ir_emitter_unnested.h
+++ b/xla/service/gpu/ir_emitter_unnested.h
@@ -1,4 +1,6 @@
-/* Copyright 2018 The OpenXLA Authors.
+/* Copyright (c) 2024 Intel Corporation
+
+Copyright 2019 The TensorFlow Authors. All Rights Reserved.
 
 Licensed under the Apache License, Version 2.0 (the "License");
 you may not use this file except in compliance with the License.
@@ -39,7 +41,8 @@ limitations under the License.
 #include "xla/service/gpu/fusions/fusion_emitter.h"
 #include "xla/service/gpu/hlo_fusion_analysis.h"
 #include "xla/service/gpu/ir_emitter.h"
-#include "xla/service/gpu/nccl_collective_thunk.h"
+// #include "xla/service/gpu/nccl_collective_thunk.h"
+#include "xla/service/gpu/ccl_collective_thunk.h"
 #include "xla/service/gpu/runtime3/send_recv_thunk.h"
 #include "xla/service/gpu/thunk.h"
 #include "xla/service/llvm_ir/ir_array.h"
@@ -148,10 +151,10 @@ class IrEmitterUnnested : public IrEmitter {
   absl::Status EmitConvolutionThunk(const HloCustomCallInstruction* instr);
   absl::Status EmitGemmThunk(mlir::Operation* op);
   absl::Status EmitGemmThunk(const HloCustomCallInstruction* instr);
-#if GOOGLE_CUDA || TF_HIPBLASLT
+#if GOOGLE_CUDA || TF_HIPBLASLT || TENSORFLOW_USE_SYCL
   absl::Status EmitCublasLtMatmulThunk(mlir::Operation* op);
   absl::Status EmitCublasLtMatmulThunk(const HloCustomCallInstruction* instr);
-#endif  // GOOGLE_CUDA || TF_HIPBLASLT
+#endif  // GOOGLE_CUDA || TF_HIPBLASLT || TENSORFLOW_USE_SYCL
 #if GOOGLE_CUDA
   absl::Status EmitCublasLtMatmulThunkF8(mlir::Operation* op);
   absl::Status EmitCublasLtMatmulThunkF8(const HloCustomCallInstruction* instr);
@@ -163,12 +166,13 @@ class IrEmitterUnnested : public IrEmitter {
   absl::Status EmitFusedMHAThunk(mlir::Operation* op);
   absl::Status EmitFusedMHABackwardThunk(mlir::Operation* op);
 #endif  // GOOGLE_CUDA
+  absl::Status EmitFusedMHAThunk(mlir::Operation* op);
 #if GOOGLE_CUDA || TENSORFLOW_USE_ROCM
   absl::Status EmitCubDeviceRadixSort(mlir::Operation* op);
   absl::Status EmitCubDeviceRadixSort(const HloCustomCallInstruction* instr);
+#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM
   absl::Status EmitCholeskyThunk(mlir::Operation* op);
   absl::Status EmitCholeskyThunk(const HloInstruction* instr);
-#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM
   absl::Status EmitCustomCallThunk(mlir::Operation* op,
                                    const HloCustomCallInstruction* instr);
   absl::Status EmitCustomCallThunk(const HloCustomCallInstruction* instr);
@@ -201,10 +205,10 @@ class IrEmitterUnnested : public IrEmitter {
 
   absl::Status EmitSort(mlir::Operation* op, const HloSortInstruction* sort);
   absl::Status EmitSort(const HloSortInstruction* sort);
-#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM
+#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM || TENSORFLOW_USE_SYCL
   absl::Status EmitTriangularSolveCustomCall(mlir::Operation* op);
   absl::Status EmitTriangularSolveCustomCall(const HloInstruction* instr);
-#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM
+#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM || TENSORFLOW_USE_SYCL
   absl::Status EmitTopKCustomCall(const HloCustomCallInstruction* instr);
 
   absl::Status EmitSendThunk(const HloSendInstruction* instr);
diff --git a/xla/service/gpu/launch_dimensions.cc b/xla/service/gpu/launch_dimensions.cc
index 741e2dd34..1e032c025 100644
--- a/xla/service/gpu/launch_dimensions.cc
+++ b/xla/service/gpu/launch_dimensions.cc
@@ -94,6 +94,9 @@ struct BlockSizes {
 BlockSizes GetBlockSizes(LaunchDimensionsConfig dim_config,
                          const se::DeviceDescription& gpu_device_info,
                          const Shape& shape, int64_t num_elements) {
+#if !TENSORFLOW_USE_SYCL
+  // TODO: It will set 128 threads per block by default. We prefer to use
+  // the max value (1024 on PVC). It can benefit instructions like scatter.
   if (!dim_config.row_vectorized && !dim_config.few_waves) {
     BlockSizes result;
     const int kWarpSchedulers = 4;
@@ -104,6 +107,7 @@ BlockSizes GetBlockSizes(LaunchDimensionsConfig dim_config,
         num_elements, result.threads_per_block_x * result.threads_per_block_y);
     return result;
   }
+#endif
 
   int64_t threads_per_block_row_vectorized =
       ThreadsPerBlockRowVectorized(shape, gpu_device_info, dim_config);
diff --git a/xla/service/gpu/llvm_gpu_backend/BUILD b/xla/service/gpu/llvm_gpu_backend/BUILD
index 9f437f78c..426f039e9 100644
--- a/xla/service/gpu/llvm_gpu_backend/BUILD
+++ b/xla/service/gpu/llvm_gpu_backend/BUILD
@@ -3,6 +3,10 @@ load(
     "@local_config_rocm//rocm:build_defs.bzl",
     "if_rocm_is_configured",
 )
+load(
+    "@local_config_sycl//sycl:build_defs.bzl",
+    "if_sycl_is_configured",
+)
 
 package(
     # copybara:uncomment default_applicable_licenses = ["//tensorflow:license"],
@@ -36,6 +40,7 @@ cc_library(
         "//xla/service/gpu:metrics",
         "//xla/service/llvm_ir:llvm_command_line_options",
         "//xla/service/llvm_ir:llvm_type_conversion_util",
+        "//xla/service/llvm_ir:llvm_util",
         "//xla/stream_executor:device_description",
         "@com_google_absl//absl/base",
         "@com_google_absl//absl/memory",
@@ -68,6 +73,8 @@ cc_library(
     ] + if_rocm_is_configured([
         "@local_config_rocm//rocm:rocm_headers",
         "@llvm-project//llvm:AMDGPUCodeGen",
+    ]) + if_sycl_is_configured([
+        "@llvm_spir//:llvm_spir_translator",
     ]),
 )
 
diff --git a/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc b/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc
index bbd1352fd..3858f52ff 100644
--- a/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc
+++ b/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc
@@ -59,6 +59,7 @@ limitations under the License.
 #include "xla/service/gpu/metrics.h"
 #include "xla/service/llvm_ir/llvm_command_line_options.h"
 #include "xla/service/llvm_ir/llvm_type_conversion_util.h"
+#include "xla/service/llvm_ir/llvm_util.h"
 #include "xla/status_macros.h"
 #include "xla/stream_executor/device_description.h"
 #include "xla/types.h"
@@ -76,6 +77,9 @@ limitations under the License.
 #include "rocm/rocm_config.h"
 #endif
 
+#include "LLVMSPIRVLib.h"
+#include "LLVMSPIRVOpts.h"
+
 namespace xla {
 namespace gpu {
 namespace {
@@ -156,7 +160,6 @@ std::unique_ptr<llvm::TargetMachine> GetTargetMachine(
                << " -- " << error;
     return nullptr;
   }
-
   llvm::TargetOptions target_options =
       llvm::codegen::InitTargetOptionsFromCodeGenFlags(llvm::Triple());
 
@@ -368,10 +371,13 @@ absl::Status LinkAndOptimizeModule(
   llvm::CGSCCAnalysisManager cgam;
   llvm::ModuleAnalysisManager mam;
 
-  fam.registerPass([&] { return target_machine->getTargetIRAnalysis(); });
+  if (target_machine)
+    fam.registerPass([&] { return target_machine->getTargetIRAnalysis(); });
 
+  // SYCL: customized config
   llvm::PipelineTuningOptions pto;
-  pto.SLPVectorization = true;
+  pto.SLPVectorization = false;
+  pto.LoopVectorization = false;
   pto.InlinerThreshold = inline_threshold;
 
   llvm::PassInstrumentationCallbacks pic;
@@ -1034,5 +1040,107 @@ absl::StatusOr<std::vector<uint8_t>> CompileToHsaco(
 
 }  // namespace amdgpu
 
+namespace {
+std::unique_ptr<llvm::TargetMachine> SPIRGetTargetMachine(
+    llvm::Triple target_triple, se::GpuComputeCapability gpu_version,
+    const DebugOptions& debug_options) {
+  return nullptr;
+}
+
+Status SPIRTargetModuleLinker(llvm::Module* module,
+                              se::GpuComputeCapability gpu_version,
+                              const DebugOptions& debug_options,
+                              const std::string& device_bitcode_dir_path) {
+  return OkStatus();
+}
+
+StatusOr<std::string> EmitModuleToSpir(llvm::Module* module,
+                                       se::GpuComputeCapability gpu_version,
+                                       const DebugOptions& debug_options) {
+  SPIRV::TranslatorOpts::ExtensionsStatusMap ExtensionsStatus;
+  SPIRV::TranslatorOpts opts(SPIRV::VersionNumber::MaximumVersion,
+                             ExtensionsStatus);
+  opts.enableAllExtensions();  // enable all SPIR-V extension first
+
+  std::ostringstream oss;
+  std::string err;
+  bool success = llvm::writeSpirv(module, opts, oss, err);
+  if (!success) {
+    return xla::Internal("Fails to convert LLVM as SPIR-V: %s", err);
+  }
+  return oss.str();
+}
+
+void SPIRBackendInit(const DebugOptions& debug_options) {
+
+  FeedLLVMWithFlags({"-slp-vectorize-hor=false"});
+
+  bool vec = true;
+  tsl::ReadBoolFromEnvVar("VECTORIZE", true, &vec);
+  if (vec) {
+    FeedLLVMWithFlags({
+        "-slp-min-reg-size=64",
+        "-slp-max-reg-size=64",
+    });
+  } else {
+    // TODO: sycl-opt disables all LLVM vectorization passes. Evaluate if it is
+    // needed.
+    FeedLLVMWithFlags({"-sycl-opt=1"});
+  }
+
+  llvm_ir::InitializeLLVMCommandLineOptions(
+      debug_options.xla_backend_extra_options());
+
+  llvm::PassRegistry* registry = llvm::PassRegistry::getPassRegistry();
+  InitializePasses(registry);
+}
+}  // namespace
+
+namespace spir {
+absl::StatusOr<std::vector<uint8_t>> CompileToSpir(
+    llvm::Module* module, se::GpuComputeCapability gpu_version,
+    const DebugOptions& debug_options) {
+  std::string libdevice_dir_path;
+  static absl::once_flag backend_init_flag;
+  absl::call_once(backend_init_flag, SPIRBackendInit, debug_options);
+
+  std::string spir;
+  {
+    XLA_SCOPED_LOGGING_TIMER("Compile module " + module->getName().str());
+
+    // If the module has no functions or globals, there's nothing to compile.
+    if (module->empty() && module->global_empty()) {
+      VLOG(2) << "Module '" << module->getName().str()
+              << "' is empty. Skipping compilation.";
+      return std::vector<uint8_t>();
+    }
+
+    // No SPIR target machine?
+    llvm::Triple default_target_triple("spir64-unknown-unknown");
+    std::unique_ptr<llvm::TargetMachine> target_machine =
+        SPIRGetTargetMachine(default_target_triple, gpu_version, debug_options);
+
+    bool opt = true;
+    tsl::ReadBoolFromEnvVar("SYCL_LLVM_OPT", true, &opt);
+    if (opt) {
+      // Link with libdevice, and optimize the LLVM module.
+      TF_RETURN_IF_ERROR(LinkAndOptimizeModule(
+          module, gpu_version, debug_options, libdevice_dir_path,
+          SPIRTargetModuleLinker, default_target_triple, target_machine.get(),
+          kDefaultInlineThreshold));
+    }
+
+#if 0
+    LOG(ERROR) << "Optimized IR before converting to spir\n" << llvm_ir::DumpToString(module);
+#endif
+
+    // Lower optimized LLVM module to SPIR.
+    TF_ASSIGN_OR_RETURN(spir,
+                        EmitModuleToSpir(module, gpu_version, debug_options));
+  }
+  return std::vector<uint8_t>(spir.begin(), spir.end());
+}
+}  // namespace spir
+
 }  // namespace gpu
 }  // namespace xla
diff --git a/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.h b/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.h
index b774daac3..0e2bce095 100644
--- a/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.h
+++ b/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.h
@@ -66,6 +66,12 @@ absl::StatusOr<std::vector<uint8_t>> CompileToHsaco(
     const std::string& module_config_cache_key);
 }  // namespace amdgpu
 
+namespace spir {
+absl::StatusOr<std::vector<uint8_t>> CompileToSpir(
+    llvm::Module* module, se::GpuComputeCapability gpu_version,
+    const DebugOptions& debug_options);
+}  // namespace spir
+
 }  // namespace gpu
 }  // namespace xla
 
diff --git a/xla/service/gpu/make_batch_pointers.cc b/xla/service/gpu/make_batch_pointers.cc
index 9cc84bf6b..741b40604 100644
--- a/xla/service/gpu/make_batch_pointers.cc
+++ b/xla/service/gpu/make_batch_pointers.cc
@@ -44,6 +44,7 @@ namespace make_batch_pointers {
 void* kernel();  // returns a pointer to a CUDA C++ device function
 }  // namespace make_batch_pointers
 
+#if !TENSORFLOW_USE_SYCL
 absl::Status MakeBatchPointers(se::Stream* stream,
                                se::DeviceMemoryBase base_ptr,
                                size_t stride_bytes, size_t n,
@@ -71,5 +72,14 @@ absl::Status MakeBatchPointers(se::Stream* stream,
 #endif
   return absl::OkStatus();
 }
+#else
+absl::Status MakeBatchPointers(se::Stream* stream,
+                               const se::DeviceMemoryBase& base_ptr,
+                               size_t stride_bytes, size_t n,
+                               se::DeviceMemoryBase& ptrs_out) {
+  ptrs_out = base_ptr;
+  return absl::OkStatus();
+}
+#endif // !TENSORFLOW_USE_SYCL
 
 }  // namespace xla::gpu
diff --git a/xla/service/gpu/make_batch_pointers.h b/xla/service/gpu/make_batch_pointers.h
index 1be4994ec..b0eeb058e 100644
--- a/xla/service/gpu/make_batch_pointers.h
+++ b/xla/service/gpu/make_batch_pointers.h
@@ -50,11 +50,16 @@ namespace xla::gpu {
 //    driver and slow down *all* work on the GPU.  So to do this right, we'd
 //    need to allocate the host memory as pinned, one alloc per stream.  Then
 //    we'd need to manage this memory without leaks.  This becomes complex!
+#if !TENSORFLOW_USE_SYCL
 absl::Status MakeBatchPointers(se::Stream* stream,
                                se::DeviceMemoryBase base_ptr,
                                size_t stride_bytes, size_t n,
                                se::DeviceMemoryBase ptrs_out);
-
+#else
+absl::Status MakeBatchPointers(se::Stream* stream,
+                               const se::DeviceMemoryBase& base_ptr,
+                               size_t stride_bytes, size_t n,
+                               se::DeviceMemoryBase& ptrs_out);
+#endif
 }  // namespace xla::gpu
-
-#endif  // XLA_SERVICE_GPU_MAKE_BATCH_POINTERS_H_
+#endif  // XLA_SERVICE_GPU_PRECOMPILED_KERNELS_H_
diff --git a/xla/service/gpu/matmul_utils.h b/xla/service/gpu/matmul_utils.h
index 2efbdd386..1988b3a46 100644
--- a/xla/service/gpu/matmul_utils.h
+++ b/xla/service/gpu/matmul_utils.h
@@ -40,6 +40,23 @@ limitations under the License.
 #include "rocm/rocm_config.h"
 #endif
 
+namespace stream_executor {
+namespace cuda {
+namespace BlasLt {
+enum class Epilogue {
+  kDefault = 1,                   // No special postprocessing
+  kReLU = 2,                      // Apply point-wise ReLU function
+  kBias = 4,                      // Add broadcasted bias vector
+  kBiasThenReLU = kBias | kReLU,  // Apply bias and then ReLU transform
+  kGELU = 32,                // Apply GELU point-wise transform to the results
+  kGELUWithAux = 32 | 1024,  // Apply GELU with auxiliary output.
+  kBiasThenGELU = kBias | kGELU,  // Apply bias and then approximate GELU.
+  kBiasThenGELUWithAux = kBiasThenGELU | 1024,
+};
+}
+}  // namespace cuda
+}  // namespace stream_executor
+
 namespace xla {
 namespace gpu {
 
diff --git a/xla/service/gpu/model/gpu_performance_model.cc b/xla/service/gpu/model/gpu_performance_model.cc
index d03686306..3a6bf8c15 100644
--- a/xla/service/gpu/model/gpu_performance_model.cc
+++ b/xla/service/gpu/model/gpu_performance_model.cc
@@ -55,7 +55,11 @@ namespace gpu {
 namespace {
 
 // Estimated values in the absence of easy ways to query them.
+#if TENSORFLOW_USE_SYCL
+static constexpr absl::Duration kKernelLaunchOverhead = absl::Microseconds(5);
+#else
 static constexpr absl::Duration kKernelLaunchOverhead = absl::Microseconds(1);
+#endif
 static constexpr absl::Duration kNcclKernelLaunchOverhead =
     absl::Microseconds(5);
 static constexpr float kL2CacheSpeedup = 2.5;
@@ -108,7 +112,11 @@ int GetCoalescingWasteFactor(PrimitiveType element_type) {
 // (1830 MHz) to saturate the memory bandwidth (3.35 TB/s).
 float AdjustBandwidth(const se::DeviceDescription& gpu_device_info,
                       float bandwidth, int64_t num_blocks) {
+#if TENSORFLOW_USE_SYCL
+  float per_block_bandwidth = gpu_device_info.clock_rate_ghz() * 1.0e9f * 64;
+#else
   float per_block_bandwidth = gpu_device_info.clock_rate_ghz() * 1.0e9f * 32;
+#endif
   float max_bandwidth = num_blocks * per_block_bandwidth;
 
   return std::min(bandwidth, max_bandwidth);
@@ -263,7 +271,12 @@ LaunchDimensions EstimateFusionLaunchDimensions(
       return kernel_emitter->launch_dimensions();
     }
   }
-  int64_t block_size = 128;  // Result for default LaunchDimensionsConfig.
+  // Result for default LaunchDimensionsConfig.
+#if TENSORFLOW_USE_SYCL
+  int64_t block_size = RoundUpTo(device_info.threads_per_block_limit(),int64_t{32});
+#else
+  int64_t block_size = 128;
+#endif
   int64_t num_blocks = CeilOfRatio(estimated_num_threads, block_size);
   return LaunchDimensions(num_blocks, block_size);
 }
diff --git a/xla/service/gpu/nccl_api.h b/xla/service/gpu/nccl_api.h
index 86f19a0d6..80db1a4b9 100644
--- a/xla/service/gpu/nccl_api.h
+++ b/xla/service/gpu/nccl_api.h
@@ -127,6 +127,11 @@ class NcclApi {
   // https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/api/comms.html#ncclgetuniqueid
   virtual absl::StatusOr<NcclCliqueId> GetUniqueId() = 0;
 
+
+  // Temporary function to get unique id for non-NCCL backend.
+  virtual absl::StatusOr<NcclCliqueId> GetId(const NcclCliqueKey& key,
+                                             const RunId& id) = 0;
+
   // Creates a new communicator.
   //
   // https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/api/comms.html#ncclcomminitrank
diff --git a/xla/service/gpu/nccl_clique.cc b/xla/service/gpu/nccl_clique.cc
index 5ea4d2d55..23539a3fc 100644
--- a/xla/service/gpu/nccl_clique.cc
+++ b/xla/service/gpu/nccl_clique.cc
@@ -70,8 +70,11 @@ absl::StatusOr<const NcclCliqueIdCallback*> GetNcclCliqueIdCallback(
       << "If non-local devices are taking part of a collective API on "
          "GPU, the nccl_clique_id_callback must be provided by the client.";
 
-  static auto* local_callback = new NcclCliqueIdCallback(
-      [](const NcclCliqueKey&) { return NcclApi::Default()->GetUniqueId(); });
+  static auto* local_callback =
+      new NcclCliqueIdCallback([](const NcclCliqueKey& key, const RunId& id) {
+        // return NcclApi::Default()->GetUniqueId();
+        return NcclApi::Default()->GetId(key, id);
+      });
   return local_callback;
 }
 
@@ -278,7 +281,7 @@ static absl::StatusOr<std::shared_ptr<NcclClique::Lock>> InitializeNcclClique(
   // Creates initialization state for participating ranks.
   auto create_initialization_state = [&](absl::Span<const int32_t* const> ranks)
       -> absl::StatusOr<InitializationState> {
-    TF_ASSIGN_OR_RETURN(auto clique_id, clique_id_callback(clique_key));
+    TF_ASSIGN_OR_RETURN(auto clique_id, clique_id_callback(clique_key, run_id));
     VLOG(3) << "Created unique clique id (hash): " << absl::HashOf(clique_id);
     return InitializationState(clique_id, ranks);
   };
diff --git a/xla/service/gpu/nccl_clique.h b/xla/service/gpu/nccl_clique.h
index 274c56ba7..8c4e31073 100644
--- a/xla/service/gpu/nccl_clique.h
+++ b/xla/service/gpu/nccl_clique.h
@@ -30,6 +30,7 @@ limitations under the License.
 #include "absl/strings/str_format.h"
 #include "xla/executable_run_options.h"
 #include "xla/service/global_device_id.h"
+#include "xla/service/gpu/ccl_api.h"
 #include "xla/service/gpu/nccl_api.h"
 #include "xla/service/gpu/nccl_clique_key.h"
 #include "xla/service/lockable.h"
diff --git a/xla/service/gpu/nccl_clique_key.h b/xla/service/gpu/nccl_clique_key.h
index ef4df8833..cc0d1d2f5 100644
--- a/xla/service/gpu/nccl_clique_key.h
+++ b/xla/service/gpu/nccl_clique_key.h
@@ -26,6 +26,7 @@ limitations under the License.
 
 #include "absl/status/statusor.h"
 #include "absl/types/span.h"
+#include "xla/executable_run_options.h"
 #include "xla/service/global_device_id.h"
 
 namespace xla::gpu {
@@ -140,7 +141,8 @@ H AbslHashValue(H h, const NcclCliqueId& id) {
 
 // A callback to get a unique clique id (see `ncclUniqueId` documentation).
 using NcclCliqueIdCallback =  // NOLINT
-    std::function<absl::StatusOr<NcclCliqueId>(const NcclCliqueKey&)>;
+    std::function<absl::StatusOr<NcclCliqueId>(const NcclCliqueKey&,
+                                               const RunId&)>;
 
 }  // namespace xla::gpu
 
diff --git a/xla/service/gpu/runtime/BUILD b/xla/service/gpu/runtime/BUILD
index cbadbe8dc..33a2cc0b7 100644
--- a/xla/service/gpu/runtime/BUILD
+++ b/xla/service/gpu/runtime/BUILD
@@ -1,7 +1,7 @@
 load("@local_config_cuda//cuda:build_defs.bzl", "cuda_library")
 load("//xla:xla.bzl", "xla_cc_test")
 load("//xla/service/gpu:build_defs.bzl", "gpu_kernel_library")
-load("//xla/stream_executor:build_defs.bzl", "if_gpu_is_configured")
+load("//xla/stream_executor:build_defs.bzl", "if_gpu_is_configured", "if_cuda_or_rocm")
 load(
     "@local_config_rocm//rocm:build_defs.bzl",
     "if_rocm_is_configured",
@@ -206,7 +206,7 @@ cc_library(
         "//xla/service:executable",
         "//xla/stream_executor:device_memory",
         "@com_google_absl//absl/status",
-    ] + if_gpu_is_configured([
+    ] + if_cuda_or_rocm([
         "//xla/service/gpu/runtime3:cub_sort_thunk",
     ]),
 )
diff --git a/xla/service/gpu/runtime3/BUILD b/xla/service/gpu/runtime3/BUILD
index 35a8ed2be..df6fd8477 100644
--- a/xla/service/gpu/runtime3/BUILD
+++ b/xla/service/gpu/runtime3/BUILD
@@ -3,6 +3,7 @@ load("//xla/service/gpu:build_defs.bzl", "get_cub_sort_kernel_types")
 load("//xla/stream_executor:build_defs.bzl", "if_gpu_is_configured")
 load("@local_config_rocm//rocm:build_defs.bzl", "if_rocm_is_configured")
 load("@tsl//tsl/platform/default:cuda_build_defs.bzl", "if_cuda_is_configured")
+load("@local_config_sycl//sycl:build_defs.bzl", "if_sycl_is_configured")
 
 package(
     # copybara:uncomment default_applicable_licenses = ["//tensorflow:license"],
@@ -273,6 +274,7 @@ cc_library(
         "@com_google_absl//absl/container:flat_hash_map",
         "@com_google_absl//absl/container:inlined_vector",
         "@com_google_absl//absl/status",
+        "@intel_extension_for_openxla//xla/service/gpu:onednn_gpu_conv_runner",
     ],
 )
 
@@ -354,8 +356,10 @@ cc_library(
         "//xla/hlo/ir:hlo",
         "//xla/service:buffer_assignment",
         "//xla/service/gpu:buffer_allocations",
+        "//xla/service/gpu:cusolver_context",
         "//xla/service/gpu:thunk",
         "//xla/stream_executor",
+        "//xla/stream_executor/gpu:gpu_helpers_header",
         "@com_google_absl//absl/container:flat_hash_map",
         "@com_google_absl//absl/status",
         "@com_google_absl//absl/status:statusor",
@@ -378,6 +382,7 @@ cc_library(
         "//xla/service/gpu:thunk",
         "//xla/stream_executor",
         "@com_google_absl//absl/container:flat_hash_map",
+        "@intel_extension_for_openxla//xla/service/gpu:xetla_gpu_fused_mha_runner",
     ],
 )
 
@@ -392,6 +397,7 @@ cc_library(
         "//xla/service/gpu:thunk",
         "//xla/stream_executor:device_memory",
         "@com_google_absl//absl/status",
+        "@intel_extension_for_openxla//xla/service/gpu:onednn_matmul_utils",
         "@tsl//tsl/platform:logging",
     ],
 )
@@ -410,6 +416,7 @@ cc_library(
         "//xla:status",
         "//xla/stream_executor:device_memory",
         "//xla/stream_executor",
+        "@intel_extension_for_openxla//xla/service/gpu:onednn_matmul_utils",
         "@tsl//tsl/platform:logging",
     ]),
 )
@@ -614,6 +621,7 @@ cc_library(
         "@com_google_absl//absl/strings:str_format",
         "//xla/service/gpu:buffer_allocations",
         "//xla/service/gpu:make_batch_pointers",
+        "//xla/service/gpu:cusolver_context",
         "//xla/service/gpu:thunk",
         "//xla:types",
         "//xla:util",
diff --git a/xla/service/gpu/runtime3/cholesky_thunk.cc b/xla/service/gpu/runtime3/cholesky_thunk.cc
index d6db64682..9d90cb0c7 100644
--- a/xla/service/gpu/runtime3/cholesky_thunk.cc
+++ b/xla/service/gpu/runtime3/cholesky_thunk.cc
@@ -33,13 +33,12 @@ namespace xla {
 namespace gpu {
 
 namespace {
-
 template <typename T>
 absl::Status DoPotrfBatched(const se::GpuAsmOpts& asm_opts,
                             CholeskyParams* params, se::Stream* stream,
                             GpuSolverContext& context) {
   T* a_base = static_cast<T*>(params->a_buffer.opaque());
-  se::DeviceMemory<int> infos(params->info_buffer);
+
 #if TENSORFLOW_USE_ROCSOLVER
   // hipsolver is not supported so allocate a GPU buffer
   se::ScopedDeviceMemory<T*> ptrs =
@@ -49,6 +48,9 @@ absl::Status DoPotrfBatched(const se::GpuAsmOpts& asm_opts,
   se::DeviceMemory<T*> as(params->workspace_buffer);
 #endif
 
+  se::DeviceMemory<int> infos(params->info_buffer);
+
+#if !TENSORFLOW_USE_SYCL
   CHECK_GE(as.size(), params->batch_size);
   CHECK_GE(infos.size(), params->batch_size);
 
@@ -61,8 +63,11 @@ absl::Status DoPotrfBatched(const se::GpuAsmOpts& asm_opts,
   // Now that we've set up the `as` array, we can call cusolver.
   return context.PotrfBatched(params->uplo, params->n, as, params->n, infos,
                               params->batch_size);
+#else // !TENSORFLOW_USE_SYCL
+  return context.PotrfBatched(params->uplo, params->n, as, params->n, infos,
+                              params->batch_size, a_base);
+#endif // !TENSORFLOW_USE_SYCL
 }
-
 }  // namespace
 
 CholeskyThunk::CholeskyThunk(ThunkInfo thunk_info,
diff --git a/xla/service/gpu/runtime3/cholesky_thunk.h b/xla/service/gpu/runtime3/cholesky_thunk.h
index 26054a194..91130ad2b 100644
--- a/xla/service/gpu/runtime3/cholesky_thunk.h
+++ b/xla/service/gpu/runtime3/cholesky_thunk.h
@@ -18,6 +18,7 @@ limitations under the License.
 
 #include <optional>
 
+#include "tsl/platform/status.h"
 #include "xla/hlo/ir/hlo_instruction.h"
 #include "xla/service/buffer_assignment.h"
 #include "xla/service/gpu/buffer_allocations.h"
@@ -28,7 +29,6 @@ limitations under the License.
 #include "xla/stream_executor/stream_executor.h"
 #include "xla/types.h"
 #include "xla/xla_data.pb.h"
-#include "tsl/platform/status.h"
 
 namespace xla {
 namespace gpu {
@@ -74,8 +74,8 @@ struct CholeskyParams {
   se::DeviceMemoryBase workspace_buffer;
   se::DeviceMemoryBase info_buffer;
 };
-absl::Status RunCholesky(const se::GpuAsmOpts& asm_opts, PrimitiveType type,
-                         CholeskyParams* params, se::Stream* stream);
+Status RunCholesky(const se::GpuAsmOpts& asm_opts, PrimitiveType type,
+                   CholeskyParams* params, se::Stream* stream);
 
 }  // namespace gpu
 }  // namespace xla
diff --git a/xla/service/gpu/runtime3/convolution_thunk.cc b/xla/service/gpu/runtime3/convolution_thunk.cc
index 56bd9617e..a25230da6 100644
--- a/xla/service/gpu/runtime3/convolution_thunk.cc
+++ b/xla/service/gpu/runtime3/convolution_thunk.cc
@@ -25,11 +25,20 @@ limitations under the License.
 #include "xla/stream_executor/stream_executor.h"
 #include "xla/util.h"
 
+#if TENSORFLOW_USE_SYCL
+#include "xla/service/gpu/onednn_gpu_conv_runner.h"
+#include "xla/stream_executor/scratch_allocator.h"
+#endif
+
 namespace xla {
 namespace gpu {
 
 ConvolutionThunk::ConvolutionThunk(
+#if TENSORFLOW_USE_SYCL
+    ThunkInfo thunk_info, GpuConvDescriptor descriptor,
+#else
     ThunkInfo thunk_info, GpuConvConfig config,
+#endif
     std::vector<BufferAllocation::Slice> operand_slices,
     std::vector<BufferAllocation::Slice> result_slices,
     BufferAllocation::Slice scratch_slice)
@@ -37,8 +46,13 @@ ConvolutionThunk::ConvolutionThunk(
       operand_buffers_(std::move(operand_slices)),
       result_buffers_(std::move(result_slices)),
       scratch_buffer_(scratch_slice),
+#if TENSORFLOW_USE_SYCL
+      descriptor_(std::move(descriptor)) {}
+#else
       config_(std::move(config)) {}
+#endif
 
+#if !TENSORFLOW_USE_SYCL
 GenericConvRunner& ConvolutionThunk::GetOrCreateRunner(
     const stream_executor::Stream* stream) {
   absl::MutexLock lock(&mu_);
@@ -50,6 +64,7 @@ GenericConvRunner& ConvolutionThunk::GetOrCreateRunner(
   }
   return *it->second;
 }
+#endif
 
 absl::Status ConvolutionThunk::ExecuteOnStream(const ExecuteParams& params) {
   const auto& buffer_allocations = *params.buffer_allocations;
@@ -68,13 +83,26 @@ absl::Status ConvolutionThunk::ExecuteOnStream(const ExecuteParams& params) {
   se::DeviceMemoryBase scratch =
       buffer_allocations.GetDeviceAddress(scratch_buffer_);
 
+#if TENSORFLOW_USE_SYCL
+  auto stream = params.stream;
+  se::OwningScratchAllocator<2> scratch_allocator(
+      buffer_allocations.device_ordinal(),
+      buffer_allocations.memory_allocator());
+  TF_ASSIGN_OR_RETURN(auto conv_primitive,
+                      GetOrCreateOneDnnConvPrimitive(stream, descriptor_, operand_se_buffers,
+                                                     result_se_buffers[0], params, &scratch_allocator));
+
+  TF_RETURN_IF_ERROR(RunGpuConv(conv_primitive, descriptor_,
+                                absl::MakeSpan(operand_se_buffers),
+                                result_se_buffers[0], params));
+#else
   RunConvOptions opts;
   opts.runner_cache = &GetOrCreateRunner(params.stream);
 
   TF_RETURN_IF_ERROR(RunGpuConv(config_, absl::MakeSpan(operand_se_buffers),
                                 absl::MakeSpan(result_se_buffers), scratch,
                                 params.stream, opts));
-
+#endif
   // Note: Convolution has a tuple buffer as an output, but we don't need to
   // populate it as no one should be reading from the tuple directly.
   if (!params.stream->ok()) {
diff --git a/xla/service/gpu/runtime3/convolution_thunk.h b/xla/service/gpu/runtime3/convolution_thunk.h
index 5c202a6b7..df9898fc2 100644
--- a/xla/service/gpu/runtime3/convolution_thunk.h
+++ b/xla/service/gpu/runtime3/convolution_thunk.h
@@ -37,7 +37,12 @@ class ConvolutionThunk : public Thunk {
   // Constructs a thunk for launching a DNN convolution.
   //
   // operand_slices should be in the same order as cudnn_call->operands().
-  ConvolutionThunk(ThunkInfo thunk_info, GpuConvConfig config,
+  ConvolutionThunk(ThunkInfo thunk_info,
+#if TENSORFLOW_USE_SYCL
+                   GpuConvDescriptor descriptor,
+#else
+                   GpuConvConfig config,
+#endif
                    std::vector<BufferAllocation::Slice> operand_slices,
                    std::vector<BufferAllocation::Slice> result_slices,
                    BufferAllocation::Slice scratch_slice);
@@ -54,7 +59,11 @@ class ConvolutionThunk : public Thunk {
   GenericConvRunner& GetOrCreateRunner(const stream_executor::Stream* stream);
 
   // Convolution config
+#if TENSORFLOW_USE_SYCL
+  const GpuConvDescriptor descriptor_;
+#else
   const GpuConvConfig config_;
+#endif
   absl::Mutex mu_;
   absl::flat_hash_map<const stream_executor::Stream*,
                       std::unique_ptr<GenericConvRunner>>
diff --git a/xla/service/gpu/runtime3/custom_call_thunk.cc b/xla/service/gpu/runtime3/custom_call_thunk.cc
index 6329f6764..51721e1e9 100644
--- a/xla/service/gpu/runtime3/custom_call_thunk.cc
+++ b/xla/service/gpu/runtime3/custom_call_thunk.cc
@@ -36,7 +36,7 @@ limitations under the License.
 #include "xla/stream_executor/device_memory.h"
 #include "xla/util.h"
 
-#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM
+#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM || TENSORFLOW_USE_SYCL
 #include "xla/stream_executor/gpu/gpu_stream.h"
 #endif
 
@@ -89,7 +89,7 @@ absl::Status CustomCallThunk::ExecuteCustomCall(const ExecuteParams& params) {
     }
   }
 
-#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM
+#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM || TENSORFLOW_USE_SYCL
   auto gpu_stream = se::gpu::AsGpuStreamValue(params.stream);
   XlaCustomCallStatus custom_call_status;
   call_target_(gpu_stream, buffers.data(), opaque_.data(), opaque_.size(),
@@ -100,11 +100,11 @@ absl::Status CustomCallThunk::ExecuteCustomCall(const ExecuteParams& params) {
   } else {
     return absl::OkStatus();
   }
-#else   //  GOOGLE_CUDA || TENSORFLOW_USE_ROCM
+#else   //  GOOGLE_CUDA || TENSORFLOW_USE_ROCM || TENSORFLOW_USE_SYCL
   return Unavailable(
       "Custom calls on GPU are not supported in this configuration. Please "
       "build with --config=cuda or --config=rocm");
-#endif  //   GOOGLE_CUDA || TENSORFLOW_USE_ROCM
+#endif  //   GOOGLE_CUDA || TENSORFLOW_USE_ROCM || TENSORFLOW_USE_SYCL
 }
 
 absl::Status CustomCallThunk::ExecuteFfiHandler(const ExecuteParams& params) {
diff --git a/xla/service/gpu/runtime3/custom_call_thunk.h b/xla/service/gpu/runtime3/custom_call_thunk.h
index 55134f592..fec30bd81 100644
--- a/xla/service/gpu/runtime3/custom_call_thunk.h
+++ b/xla/service/gpu/runtime3/custom_call_thunk.h
@@ -35,7 +35,7 @@ limitations under the License.
 #include "xla/shape.h"
 #include "xla/status.h"
 
-#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM
+#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM || TENSORFLOW_USE_SYCL
 #include "xla/stream_executor/gpu/gpu_types.h"
 #endif
 
@@ -55,11 +55,11 @@ namespace gpu {
 // compiler is allowed to create.
 class CustomCallThunk : public Thunk {
  public:
-#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM
+#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM || TENSORFLOW_USE_SYCL
   using Stream = stream_executor::gpu::GpuStreamHandle;
-#else   //  GOOGLE_CUDA || TENSORFLOW_USE_ROCM
+#else   //  GOOGLE_CUDA || TENSORFLOW_USE_ROCM || TENSORFLOW_USE_SYCL
   using Stream = void*;
-#endif  //  GOOGLE_CUDA || TENSORFLOW_USE_ROCM
+#endif  //  GOOGLE_CUDA || TENSORFLOW_USE_ROCM || TENSORFLOW_USE_SYCL
 
   using CustomCallTarget = std::function<void(Stream, void**, const char*,
                                               size_t, XlaCustomCallStatus*)>;
diff --git a/xla/service/gpu/runtime3/fft_thunk.cc b/xla/service/gpu/runtime3/fft_thunk.cc
index 618eed1a2..1e7ad80ab 100644
--- a/xla/service/gpu/runtime3/fft_thunk.cc
+++ b/xla/service/gpu/runtime3/fft_thunk.cc
@@ -67,6 +67,7 @@ std::string FftTypeToString(se::fft::Type type) {
   }
 }
 
+
 absl::StatusOr<stream_executor::blas::BlasSupport*> GetBlas(
     se::Stream* stream) {
   auto blas = stream->parent()->AsBlas();
@@ -83,6 +84,7 @@ absl::StatusOr<stream_executor::fft::FftSupport*> GetFft(se::Stream* stream) {
   }
   return fft;
 }
+
 }  // namespace
 
 FftThunk::FftThunk(ThunkInfo thunk_info, FftType fft_type,
diff --git a/xla/service/gpu/runtime3/fused_mha_thunk.cc b/xla/service/gpu/runtime3/fused_mha_thunk.cc
index 731f4a087..e3590adb5 100644
--- a/xla/service/gpu/runtime3/fused_mha_thunk.cc
+++ b/xla/service/gpu/runtime3/fused_mha_thunk.cc
@@ -21,6 +21,10 @@ limitations under the License.
 #include "xla/stream_executor/stream_executor.h"
 #include "xla/util.h"
 
+#if TENSORFLOW_USE_SYCL
+#include "xla/service/gpu/xetla_gpu_fused_mha_runner.h"
+#endif
+
 namespace xla {
 namespace gpu {
 
@@ -83,12 +87,19 @@ absl::Status FusedMHAThunk::ExecuteOnStream(const ExecuteParams& params) {
   std::optional<se::DeviceMemoryBase> activation_buffer =
       AssignBufferIfNotNull(buffer_allocations, activation_buffer_);
 
+#if TENSORFLOW_USE_SYCL
+  TF_RETURN_IF_ERROR(RunXetlaGpuFMHA(config_, lhs_bmm1_buffer, rhs_bmm1_buffer,
+                                     rhs_bmm2_buffer, output_buffer, scratch_buffer,
+                                     mask_buffer, bias_buffer,
+                                     activation_buffer, params.stream));
+#else
   RunFusedMHAOptions opts;
   opts.runner_cache = &GetOrCreateRunner(params.stream);
   TF_RETURN_IF_ERROR(RunGpuFMHA(config_, lhs_bmm1_buffer, rhs_bmm1_buffer,
                                 rhs_bmm2_buffer, output_buffer, scratch_buffer,
                                 mask_buffer, bias_buffer, activation_buffer,
                                 params.stream, opts));
+#endif
 
   if (!params.stream->ok()) {
     return Internal("FusedMHAThunk::ExecuteOnStream failed.");
diff --git a/xla/service/gpu/runtime3/gemm_thunk.cc b/xla/service/gpu/runtime3/gemm_thunk.cc
index 88fcb1e72..8599b2f3e 100644
--- a/xla/service/gpu/runtime3/gemm_thunk.cc
+++ b/xla/service/gpu/runtime3/gemm_thunk.cc
@@ -24,6 +24,10 @@ limitations under the License.
 #include "xla/stream_executor/device_memory.h"
 #include "tsl/platform/logging.h"
 
+#if TENSORFLOW_USE_SYCL
+#include "xla/service/gpu/onednn_matmul_utils.h"
+#endif
+
 namespace xla {
 namespace gpu {
 
@@ -41,6 +45,7 @@ GemmThunk::GemmThunk(ThunkInfo thunk_info, GemmConfig config,
       workspace_(workspace),
       deterministic_(deterministic) {}
 
+#if !TENSORFLOW_USE_SYCL
 absl::Status GemmThunk::ExecuteOnStream(const ExecuteParams& params) {
   VLOG(3) << "Running GEMM thunk";
   const BufferAllocations& allocs = *params.buffer_allocations;
@@ -57,6 +62,26 @@ absl::Status GemmThunk::ExecuteOnStream(const ExecuteParams& params) {
                  allocs.GetDeviceAddress(output_buffer_), workspace,
                  deterministic_, stream);
 }
+#else // !TENSORFLOW_USE_SYCL
+Status GemmThunk::ExecuteOnStream(const ExecuteParams& params) {
+  VLOG(3) << "Running GEMM thunk";
+  const BufferAllocations& allocs = *params.buffer_allocations;
+  se::DeviceMemoryBase lhs_data = allocs.GetDeviceAddress(lhs_buffer_);
+  se::DeviceMemoryBase rhs_data = allocs.GetDeviceAddress(rhs_buffer_);
+  se::DeviceMemoryBase output_data = allocs.GetDeviceAddress(output_buffer_);
+  se::DeviceMemoryBase add_data;
+  se::DeviceMemoryBase bias_data;
+
+  auto& buffer_allocations = *params.buffer_allocations;
+  se::OwningScratchAllocator<> scratch_allocator(
+      buffer_allocations.device_ordinal(),
+      buffer_allocations.memory_allocator());
+
+  se::gpu::BlasLt::Epilogue epilogue = se::gpu::BlasLt::Epilogue::kDefault;
+  return RunGemm(config_, lhs_data, rhs_data, add_data, output_data, bias_data,
+                 params.stream, epilogue, &scratch_allocator);
+}
+#endif // !TENSORFLOW_USE_SYCL
 
 absl::Status GemmThunk::Initialize(const InitializeParams& params) {
   if (!params.executor->AsBlas()) {
diff --git a/xla/service/gpu/runtime3/gpublas_lt_matmul_thunk.cc b/xla/service/gpu/runtime3/gpublas_lt_matmul_thunk.cc
index ae0c7eb90..4e76b14b3 100644
--- a/xla/service/gpu/runtime3/gpublas_lt_matmul_thunk.cc
+++ b/xla/service/gpu/runtime3/gpublas_lt_matmul_thunk.cc
@@ -24,6 +24,10 @@ limitations under the License.
 #include "xla/stream_executor/scratch_allocator.h"
 #include "tsl/platform/logging.h"
 
+#if TENSORFLOW_USE_SYCL
+#include "xla/service/gpu/onednn_matmul_utils.h"
+#endif
+
 namespace xla {
 namespace gpu {
 
@@ -52,6 +56,36 @@ CublasLtMatmulThunk::CublasLtMatmulThunk(
       d_scale_buffer_(d_scale),
       d_amax_buffer_(d_amax) {}
 
+#if TENSORFLOW_USE_SYCL
+absl::Status CublasLtMatmulThunk::ExecuteOnStream(const ExecuteParams& params) {
+  VLOG(3) << "Running cublas_lt matmul thunk";
+  const BufferAllocations& allocs = *params.buffer_allocations;
+
+  se::DeviceMemoryBase a, b, c, d;
+  if (a_buffer_.allocation() != nullptr) {
+    a = allocs.GetDeviceAddress(a_buffer_);
+  }
+  if (b_buffer_.allocation() != nullptr) {
+    b = allocs.GetDeviceAddress(b_buffer_);
+  }
+  if (c_buffer_.allocation() != nullptr) {
+    c = allocs.GetDeviceAddress(c_buffer_);
+  }
+  if (d_buffer_.allocation() != nullptr) {
+    d = allocs.GetDeviceAddress(d_buffer_);
+  }
+
+  se::DeviceMemoryBase bias, a_scale, b_scale, c_scale, d_scale, d_amax;
+  if (bias_buffer_.allocation() != nullptr) {
+    bias = allocs.GetDeviceAddress(bias_buffer_);
+  }
+
+  se::OwningScratchAllocator<> scratch_allocator(allocs.device_ordinal(),
+                                                 allocs.memory_allocator());
+  return RunGemm(gemm_config_, a, b, c, d, bias, params.stream, epilogue_,
+                 &scratch_allocator);
+}
+#else  // TENSORFLOW_USE_SYCL
 absl::Status CublasLtMatmulThunk::ExecuteOnStream(const ExecuteParams& params) {
   TF_ASSIGN_OR_RETURN(auto plan, GetMatmulPlan(params.stream));
   TF_ASSIGN_OR_RETURN(auto algorithm, GetMatmulAlgorithm(plan));
@@ -118,6 +152,6 @@ CublasLtMatmulThunk::GetMatmulAlgorithm(
   }
   return it->second;
 }
-
+#endif  // TENSORFLOW_USE_SYCL
 }  // namespace gpu
 }  // namespace xla
diff --git a/xla/service/gpu/runtime3/gpublas_lt_matmul_thunk.h b/xla/service/gpu/runtime3/gpublas_lt_matmul_thunk.h
index e7eaf3a35..dd3d72935 100644
--- a/xla/service/gpu/runtime3/gpublas_lt_matmul_thunk.h
+++ b/xla/service/gpu/runtime3/gpublas_lt_matmul_thunk.h
@@ -45,6 +45,7 @@ class CublasLtMatmulThunk : public Thunk {
   absl::Status ExecuteOnStream(const ExecuteParams& params) override;
 
  private:
+#if !TENSORFLOW_USE_SYCL
   absl::StatusOr<se::gpu::BlasLt::MatmulPlan*> GetMatmulPlan(
       const stream_executor::Stream* stream);
   absl::StatusOr<std::optional<se::gpu::BlasLt::MatmulAlgorithm> >
@@ -59,7 +60,7 @@ class CublasLtMatmulThunk : public Thunk {
   absl::flat_hash_map<const se::gpu::BlasLt::MatmulPlan*,
                       se::gpu::BlasLt::MatmulAlgorithm>
       matmul_algorithm_cache_ ABSL_GUARDED_BY(matmul_algorithm_cache_mutex_);
-
+#endif
   GemmConfig gemm_config_;
   se::gpu::BlasLt::Epilogue epilogue_;
   int64_t algorithm_idx_;
diff --git a/xla/service/gpu/spir_compiler.cc b/xla/service/gpu/spir_compiler.cc
new file mode 100644
index 000000000..93711c700
--- /dev/null
+++ b/xla/service/gpu/spir_compiler.cc
@@ -0,0 +1,281 @@
+/* Copyright (c) 2023 Intel Corporation
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#include "xla/service/gpu/spir_compiler.h"
+
+#include <stdlib.h>
+
+#include <fstream>
+#include <memory>
+#include <string>
+#include <utility>
+#include <vector>
+
+#include "tsl/platform/path.h"
+#include "tsl/platform/status.h"
+#include "tsl/util/env_var.h"
+#include "xla/hlo/ir/hlo_opcode.h"
+#include "xla/service/algebraic_simplifier.h"
+#include "xla/service/call_inliner.h"
+#include "xla/service/convert_mover.h"
+#include "xla/service/dot_dimension_merger.h"
+#include "xla/service/dump.h"
+#include "xla/service/float_normalization.h"
+#include "xla/service/float_support.h"
+#include "xla/service/gpu/backend_configs.pb.h"
+#include "xla/service/gpu/buffer_sharing.h"
+#include "xla/service/gpu/cublas_cudnn.h"
+#include "xla/service/gpu/cudnn_fused_conv_rewriter.h"
+#include "xla/service/gpu/cudnn_fused_mha_rewriter.h"
+#include "xla/service/gpu/cusolver_rewriter.h"
+#include "xla/service/gpu/gemm_impl_picker.h"
+#include "xla/service/gpu/gpu_conv_padding_legalization.h"
+#include "xla/service/gpu/gpu_conv_rewriter.h"
+#include "xla/service/gpu/gpu_layout_assignment.h"
+#include "xla/service/gpu/ir_emission_utils.h"
+#include "xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.h"
+#include "xla/service/gpu/move_copy_to_users.h"
+#include "xla/service/gpu/redundant_convert_mover.h"
+#include "xla/service/gpu/target_constants.h"
+#include "xla/service/gpu/triangular_solve_rewriter.h"
+#include "xla/service/hlo_constant_folding.h"
+#include "xla/service/hlo_cse.h"
+#include "xla/service/hlo_dce.h"
+#include "xla/service/hlo_pass_fix.h"
+#include "xla/service/hlo_pass_pipeline.h"
+#include "xla/service/hlo_verifier.h"
+#include "xla/service/layout_normalization.h"
+#include "xla/service/llvm_ir/llvm_util.h"
+#include "xla/service/reshape_decomposer.h"
+#include "xla/service/reshape_mover.h"
+#include "xla/service/tuple_simplifier.h"
+#include "xla/status_macros.h"
+#include "xla/stream_executor/sycl/hw_info.h"
+#include "xla/stream_executor/sycl/sycl_platform_id.h"
+#include "xla/types.h"
+#include "xla/util.h"
+
+namespace xla {
+namespace gpu {
+namespace {
+
+class ConvBfloat16Support : public FloatSupport {
+ public:
+  explicit ConvBfloat16Support()
+      : FloatSupport(BF16), is_conv_bf16_supported_(true) {}
+
+  bool SupportsLowPrecisionOperand(const HloInstruction& hlo,
+                                   int64_t operand_index) const override {
+    return (hlo.opcode() != HloOpcode::kConvolution) || is_conv_bf16_supported_;
+  }
+
+  bool SupportsLowPrecisionOutput(const HloInstruction& hlo) const override {
+    return (hlo.opcode() != HloOpcode::kConvolution) || is_conv_bf16_supported_;
+  }
+
+  bool SupportsMixedPrecisions(const HloInstruction& hlo) const override {
+    // Skip all HLOs other than convolutions.
+    return (hlo.opcode() != HloOpcode::kConvolution);
+  }
+
+ private:
+  bool is_conv_bf16_supported_;
+};
+
+}  // namespace
+
+absl::Status SPIRCompiler::OptimizeHloConvolutionCanonicalization(
+    HloModule* hlo_module, se::GpuComputeCapability gpu_version,
+    se::dnn::VersionInfo dnn_version,
+    se::DeviceMemoryAllocator* device_allocator) {
+  auto cuda_compute_capability =
+      std::get<se::CudaComputeCapability>(gpu_version);
+  // Convert convolutions into CustomCalls to onednn, then canonicalize them
+  // (GpuConvPaddingLegalization). Also expand cuSolver calls.
+  HloPassPipeline pipeline("conv_canonicalization");
+  pipeline.AddInvariantCheckerDebug<HloVerifier>(
+      /*layout_sensitive=*/false,
+      /*allow_mixed_precision=*/false);
+
+  // Convert upsupported bf16 convolutions to f32.
+  ConvBfloat16Support conv_bf16_support;
+  pipeline.AddPass<FloatNormalization>(&conv_bf16_support);
+
+  pipeline.AddPass<GpusolverRewriter>();
+  pipeline.AddPass<GpuConvRewriter>();
+  pipeline.AddPass<CudnnFusedConvRewriter>(cuda_compute_capability);
+  pipeline.AddPass<GpuConvPaddingLegalization>();
+
+  // The conv padding/vectorization passes which we need to get rid of.  They
+  // also leave behind unnecessary tuple/get-tuple-element pairs that
+  // TupleSimplifier fixes.
+  pipeline.AddPass<CallInliner>();
+  pipeline.AddPass<TupleSimplifier>();
+
+  AlgebraicSimplifierOptions algsimp_options =
+      GetAlgebraicSimplifierOptions(hlo_module->config());
+  algsimp_options.set_enable_conv_operand_swap(false);
+  algsimp_options.set_enable_unconditional_reduce_of_concat_replacement(false);
+  pipeline.AddPass<HloPassFix<AlgebraicSimplifier>>(algsimp_options);
+
+  // tf2xla bridge, DepthwiseConvolutionConverter, GpuConvRewriter, and
+  // CudnnSimplifyPadding introduce reshapes and transposes.  Run ReshapeMover
+  // to a fixed point.  Include algsimp because ReshapeMover relies on it.
+  [&, &pipeline = pipeline.AddPass<HloPassFix<HloPassPipeline>>(
+          "reshape_mover_after_conv_canonicalization")] {
+    ReshapeMoverOptions reshape_mover_options;
+    reshape_mover_options.reshape_of_1d_broadcast_is_cheap = true;
+    pipeline.AddPass<HloPassFix<ReshapeMover>>(reshape_mover_options);
+    pipeline.AddPass<AlgebraicSimplifier>(algsimp_options);
+  }();
+
+  // The reshapes and transposes can possibly be eliminated using
+  // AlgebraicSimplifier. ConvertMover and ReshapeMover fight with each other.
+  // ConvertMover wants to move some converts down the graph, but ReshapeMover
+  // wants to move them up the graph. We run ConvertMover and algsimp to a fixed
+  // point.
+  [&, &pipeline = pipeline.AddPass<HloPassFix<HloPassPipeline>>(
+          "simplify_after_conv_canonicalization")] {
+    pipeline.AddPass<ConvertMover>();
+    pipeline.AddPass<AlgebraicSimplifier>(algsimp_options);
+  }();
+
+  // GpuConvRewriter, GpuConvPaddingLegalization and
+  // CudnnConvPadForTensorCores may add instructions which can be simplified
+  // by constant folding.
+  pipeline.AddPass<HloConstantFolding>();
+  TF_RETURN_IF_ERROR(pipeline.Run(hlo_module).status());
+
+  return absl::OkStatus();
+}
+
+absl::Status SPIRCompiler::OptimizeHloPostLayoutAssignment(
+    HloModule* hlo_module, se::StreamExecutor* stream_exec,
+    const CompileOptions& options, const TargetConfig& gpu_target_config,
+    tsl::thread::ThreadPool* thread_pool) {
+  HloPassPipeline pre_pipeline("spir post-layout_assignment part 1");
+
+  // This needs to run before GemmRewriter, which is part of
+  // OptimizeHloPostLayoutAssignment().
+  auto cuda_compute_capability = std::get<se::CudaComputeCapability>(
+      gpu_target_config.device_description.gpu_compute_capability());
+
+  bool use_mha = true;
+  TF_CHECK_OK(tsl::ReadBoolFromEnvVar("MHA", true, &use_mha));
+  if (use_mha && IsXetlaHardwareSupport()) {
+    HloPassPipeline mha_fusion_pipeline("multi-headed attention fusion");
+    const DebugOptions& debug_options = hlo_module->config().debug_options();
+    // The LayoutAssignment pass may leave behind kCopy instructions which are
+    // duplicate or NOPs, so remove them with algebraic simplification and CSE.
+    AlgebraicSimplifierOptions alg_sim_options;
+    alg_sim_options.set_supports_non_canonical_dots(false);
+    alg_sim_options.set_is_layout_sensitive(true);
+    alg_sim_options.set_enable_conv_operand_swap(false);
+    // "slow" minmax means we propagate nan.
+    alg_sim_options.set_minmax_propagate_nan(
+        !hlo_module->config().debug_options().xla_gpu_enable_fast_min_max());
+    alg_sim_options.set_enable_unconditional_reduce_of_concat_replacement(
+        false);
+    if (debug_options.xla_gpu_normalize_layouts()) {
+      mha_fusion_pipeline.AddPass<ReshapeDecomposer>();
+      mha_fusion_pipeline.AddPass<HloPassFix<MoveCopyToUsers>>();
+      mha_fusion_pipeline.AddPass<LayoutNormalization>();
+    }
+    mha_fusion_pipeline.AddPass<HloCSE>(/*is_layout_sensitive=*/true);
+    mha_fusion_pipeline.AddPass<HloPassFix<AlgebraicSimplifier>>(
+        alg_sim_options);
+    mha_fusion_pipeline.AddPass<HloCSE>(/*is_layout_sensitive=*/true);
+
+    // Rewrite Multi-Headed Attention modules to Fused MHA custom-calls.
+    mha_fusion_pipeline.AddPass<RedundantConvertMover>();
+    mha_fusion_pipeline.AddPass<HloDCE>();
+    mha_fusion_pipeline.AddPass<CudnnFusedMHARewriter>(cuda_compute_capability,
+                                                       stream_exec);
+    mha_fusion_pipeline.AddPass<AlgebraicSimplifier>(alg_sim_options);
+    mha_fusion_pipeline.AddPass<HloDCE>();
+    mha_fusion_pipeline.AddPass<HloCSE>(/*is_layout_sensitive=*/true,
+                                        /*only_fusion_computations*/ false);
+    TF_RETURN_IF_ERROR(mha_fusion_pipeline.Run(hlo_module).status());
+  }
+
+  pre_pipeline.AddPass<DotDimensionMerger>();
+
+  // Padding a gemm operand that's a constant results in pad(constant).  Run
+  // constant-folding to simplify this into a new constant.
+  pre_pipeline.AddPass<HloConstantFolding>();
+  TF_RETURN_IF_ERROR(pre_pipeline.Run(hlo_module).status());
+
+  TF_RETURN_IF_ERROR(GpuCompiler::OptimizeHloPostLayoutAssignment(
+      hlo_module, stream_exec, options, gpu_target_config, thread_pool));
+
+  HloPassPipeline post_pipeline("spir post-layout_assignment part 2");
+
+  // Transform TriangularSolve ops into custom-calls, so we can add temp
+  // memory.
+  post_pipeline.AddPass<TriangularSolveRewriter>();
+
+  TF_RETURN_IF_ERROR(post_pipeline.Run(hlo_module).status());
+
+  return absl::OkStatus();
+}
+
+absl::Status SPIRCompiler::AddConvAndGemmAutotuningPasses(
+    HloPassPipeline* pipeline, HloModule* hlo_module,
+    AutotuneConfig& autotune_config, tsl::thread::ThreadPool* thread_pool) {
+  pipeline->AddPass<GemmAlgorithmPicker>(autotune_config);
+  return OkStatus();
+}
+
+SPIRCompiler::SPIRCompiler()
+    : GpuCompiler(stream_executor::sycl::kSyclPlatformId, spir::TargetTriple(),
+                  spir::DataLayout()) {}
+
+HloDataflowAnalysis::CanShareBuffer SPIRCompiler::GetCanShareBuffer() const {
+  return &CanShareBufferHint;
+}
+
+absl::StatusOr<GpuCompiler::BackendCompileResult>
+SPIRCompiler::CompileTargetBinary(const HloModuleConfig& module_config,
+                                  llvm::Module* llvm_module,
+                                  se::GpuComputeCapability gpu_version,
+                                  bool relocatable,
+                                  const HloModule* debug_module,
+                                  const CompileOptions& options) {
+  if (relocatable) {
+    return Unimplemented("relocatable target binary is not implemented");
+  }
+
+  std::vector<uint8_t> spir;
+  {
+    // This may print multiple lines per HLO compilation because of the
+    // parallelized compilation of LLVM modules.
+    XLA_SCOPED_LOGGING_TIMER_IF(
+        "SPIRCompiler::CompileTargetBinary - CompileToSpir",
+        !options.is_autotuning_compilation);
+    TF_ASSIGN_OR_RETURN(spir,
+                        spir::CompileToSpir(llvm_module, gpu_version,
+                                            module_config.debug_options()));
+  }
+
+  return BackendCompileResult{"", std::move(spir)};
+}
+
+/*static*/ SPIRCompiler* SPIRCompiler::CreateSPIRCompiler() {
+  static auto compiler = absl::make_unique<SPIRCompiler>();
+  return compiler.get();
+}
+
+}  // namespace gpu
+}  // namespace xla
diff --git a/xla/service/gpu/spir_compiler.h b/xla/service/gpu/spir_compiler.h
new file mode 100644
index 000000000..f620a7904
--- /dev/null
+++ b/xla/service/gpu/spir_compiler.h
@@ -0,0 +1,70 @@
+/* Copyright (c) 2023 Intel Corporation
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#ifndef XLA_SERVICE_GPU_SPIR_COMPILER_H_
+#define XLA_SERVICE_GPU_SPIR_COMPILER_H_
+
+#include <string>
+#include <utility>
+#include <vector>
+
+#include "absl/base/call_once.h"
+#include "llvm/IRReader/IRReader.h"
+#include "llvm/Support/SourceMgr.h"
+#include "xla/service/gpu/gpu_compiler.h"
+#include "xla/statusor.h"
+
+namespace xla {
+namespace gpu {
+
+// SPIRCompiler generates efficient GPU executables for NVPTX target.
+class SPIRCompiler : public GpuCompiler {
+ public:
+  SPIRCompiler();
+  ~SPIRCompiler() override {}
+
+  absl::Status OptimizeHloConvolutionCanonicalization(
+      HloModule* hlo_module, se::GpuComputeCapability gpu_version,
+      se::dnn::VersionInfo dnn_version,
+      se::DeviceMemoryAllocator* device_allocator) override;
+
+  absl::Status OptimizeHloPostLayoutAssignment(
+      HloModule* hlo_module, se::StreamExecutor* stream_exec,
+      const CompileOptions& options, const TargetConfig& gpu_target_config,
+      tsl::thread::ThreadPool* thread_pool) override;
+
+  absl::Status AddConvAndGemmAutotuningPasses(
+      HloPassPipeline* pipeline, HloModule* hlo_module,
+      AutotuneConfig& autotune_config,
+      tsl::thread::ThreadPool* thread_pool) override;
+
+  HloDataflowAnalysis::CanShareBuffer GetCanShareBuffer() const override;
+
+  absl::StatusOr<BackendCompileResult> CompileTargetBinary(
+      const HloModuleConfig& module_config, llvm::Module* llvm_module,
+      se::GpuComputeCapability gpu_version, bool relocatable,
+      const HloModule* debug_module, const CompileOptions& options) override;
+
+  static SPIRCompiler* CreateSPIRCompiler();
+
+ private:
+  SPIRCompiler(const SPIRCompiler&) = delete;
+  SPIRCompiler& operator=(const SPIRCompiler&) = delete;
+};
+
+}  // namespace gpu
+}  // namespace xla
+
+#endif  // XLA_SERVICE_GPU_SPIR_COMPILER_H_
diff --git a/xla/service/gpu/spir_compiler_registration.cc b/xla/service/gpu/spir_compiler_registration.cc
new file mode 100644
index 000000000..a397ae7a1
--- /dev/null
+++ b/xla/service/gpu/spir_compiler_registration.cc
@@ -0,0 +1,27 @@
+/* Copyright (c) 2023 Intel Corporation
+
+Copyright 2019 The TensorFlow Authors. All Rights Reserved.
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+==============================================================================*/
+
+#include "xla/service/gpu/spir_compiler.h"
+#include "xla/stream_executor/sycl/sycl_platform_id.h"
+
+static bool InitCompilerModule() {
+  xla::Compiler::RegisterCompilerFactory(
+      stream_executor::sycl::kSyclPlatformId,
+      []() { return std::make_unique<xla::gpu::SPIRCompiler>(); });
+  return true;
+}
+static bool compiler_module_initialized = InitCompilerModule();
diff --git a/xla/service/gpu/stream_executor_util.cc b/xla/service/gpu/stream_executor_util.cc
index 60c6c9fec..5e56adbd8 100644
--- a/xla/service/gpu/stream_executor_util.cc
+++ b/xla/service/gpu/stream_executor_util.cc
@@ -142,6 +142,13 @@ StreamExecutorConvLayoutsToXlaLayouts(const ConvolutionDimensionNumbers& dnums,
                            dnums.kernel_spatial_dimensions().end());
       filter_layout.push_back(dnums.kernel_input_feature_dimension());
       break;
+    case FilterLayout::kYXInputOutput:  // HWIO
+      filter_layout.insert(filter_layout.end(),
+                           dnums.kernel_spatial_dimensions().begin(),
+                           dnums.kernel_spatial_dimensions().end());
+      filter_layout.push_back(dnums.kernel_input_feature_dimension());
+      filter_layout.push_back(dnums.kernel_output_feature_dimension());
+      break;
     default:
       return Internal("Invalid filter layout %s for conv with dnums %s,",
                       FilterLayoutString(filter),
@@ -179,7 +186,7 @@ XlaConvShapesToStreamExecutorLayouts(const ConvolutionDimensionNumbers& dnums,
   Layout nhwc_input, nhwc_filter, nhwc_output;
   std::tie(nhwc_input, nhwc_filter, nhwc_output) =
       StreamExecutorConvLayoutsToXlaLayouts(dnums, DataLayout::kBatchYXDepth,
-                                            FilterLayout::kOutputYXInput,
+                                            FilterLayout::kYXInputOutput,
                                             DataLayout::kBatchYXDepth)
           .value();
 
@@ -228,7 +235,7 @@ XlaConvShapesToStreamExecutorLayouts(const ConvolutionDimensionNumbers& dnums,
           ConvolutionDimensionNumbersToString(dnums), vect_size);
     }
   } else if (LayoutUtil::Equal(filter.layout(), nhwc_filter)) {
-    filter_layout = FilterLayout::kOutputYXInput;
+    filter_layout = FilterLayout::kYXInputOutput;
   } else {
     return Internal(
         "Invalid filter layout %s for conv with dnums %s, expected one of (%s, "
@@ -331,7 +338,8 @@ absl::StatusOr<std::unique_ptr<se::Kernel>> CreateKernel(
 
   if (!cubin_data.empty()) {
     loader_spec.AddCudaCubinInMemory(
-        reinterpret_cast<const char*>(cubin_data.data()), kernel_name);
+        reinterpret_cast<const char*>(cubin_data.data()), cubin_data.size(),
+        kernel_name);
   }
 
   auto kernel_base = std::make_unique<se::Kernel>(stream_exec);
diff --git a/xla/service/gpu/target_constants.h b/xla/service/gpu/target_constants.h
index 13190ae69..574a4120d 100644
--- a/xla/service/gpu/target_constants.h
+++ b/xla/service/gpu/target_constants.h
@@ -67,7 +67,7 @@ inline const char* DataLayout() {
       "e-p:64:64:64-i1:8:8-i8:8:8-i16:16:16-i32:32:32-i64:64:64-f32:"
       "32:32-f64:64:64-v16:16:16-v24:32:32-v32:32:32-v48:64:64-v64:64:64-v96:"
       "128:128-v128:128:128-v192:256:256-v256:256:256-v512:512:512-v1024:1024:"
-      "1024";
+      "1024-n8:16:32:64";
   return kDataLayout;
 }
 }  // namespace spir
diff --git a/xla/service/gpu/target_util.cc b/xla/service/gpu/target_util.cc
index 15d0799b0..8ad26a2dd 100644
--- a/xla/service/gpu/target_util.cc
+++ b/xla/service/gpu/target_util.cc
@@ -23,12 +23,12 @@ limitations under the License.
 #include "llvm/IR/IntrinsicsAMDGPU.h"
 #include "llvm/IR/IntrinsicsNVPTX.h"
 #include "llvm/IR/MDBuilder.h"
+#include "tsl/platform/logging.h"
 #include "xla/hlo/ir/hlo_opcode.h"
 #include "xla/primitive_util.h"
 #include "xla/service/llvm_ir/llvm_type_conversion_util.h"
 #include "xla/service/llvm_ir/llvm_util.h"
 #include "xla/status.h"
-#include "tsl/platform/logging.h"
 
 namespace xla {
 namespace gpu {
diff --git a/xla/service/llvm_ir/fused_ir_emitter.cc b/xla/service/llvm_ir/fused_ir_emitter.cc
index d15499c60..57d876d1b 100644
--- a/xla/service/llvm_ir/fused_ir_emitter.cc
+++ b/xla/service/llvm_ir/fused_ir_emitter.cc
@@ -23,6 +23,7 @@ limitations under the License.
 #include "llvm/IR/IRBuilder.h"
 #include "llvm/IR/Module.h"
 #include "llvm/IR/Value.h"
+#include "llvm/TargetParser/Triple.h"
 #include "xla/hlo/ir/hlo_computation.h"
 #include "xla/hlo/ir/hlo_instruction.h"
 #include "xla/hlo/ir/hlo_opcode.h"
@@ -97,6 +98,8 @@ FusedIrEmitter::IndexedGenerator FusedIrEmitter::HandleConstant(
 
   llvm::Constant* initializer =
       llvm_ir::ConvertLiteralToIrConstant(constant.literal(), module);
+  // SYCL: Hardcode to global addrspace
+  int addrspace = llvm::Triple(module->getTargetTriple()).isSPIR() ? 1 : 0;
   llvm::GlobalVariable* global = new llvm::GlobalVariable(
       *b->GetInsertBlock()->getModule(), initializer->getType(),
       /*isConstant=*/true,
@@ -104,7 +107,7 @@ FusedIrEmitter::IndexedGenerator FusedIrEmitter::HandleConstant(
       /*Initializer=*/initializer,
       /*Name=*/"", /*InsertBefore=*/nullptr,
       /*TLMode=*/llvm::GlobalValue::NotThreadLocal,
-      /*AddressSpace=*/0,
+      /*AddressSpace=*/addrspace,
       /*isExternallyInitialized=*/false);
   global->setUnnamedAddr(llvm::GlobalVariable::UnnamedAddr::Global);
 
diff --git a/xla/service/llvm_ir/ir_array.cc b/xla/service/llvm_ir/ir_array.cc
index 25785d175..30ffbb7c9 100644
--- a/xla/service/llvm_ir/ir_array.cc
+++ b/xla/service/llvm_ir/ir_array.cc
@@ -26,6 +26,7 @@ limitations under the License.
 #include "llvm/IR/Instructions.h"
 #include "llvm/IR/Type.h"
 #include "llvm/IR/Value.h"
+#include "tsl/platform/logging.h"
 #include "xla/layout_util.h"
 #include "xla/permutation_util.h"
 #include "xla/primitive_util.h"
@@ -35,7 +36,6 @@ limitations under the License.
 #include "xla/statusor.h"
 #include "xla/util.h"
 #include "xla/xla_data.pb.h"
-#include "tsl/platform/logging.h"
 
 namespace xla {
 namespace llvm_ir {
@@ -564,7 +564,6 @@ llvm::Value* IrArray::EmitLinearArrayElementAddress(
       llvm::Value* index_operand_1 = linear_index->getOperand(1);
       llvm::Value* ptr_address =
           b->CreateGEP(type, base_ptr_, index_operand_0, "");
-
       return b->CreateInBoundsGEP(type, ptr_address, index_operand_1,
                                   llvm_ir::AsStringRef(name));
     } else {
diff --git a/xla/service/llvm_ir/llvm_util.cc b/xla/service/llvm_ir/llvm_util.cc
index ece29e638..e58250677 100644
--- a/xla/service/llvm_ir/llvm_util.cc
+++ b/xla/service/llvm_ir/llvm_util.cc
@@ -50,6 +50,7 @@ limitations under the License.
 #include "llvm/Support/Casting.h"
 #include "llvm/Support/CodeGen.h"
 #include "llvm/Support/raw_ostream.h"
+#include "llvm/TargetParser/Triple.h"
 #include "llvm/Transforms/Utils/Cloning.h"
 #include "mlir/IR/BuiltinOps.h"  // from @llvm-project
 #include "mlir/IR/Location.h"  // from @llvm-project
@@ -420,8 +421,13 @@ llvm::AllocaInst* EmitAllocaAtFunctionEntryWithCount(llvm::Type* type,
   llvm::Function* function = b->GetInsertBlock()->getParent();
   b->SetInsertPoint(&function->getEntryBlock(),
                     function->getEntryBlock().getFirstInsertionPt());
+  // SYCL: Fix atomic issue by allocating on private addrspace
+  int addrspace =
+      llvm::Triple(b->GetInsertBlock()->getModule()->getTargetTriple()).isSPIR()
+          ? 5
+          : 0;
   llvm::AllocaInst* alloca =
-      b->CreateAlloca(type, element_count, AsStringRef(name));
+      b->CreateAlloca(type, addrspace, element_count, AsStringRef(name));
   if (alignment != 0) {
     alloca->setAlignment(llvm::Align(alignment));
   }
@@ -540,6 +546,7 @@ void SetDereferenceableMetadataForLoad(llvm::LoadInst* load,
 
 llvm::Instruction* AddRangeMetadata(int32_t lower, int32_t upper,
                                     llvm::Instruction* inst) {
+  /* SYCL: This range is for NVPTX target only.
   llvm::LLVMContext& context = inst->getParent()->getContext();
   llvm::IntegerType* i32 = llvm::Type::getInt32Ty(context);
   inst->setMetadata(
@@ -548,6 +555,7 @@ llvm::Instruction* AddRangeMetadata(int32_t lower, int32_t upper,
           context,
           {llvm::ConstantAsMetadata::get(llvm::ConstantInt::get(i32, lower)),
            llvm::ConstantAsMetadata::get(llvm::ConstantInt::get(i32, upper))}));
+  */
   return inst;
 }
 
diff --git a/xla/service/spmd/BUILD b/xla/service/spmd/BUILD
index 843bdce9b..aab03ce67 100644
--- a/xla/service/spmd/BUILD
+++ b/xla/service/spmd/BUILD
@@ -48,6 +48,7 @@ cc_library(
         "//xla:window_util",
         "//xla:xla_data_proto_cc",
         "//xla/client:xla_builder",
+        "//xla/client:xla_computation",
         "//xla/client/lib:comparators",
         "//xla/hlo/ir:hlo",
         "//xla/hlo/ir:hlo_reachability",
@@ -78,6 +79,7 @@ cc_library(
         "@com_google_absl//absl/functional:function_ref",
         "@com_google_absl//absl/log",
         "@com_google_absl//absl/log:check",
+        "@com_google_absl//absl/status:statusor",
         "@com_google_absl//absl/strings",
         "@com_google_absl//absl/types:span",
         "@com_google_absl//absl/utility",
diff --git a/xla/service/spmd/custom_call_handler.cc b/xla/service/spmd/custom_call_handler.cc
index f1cb7277c..a36fa4c73 100644
--- a/xla/service/spmd/custom_call_handler.cc
+++ b/xla/service/spmd/custom_call_handler.cc
@@ -15,28 +15,43 @@ limitations under the License.
 
 #include "xla/service/spmd/custom_call_handler.h"
 
+#include <algorithm>
+#include <cstdint>
+#include <memory>
+#include <string>
+#include <utility>
 #include <vector>
 
-#include "absl/algorithm/container.h"
 #include "absl/container/flat_hash_map.h"
+#include "absl/log/check.h"
+#include "absl/status/statusor.h"
 #include "absl/strings/str_cat.h"
+#include "absl/strings/string_view.h"
+#include "absl/types/span.h"
 #include "xla/client/lib/comparators.h"
 #include "xla/client/xla_builder.h"
+#include "xla/client/xla_computation.h"
+#include "xla/comparison_util.h"
 #include "xla/hlo/ir/hlo_casting_utils.h"
+#include "xla/hlo/ir/hlo_clone_context.h"
 #include "xla/hlo/ir/hlo_computation.h"
 #include "xla/hlo/ir/hlo_instruction.h"
 #include "xla/hlo/ir/hlo_instructions.h"
+#include "xla/hlo/ir/hlo_opcode.h"
 #include "xla/hlo/ir/hlo_sharding.h"
 #include "xla/hlo/utils/hlo_sharding_util.h"
 #include "xla/literal_util.h"
 #include "xla/service/custom_call_sharding_helper.h"
 #include "xla/service/hlo_lexer.h"
-#include "xla/service/shape_inference.h"
+#include "xla/service/hlo_module_config.h"
 #include "xla/service/spmd/spmd_partitioner.h"
 #include "xla/service/spmd/spmd_partitioner_util.h"
+#include "xla/shape.h"
 #include "xla/shape_util.h"
+#include "xla/status.h"
+#include "xla/status_macros.h"
 #include "xla/util.h"
-#include "xla/window_util.h"
+#include "tsl/platform/statusor.h"
 
 namespace xla {
 namespace spmd {
@@ -82,8 +97,11 @@ Status SpmdPartitioningVisitor::HandleCustomCallTopK(HloInstruction* hlo) {
 
   const int64_t batch_dim = 0;
   const int64_t sort_dim = 1;
+
+  CHECK(sharding.IsTiled());
   const int64_t shard_count = sharding.tile_assignment().dim(sort_dim);
   const int64_t batch_dim_partition = sharding.tile_assignment().dim(batch_dim);
+
   const int64_t input_size = hlo->operand(0)->shape().dimensions(sort_dim);
   const int64_t batch_size = hlo->shape().tuple_shapes(0).dimensions(batch_dim);
   const int64_t k = hlo->shape().tuple_shapes(0).dimensions(sort_dim);
@@ -403,10 +421,6 @@ Status SpmdPartitioningVisitor::HandleCustomCall(HloInstruction* hlo) {
     return OkStatus();
   }
 
-  if (hlo->custom_call_target() == "TopK") {
-    return HandleCustomCallTopK(hlo);
-  }
-
   if (hlo->custom_call_target() == kSPMDOpRotateRight) {
     return HandleCustomCallSPMDInternal_RotateRight(hlo);
   }
@@ -439,6 +453,10 @@ Status SpmdPartitioningVisitor::HandleCustomCall(HloInstruction* hlo) {
     return OkStatus();
   }
 
+  if (hlo->custom_call_target() == "TopK") {
+    return HandleCustomCallTopK(hlo);
+  }
+
   return DefaultAction(hlo);
 }
 
diff --git a/xla/service/spmd/spmd_partitioner_test.cc b/xla/service/spmd/spmd_partitioner_test.cc
index db8222c7a..39febfb3d 100644
--- a/xla/service/spmd/spmd_partitioner_test.cc
+++ b/xla/service/spmd/spmd_partitioner_test.cc
@@ -13948,6 +13948,36 @@ ENTRY %entry {
   EXPECT_THAT(topk_operand, op::Shape("bf16[64,128000]{1,0}"));
 }
 
+TEST_P(SpmdPartitioningTest, TopKCustomCallManualSharding) {
+  absl::string_view hlo_string = R"(
+HloModule module
+
+region {
+  Arg_2.22549 = s32[] parameter(2)
+  Arg_3.22550 = s32[] parameter(3)
+  Arg_0.22547 = bf16[] parameter(0)
+  Arg_1.22548 = bf16[] parameter(1)
+  ROOT compare.22551 = pred[] compare(Arg_0.22547, Arg_1.22548), direction=GT, type=TOTALORDER
+}
+
+ENTRY %entry {
+  %p0 = bf16[64,256000]{1,0} parameter(0), sharding={manual}
+  %custom-call = (bf16[64,40]{1,0}, s32[64,40]{1,0}) custom-call(bf16[64,256000]{1,0} %p0), custom_call_target="TopK", called_computations={%region}, sharding={{manual}, {manual}}
+  %get-tuple-element.336 = bf16[64,40]{1,0} get-tuple-element((bf16[64,40]{1,0}, s32[64,40]{1,0}) %custom-call), index=0, sharding={manual}
+})";
+
+  TF_ASSERT_OK_AND_ASSIGN(auto module,
+                          PartitionComputation(hlo_string, /*num_devices=*/2));
+  VLOG(1) << module->ToString();
+  EXPECT_EQ(FindInstruction(module.get(), HloOpcode::kSort), nullptr);
+
+  auto topk_instruction = FindInstruction(module.get(), HloOpcode::kCustomCall);
+  EXPECT_EQ(topk_instruction->custom_call_target(), "TopK");
+  EXPECT_THAT(topk_instruction->operand(0), op::Shape("bf16[64,256000]{1,0}"));
+  EXPECT_THAT(topk_instruction,
+              op::Shape("(bf16[64,40]{1,0}, s32[64,40]{1,0})"));
+}
+
 TEST_P(SpmdPartitioningTest, WindowedEinsumShouldMatchLhs_b305313406) {
   absl::string_view hlo_string = R"(
 HloModule module
diff --git a/xla/stream_executor/blas.h b/xla/stream_executor/blas.h
index 2d5493614..c481880a1 100644
--- a/xla/stream_executor/blas.h
+++ b/xla/stream_executor/blas.h
@@ -137,6 +137,9 @@ constexpr AlgorithmType kDefaultBlasGemv = -3;
 constexpr AlgorithmType kNoAlgorithm = -4;
 constexpr AlgorithmType kRuntimeAutotuning = -5;
 
+constexpr AlgorithmType kXetlaGemm = -6;
+constexpr AlgorithmType kOneDnnGemm = -7;
+
 // blas uses -1 to represent the default algorithm. This happens to match up
 // with the CUBLAS_GEMM_DFALT constant, so cuda_blas.cc is using static_cast
 // to convert from AlgorithmType to cublasGemmAlgo_t, and uses a static_assert
diff --git a/xla/stream_executor/build_defs.bzl b/xla/stream_executor/build_defs.bzl
index a29f7d28d..e77a9b6f3 100644
--- a/xla/stream_executor/build_defs.bzl
+++ b/xla/stream_executor/build_defs.bzl
@@ -2,6 +2,7 @@
 
 load("@local_config_cuda//cuda:build_defs.bzl", "if_cuda_is_configured")
 load("@local_config_rocm//rocm:build_defs.bzl", "if_rocm_is_configured")
+load("@local_config_sycl//sycl:build_defs.bzl", "if_sycl_is_configured")
 
 def stream_executor_friends():
     return ["//..."]
@@ -18,10 +19,10 @@ def tf_additional_cudnn_plugin_copts():
 
 # Returns whether any GPU backend is configuered.
 def if_gpu_is_configured(x):
-    return if_cuda_is_configured(x) + if_rocm_is_configured(x)
+    return if_cuda_is_configured(x) + if_rocm_is_configured(x) + if_sycl_is_configured(x)
 
 def if_cuda_or_rocm(x):
-    return if_gpu_is_configured(x)
+    return if_cuda_is_configured(x) + if_rocm_is_configured(x)
 
 # nvlink is not available via the pip wheels, disable it since it will create
 # unnecessary dependency
diff --git a/xla/stream_executor/cuda/cuda_driver.cc b/xla/stream_executor/cuda/cuda_driver.cc
index 36662e6cc..77335a4e6 100644
--- a/xla/stream_executor/cuda/cuda_driver.cc
+++ b/xla/stream_executor/cuda/cuda_driver.cc
@@ -1400,6 +1400,14 @@ struct BitPatternToValue {
       "Feature not supported on CUDA platform (LoadHsaco)");
 }
 
+/* static */ absl::Status GpuDriver::LoadLevelzero(GpuContext* context,
+                                                  const char* spir_contents,
+                                                  const size_t size,
+                                                  CUmodule* module) {
+  return absl::InternalError(
+      "Feature not supported on CUDA platform (LoadLevelzero)");
+}
+
 /* static */ absl::Status GpuDriver::SynchronousMemsetUint8(
     GpuContext* context, CUdeviceptr location, uint8_t value, size_t size) {
   ScopedActivateContext activation(context);
diff --git a/xla/stream_executor/cuda/cuda_executor.cc b/xla/stream_executor/cuda/cuda_executor.cc
index 2b3f6d414..9ef02730a 100644
--- a/xla/stream_executor/cuda/cuda_executor.cc
+++ b/xla/stream_executor/cuda/cuda_executor.cc
@@ -190,6 +190,12 @@ absl::Status GpuExecutor::LoadModuleFromHsaco(const char* hsaco,
       "Feature not supported on CUDA platform (LoadModuleFromHsaco)");
 }
 
+absl::Status GpuExecutor::LoadModuleFromSpir(const char* spir, const size_t size,
+                                             CUmodule* module) {
+  return absl::InternalError(
+      "Feature not supported on CUDA platform (LoadModuleFromSpir)");
+}
+
 absl::Status GpuExecutor::GetKernel(const MultiKernelLoaderSpec& spec,
                                     Kernel* kernel) {
   GpuKernel* cuda_kernel = AsGpuKernel(kernel);
diff --git a/xla/stream_executor/gpu/BUILD b/xla/stream_executor/gpu/BUILD
index e4a4e3ac0..29f3edc67 100644
--- a/xla/stream_executor/gpu/BUILD
+++ b/xla/stream_executor/gpu/BUILD
@@ -26,6 +26,10 @@ load(
     "if_rocm",
     "if_rocm_is_configured",
 )
+load(
+    "@local_config_sycl//sycl:build_defs.bzl",
+    "if_sycl_is_configured",
+)
 load(
     "@tsl//tsl:tsl.bzl",
     "if_libtpu",
@@ -179,6 +183,8 @@ cc_library(
     ]) + if_rocm_is_configured([
         "//xla/stream_executor/rocm:hip_conditional_kernels",
         "//xla/stream_executor/rocm:hip_noop_kernel",
+    ]) + if_sycl_is_configured([
+        "@intel_extension_for_openxla//xla/stream_executor/sycl:sycl_conditional_kernels",
     ]),
 )
 
@@ -362,6 +368,8 @@ cc_library(
         "@local_config_cuda//cuda:cuda_headers",
     ]) + if_rocm_is_configured([
         "@local_config_rocm//rocm:rocm_headers",
+    ]) + if_sycl_is_configured([
+        "@local_config_sycl//sycl:sycl_headers",
     ]),
 )
 
diff --git a/xla/stream_executor/gpu/gpu_driver.h b/xla/stream_executor/gpu/gpu_driver.h
index 8124a3c20..f9a51732c 100644
--- a/xla/stream_executor/gpu/gpu_driver.h
+++ b/xla/stream_executor/gpu/gpu_driver.h
@@ -628,6 +628,9 @@ class GpuDriver {
   // (supported on ROCm only)
   static absl::Status LoadHsaco(GpuContext* context, const char* hsaco_contents,
                                 GpuModuleHandle* module);
+  static absl::Status LoadLevelzero(GpuContext* context,
+                                    const char* spir_contents, const size_t size,
+                                    GpuModuleHandle* module);
 
   // Retrieves a named kernel from a loaded module, and places the resulting
   // handle into function (outparam) on success. Neither kernel_name nor
diff --git a/xla/stream_executor/gpu/gpu_executor.h b/xla/stream_executor/gpu/gpu_executor.h
index 88b722869..70efb602b 100644
--- a/xla/stream_executor/gpu/gpu_executor.h
+++ b/xla/stream_executor/gpu/gpu_executor.h
@@ -341,6 +341,11 @@ class GpuExecutor : public internal::StreamExecutorInterface {
   absl::Status LoadModuleFromHsaco(const char* hsaco, GpuModuleHandle* module)
       TF_EXCLUSIVE_LOCKS_REQUIRED(in_memory_modules_mu_);
 
+  // (supported on SYCL only)
+  absl::Status LoadModuleFromSpir(const char* spir, const size_t size,
+                                  GpuModuleHandle* module)
+      TF_EXCLUSIVE_LOCKS_REQUIRED(in_memory_modules_mu_);
+
   absl::Status Launch(Stream* stream, const ThreadDim& thread_dims,
                       const BlockDim& block_dims,
                       const std::optional<ClusterDim>& cluster_dims,
diff --git a/xla/stream_executor/gpu/gpu_helpers.h b/xla/stream_executor/gpu/gpu_helpers.h
index 16f18454f..cd2de4ea1 100644
--- a/xla/stream_executor/gpu/gpu_helpers.h
+++ b/xla/stream_executor/gpu/gpu_helpers.h
@@ -52,14 +52,18 @@ T* GpuMemoryMutable(DeviceMemory<T>* mem) {
 static_assert(
     sizeof(std::complex<float>) == sizeof(GpuComplexType),
     "std::complex<float> and GpuComplexType should have the same size");
+#if !TENSORFLOW_USE_SYCL
 static_assert(offsetof(GpuComplexType, x) == 0,
               "The real part of GpuComplexType should appear first.");
+#endif // !TENSORFLOW_USE_SYCL
 static_assert(
     sizeof(std::complex<double>) == sizeof(GpuDoubleComplexType),
     "std::complex<double> and GpuDoubleComplexType should have the same "
     "size");
+#if !TENSORFLOW_USE_SYCL
 static_assert(offsetof(GpuDoubleComplexType, x) == 0,
               "The real part of GpuDoubleComplexType should appear first.");
+#endif // !TENSORFLOW_USE_SYCL
 
 // Type traits to get CUDA complex types from std::complex<>.
 
diff --git a/xla/stream_executor/gpu/gpu_timer.cc b/xla/stream_executor/gpu/gpu_timer.cc
index cda8c5646..0bfdf5034 100644
--- a/xla/stream_executor/gpu/gpu_timer.cc
+++ b/xla/stream_executor/gpu/gpu_timer.cc
@@ -51,10 +51,10 @@ absl::Duration RandomDuration() {
 /*static*/ absl::StatusOr<GpuTimer> GpuTimer::Create(GpuStream* stream) {
   GpuExecutor* parent = stream->parent();
   GpuContext* context = parent->gpu_context();
-  GpuEventHandle start_event;
+  GpuEventHandle start_event = nullptr;
   TF_RETURN_IF_ERROR(GpuDriver::InitEvent(context, &start_event,
                                           GpuDriver::EventFlags::kDefault));
-  GpuEventHandle stop_event;
+  GpuEventHandle stop_event = nullptr;
   TF_RETURN_IF_ERROR(GpuDriver::InitEvent(context, &stop_event,
                                           GpuDriver::EventFlags::kDefault));
   CHECK(start_event != nullptr && stop_event != nullptr);
diff --git a/xla/stream_executor/gpu/gpu_types.h b/xla/stream_executor/gpu/gpu_types.h
index c8d6266b3..c552f1acb 100644
--- a/xla/stream_executor/gpu/gpu_types.h
+++ b/xla/stream_executor/gpu/gpu_types.h
@@ -18,7 +18,18 @@ limitations under the License.
 #ifndef XLA_STREAM_EXECUTOR_GPU_GPU_TYPES_H_
 #define XLA_STREAM_EXECUTOR_GPU_GPU_TYPES_H_
 
-#if TENSORFLOW_USE_ROCM
+#if TENSORFLOW_USE_SYCL
+
+#if __has_include(<sycl/sycl.hpp>)
+#include <sycl/sycl.hpp>
+#elif __has_include(<CL/sycl.hpp>)
+#include <CL/sycl.hpp>
+#else
+#error "Unsupported compiler"
+#endif
+#include <level_zero/ze_api.h>
+
+#elif TENSORFLOW_USE_ROCM
 
 #define __HIP_DISABLE_CPP_FUNCTIONS__
 
@@ -40,7 +51,34 @@ namespace gpu {
 // current CUDA/HIP version.
 struct UnsupportedGpuFeature {};
 
-#if TENSORFLOW_USE_ROCM
+#if TENSORFLOW_USE_SYCL
+typedef struct SYCLEventWrapper {
+  ::sycl::event* event;
+  ::sycl::queue* queue;
+} EventWrapper;
+
+using GpuContextHandle = const void*;
+using GpuStreamHandle = ::sycl::queue*;
+using GpuEventHandle = EventWrapper*;
+using GpuFunctionHandle = ::sycl::kernel*;
+using GpuFunctionAttribute = const void*;
+using GpuDeviceHandle = ::sycl::device*;
+using GpuDevicePtr = void*;
+using GpuDeviceAttribute = const void*;
+using GpuDeviceProperty = const void*;
+using GpuModuleHandle = ze_module_handle_t;
+using GpuStatus = const void*;
+using GpuFuncCachePreference = const void*;
+using GpuSharedMemConfig = const void*;
+using GpuComplexType = std::complex<float>;
+using GpuDoubleComplexType = std::complex<double>;
+using GpuRngHandle = const void*;
+using GpuGraphHandle = const void*;
+using GpuGraphExecHandle = const void*;
+using GpuGraphNodeHandle = const void*;
+using GpuGraphConditionalHandle = UnsupportedGpuFeature;
+
+#elif TENSORFLOW_USE_ROCM
 
 using GpuContextHandle = hipCtx_t;
 using GpuStreamHandle = hipStream_t;
diff --git a/xla/stream_executor/gpu/redzone_allocator.cc b/xla/stream_executor/gpu/redzone_allocator.cc
index aa1643f5d..f7daaac8f 100644
--- a/xla/stream_executor/gpu/redzone_allocator.cc
+++ b/xla/stream_executor/gpu/redzone_allocator.cc
@@ -305,6 +305,7 @@ static absl::StatusOr<RedzoneCheckStatus> CheckRedzonesForBuffer(
   return RedzoneCheckStatus::OK();
 }
 
+#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM
 absl::StatusOr<RedzoneCheckStatus> RedzoneAllocator::CheckRedzones() const {
   StreamExecutor* executor = stream_->parent();
 
@@ -354,6 +355,7 @@ absl::StatusOr<RedzoneCheckStatus> RedzoneAllocator::CheckRedzones() const {
 
   return RedzoneCheckStatus::OK();
 }
+#endif  // GOOGLE_CUDA || TENSORFLOW_USE_ROCM
 
 std::string RedzoneCheckStatus::RedzoneFailureMsg() const {
   return absl::StrFormat(
diff --git a/xla/stream_executor/kernel_spec.cc b/xla/stream_executor/kernel_spec.cc
index 0cafa8280..9b544e767 100644
--- a/xla/stream_executor/kernel_spec.cc
+++ b/xla/stream_executor/kernel_spec.cc
@@ -35,9 +35,9 @@ KernelLoaderSpec::KernelLoaderSpec(absl::string_view kernel_name)
 InProcessSymbol::InProcessSymbol(void *symbol, std::string kernel_name)
     : KernelLoaderSpec(std::move(kernel_name)), symbol_(symbol) {}
 
-CudaCubinInMemory::CudaCubinInMemory(const char *bytes,
+CudaCubinInMemory::CudaCubinInMemory(const char *bytes, int size,
                                      absl::string_view kernel_name)
-    : KernelLoaderSpec(kernel_name), bytes_(bytes) {}
+    : KernelLoaderSpec(kernel_name), size_(size), bytes_(bytes) {}
 
 bool CompareComputeCapability(const std::tuple<int, int> &lhs,
                               const std::tuple<int, int> &rhs) {
@@ -167,9 +167,9 @@ MultiKernelLoaderSpec *MultiKernelLoaderSpec::AddInProcessSymbol(
 }
 
 MultiKernelLoaderSpec *MultiKernelLoaderSpec::AddCudaCubinInMemory(
-    const char *bytes, absl::string_view kernel_name) {
+    const char *bytes, int size, absl::string_view kernel_name) {
   CHECK(cuda_cubin_in_memory_ == nullptr);
-  cuda_cubin_in_memory_.reset(new CudaCubinInMemory{bytes, kernel_name});
+  cuda_cubin_in_memory_.reset(new CudaCubinInMemory{bytes, size, kernel_name});
   return this;
 }
 
diff --git a/xla/stream_executor/kernel_spec.h b/xla/stream_executor/kernel_spec.h
index fa02d7206..13a1b0a60 100644
--- a/xla/stream_executor/kernel_spec.h
+++ b/xla/stream_executor/kernel_spec.h
@@ -186,13 +186,16 @@ class CudaPtxInMemory : public KernelLoaderSpec {
 // Kernel loader specification for a CUBIN blob that resides in memory.
 class CudaCubinInMemory : public KernelLoaderSpec {
  public:
-  CudaCubinInMemory(const char *bytes, absl::string_view kernel_name);
+  CudaCubinInMemory(const char *bytes, int size, absl::string_view kernel_name);
   ~CudaCubinInMemory() override {}
 
   const char *bytes() const { return bytes_; }
+  const int size() const { return size_; }
 
  private:
   const char *bytes_;
+  // SYCL: this is needed only for SPIRV
+  int size_;
 
   CudaCubinInMemory(const CudaCubinInMemory &) = delete;
   void operator=(const CudaCubinInMemory &) = delete;
@@ -246,7 +249,7 @@ class MultiKernelLoaderSpec {
   // mangled by the compiler if it is not declared in an extern "C" scope.
   MultiKernelLoaderSpec *AddInProcessSymbol(void *symbol,
                                             absl::string_view kernel_name);
-  MultiKernelLoaderSpec *AddCudaCubinInMemory(const char *cubin_bytes,
+  MultiKernelLoaderSpec *AddCudaCubinInMemory(const char *cubin_bytes, int size,
                                               absl::string_view kernel_name);
   MultiKernelLoaderSpec *AddCudaPtxInMemory(absl::string_view ptx,
                                             absl::string_view kernel_name);
diff --git a/xla/stream_executor/rocm/rocm_driver.cc b/xla/stream_executor/rocm/rocm_driver.cc
index 02dcac01c..1e240b2ac 100644
--- a/xla/stream_executor/rocm/rocm_driver.cc
+++ b/xla/stream_executor/rocm/rocm_driver.cc
@@ -1165,6 +1165,14 @@ struct BitPatternToValue {
   return ret;
 }
 
+/* static */ absl::Status GpuDriver::LoadLevelzero(GpuContext* context,
+                                                  const char* spir_contents,
+                                                  const size_t size,
+                                                  hipModule_t* module) {
+  return absl::InternalError(
+      "Feature not supported on ROCm platform (LoadLevelzero)");
+}
+
 /* static */ absl::Status GpuDriver::SynchronousMemsetUint8(
     GpuContext* context, hipDeviceptr_t location, uint8 value, size_t size) {
   ScopedActivateContext activation{context};
diff --git a/xla/stream_executor/rocm/rocm_executor.cc b/xla/stream_executor/rocm/rocm_executor.cc
index 6ee1824f9..fa4e64f01 100644
--- a/xla/stream_executor/rocm/rocm_executor.cc
+++ b/xla/stream_executor/rocm/rocm_executor.cc
@@ -461,6 +461,11 @@ absl::Status GpuExecutor::LoadModuleFromHsaco(const char* hsaco,
   return absl::OkStatus();
 }
 
+tsl::Status GpuExecutor::LoadModuleFromSpir(const char* spir, const size_t size,
+                                            hipModule_t* module) {
+  LOG(FATAL) << "Feature not supported on ROCM platform (LoadModuleFromSpir)";
+}
+
 // This is a non-essential operation; if there's a failure, proceed without
 // logging an error. It's nearly certain that in case of failures, we'd never
 // get here in the first place; these are very low-impact routines.
diff --git a/xla/stream_executor/stream_executor_pimpl.h b/xla/stream_executor/stream_executor_pimpl.h
index 2a624ffc7..9ae7931f7 100644
--- a/xla/stream_executor/stream_executor_pimpl.h
+++ b/xla/stream_executor/stream_executor_pimpl.h
@@ -584,7 +584,8 @@ StreamExecutor::CreateTypedKernel(absl::string_view kernel_name,
 
   if (!cubin_data.empty()) {
     loader_spec.AddCudaCubinInMemory(
-        reinterpret_cast<const char*>(cubin_data.data()), kernel_name);
+        reinterpret_cast<const char*>(cubin_data.data()), cubin_data.size(),
+        kernel_name);
   }
 
   TF_RETURN_IF_ERROR(GetKernel(loader_spec, kernel_base.get()));
