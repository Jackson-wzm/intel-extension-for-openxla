diff --git a/third_party/tsl/third_party/grpc/upb_platform_fix.patch b/third_party/tsl/third_party/grpc/upb_platform_fix.patch
index 6edd66067..022c9d155 100644
--- a/third_party/tsl/third_party/grpc/upb_platform_fix.patch
+++ b/third_party/tsl/third_party/grpc/upb_platform_fix.patch
@@ -11,3 +11,12 @@ index ad85b202..2311b2e4 100644
  )
 
  config_setting(
+@@ -24,7 +24,7 @@ exports_files([
+
+ CPPOPTS = [
+     # copybara:strip_for_google3_begin
+-    "-Werror",
++    # "-Werror",
+     "-Wno-long-long",
+     # copybara:strip_end
+ ]
diff --git a/third_party/tsl/third_party/llvm/build.patch b/third_party/tsl/third_party/llvm/build.patch
index bbf8f587a..ab0df933f 100644
--- a/third_party/tsl/third_party/llvm/build.patch
+++ b/third_party/tsl/third_party/llvm/build.patch
@@ -2,6 +2,14 @@ diff --git a/utils/bazel/llvm-project-overlay/llvm/BUILD.bazel b/utils/bazel/llv
 index 2b88729d748b..e12d979b4908 100644
 --- a/utils/bazel/llvm-project-overlay/llvm/BUILD.bazel
 +++ b/utils/bazel/llvm-project-overlay/llvm/BUILD.bazel
+@@ -25,6 +25,7 @@ exports_files(["LICENSE.TXT"])
+ # widely available feature to enable unlimited stack frame instead of using
+ # this `Make` variable.
+ llvm_copts = [
++    "-fvisibility=hidden",
+     "$(STACK_FRAME_UNLIMITED)",
+ ]
+
 @@ -207,13 +207,15 @@ cc_library(
          "lib/Support/BLAKE3/llvm_blake3_prefix.h",
      ] + select({
@@ -44,3 +52,24 @@ index 2b88729d748b..e12d979b4908 100644
          "//conditions:default": [
              "BLAKE3_NO_AVX2",
              "BLAKE3_NO_AVX512",
+diff --git a/mlir/lib/Bytecode/Reader/BytecodeReader.cpp b/mlir/lib/Bytecode/Reader/BytecodeReader.cpp
+index 177372c68046..40d49dc13b2f 100644
+--- a/mlir/lib/Bytecode/Reader/BytecodeReader.cpp
++++ b/mlir/lib/Bytecode/Reader/BytecodeReader.cpp
+@@ -1549,6 +1549,8 @@ private:
+   const std::shared_ptr<llvm::SourceMgr> &bufferOwnerRef;
+ };
+
++// TODO: Disable clang opt since it crashes with clang-17 compiler.
++#pragma clang optimize off
+ LogicalResult BytecodeReader::Impl::read(
+     Block *block, llvm::function_ref<bool(Operation *)> lazyOpsCallback) {
+   EncodingReader reader(buffer.getBuffer(), fileLoc);
+@@ -1628,6 +1630,7 @@ LogicalResult BytecodeReader::Impl::read(
+   // Finally, process the IR section.
+   return parseIRSection(*sectionDatas[bytecode::Section::kIR], block);
+ }
++#pragma clang optimize on
+
+ LogicalResult BytecodeReader::Impl::parseVersion(EncodingReader &reader) {
+   if (failed(reader.parseVarInt(version)))
diff --git a/third_party/tsl/third_party/llvm/opt.patch b/third_party/tsl/third_party/llvm/opt.patch
new file mode 100644
index 000000000..81183a1f8
--- /dev/null
+++ b/third_party/tsl/third_party/llvm/opt.patch
@@ -0,0 +1,18 @@
+diff --git a/llvm/lib/Transforms/Scalar/MemCpyOptimizer.cpp b/llvm/lib/Transforms/Scalar/MemCpyOptimizer.cpp
+index 1a83c713092d..8a00c1f1e553 100644
+--- a/llvm/lib/Transforms/Scalar/MemCpyOptimizer.cpp
++++ b/llvm/lib/Transforms/Scalar/MemCpyOptimizer.cpp
+@@ -1649,9 +1649,10 @@ PreservedAnalyses MemCpyOptPass::run(Function &F, FunctionAnalysisManager &AM) {
+   auto *DT = &AM.getResult<DominatorTreeAnalysis>(F);
+   auto *MSSA = &AM.getResult<MemorySSAAnalysis>(F);
+ 
+-  bool MadeChange = runImpl(F, &TLI, AA, AC, DT, &MSSA->getMSSA());
+-  if (!MadeChange)
+-    return PreservedAnalyses::all();
++  // FIXME: Disable MemCpyOptPass to avoid llvm.memcpy issue
++  // bool MadeChange = runImpl(F, &TLI, AA, AC, DT, &MSSA->getMSSA());
++  // if (!MadeChange)
++  //   return PreservedAnalyses::all();
+ 
+   PreservedAnalyses PA;
+   PA.preserveSet<CFGAnalyses>();
diff --git a/third_party/tsl/third_party/llvm/spirv.patch b/third_party/tsl/third_party/llvm/spirv.patch
new file mode 100644
index 000000000..378e1682d
--- /dev/null
+++ b/third_party/tsl/third_party/llvm/spirv.patch
@@ -0,0 +1,104 @@
+diff --git a/llvm/lib/Passes/PassBuilderPipelines.cpp b/llvm/lib/Passes/PassBuilderPipelines.cpp
+index 34d3b9d7467e..bd26295c6527 100644
+--- a/llvm/lib/Passes/PassBuilderPipelines.cpp
++++ b/llvm/lib/Passes/PassBuilderPipelines.cpp
+@@ -180,6 +180,10 @@ static cl::opt<bool> EnableGlobalAnalyses(
+     "enable-global-analyses", cl::init(true), cl::Hidden,
+     cl::desc("Enable inter-procedural analyses"));
+ 
++static cl::opt<bool>
++    SYCLOptimizationMode("sycl-opt", cl::init(false), cl::Hidden,
++                         cl::desc("Enable SYCL optimization mode."));
++
+ static cl::opt<bool>
+     RunPartialInlining("enable-partial-inlining", cl::init(false), cl::Hidden,
+                        cl::desc("Run Partial inlinining pass"));
+@@ -396,6 +400,7 @@ PassBuilder::buildO1FunctionSimplificationPipeline(OptimizationLevel Level,
+   // Form canonically associated expression trees, and simplify the trees using
+   // basic mathematical properties. For example, this will form (nearly)
+   // minimal multiplication trees.
++  if (!SYCLOptimizationMode) {
+   FPM.addPass(ReassociatePass());
+ 
+   // Add the primary loop simplification pipeline.
+@@ -467,7 +472,7 @@ PassBuilder::buildO1FunctionSimplificationPipeline(OptimizationLevel Level,
+   FPM.addPass(createFunctionToLoopPassAdaptor(std::move(LPM2),
+                                               /*UseMemorySSA=*/false,
+                                               /*UseBlockFrequencyInfo=*/false));
+-
++  }
+   // Delete small array after loop unroll.
+   FPM.addPass(SROAPass(SROAOptions::ModifyCFG));
+ 
+@@ -573,6 +578,7 @@ PassBuilder::buildFunctionSimplificationPipeline(OptimizationLevel Level,
+   // Form canonically associated expression trees, and simplify the trees using
+   // basic mathematical properties. For example, this will form (nearly)
+   // minimal multiplication trees.
++  if (!SYCLOptimizationMode) {
+   FPM.addPass(ReassociatePass());
+ 
+   // Add the primary loop simplification pipeline.
+@@ -647,7 +653,7 @@ PassBuilder::buildFunctionSimplificationPipeline(OptimizationLevel Level,
+   FPM.addPass(createFunctionToLoopPassAdaptor(std::move(LPM2),
+                                               /*UseMemorySSA=*/false,
+                                               /*UseBlockFrequencyInfo=*/false));
+-
++  }
+   // Delete small array after loop unroll.
+   FPM.addPass(SROAPass(SROAOptions::ModifyCFG));
+ 
+@@ -705,6 +711,9 @@ PassBuilder::buildFunctionSimplificationPipeline(OptimizationLevel Level,
+ 
+   invokeScalarOptimizerLateEPCallbacks(FPM, Level);
+ 
++  if (SYCLOptimizationMode)
++    FPM.addPass(SimplifyCFGPass());
++  else
+   FPM.addPass(SimplifyCFGPass(SimplifyCFGOptions()
+                                   .convertSwitchRangeToICmp(true)
+                                   .hoistCommonInsts(true)
+@@ -1338,6 +1347,7 @@ PassBuilder::buildModuleOptimizationPipeline(OptimizationLevel Level,
+ 
+   invokeVectorizerStartEPCallbacks(OptimizePM, Level);
+ 
++  if (!SYCLOptimizationMode) {
+   LoopPassManager LPM;
+   // First rotate loops that may have been un-rotated by prior passes.
+   // Disable header duplication at -Oz.
+@@ -1361,7 +1371,7 @@ PassBuilder::buildModuleOptimizationPipeline(OptimizationLevel Level,
+   OptimizePM.addPass(InjectTLIMappings());
+ 
+   addVectorPasses(Level, OptimizePM, /* IsFullLTO */ false);
+-
++  }
+   // LoopSink pass sinks instructions hoisted by LICM, which serves as a
+   // canonicalization pass that enables other optimizations. As a result,
+   // LoopSink pass needs to be a very late IR pass to avoid undoing LICM
+diff --git a/llvm/lib/Transforms/InstCombine/InstCombineCompares.cpp b/llvm/lib/Transforms/InstCombine/InstCombineCompares.cpp
+index 03bbd22236ff..846f6965984d 100644
+--- a/llvm/lib/Transforms/InstCombine/InstCombineCompares.cpp
++++ b/llvm/lib/Transforms/InstCombine/InstCombineCompares.cpp
+@@ -6479,6 +6479,7 @@ static Instruction *foldReductionIdiom(ICmpInst &I,
+ 
+ Instruction *InstCombinerImpl::visitICmpInst(ICmpInst &I) {
+   bool Changed = false;
++  return Changed ? &I : nullptr;
+   const SimplifyQuery Q = SQ.getWithInstruction(&I);
+   Value *Op0 = I.getOperand(0), *Op1 = I.getOperand(1);
+   unsigned Op0Cplxity = getComplexity(Op0);
+diff --git a/llvm/lib/Transforms/Scalar/SROA.cpp b/llvm/lib/Transforms/Scalar/SROA.cpp
+index f6848d1ea838..eec81017d337 100644
+--- a/llvm/lib/Transforms/Scalar/SROA.cpp
++++ b/llvm/lib/Transforms/Scalar/SROA.cpp
+@@ -1256,9 +1256,9 @@ private:
+     insertUse(I, Offset, Size);
+   }
+ 
+-  void visitPHINode(PHINode &PN) { visitPHINodeOrSelectInst(PN); }
++  // void visitPHINode(PHINode &PN) { visitPHINodeOrSelectInst(PN); }
+ 
+-  void visitSelectInst(SelectInst &SI) { visitPHINodeOrSelectInst(SI); }
++  // void visitSelectInst(SelectInst &SI) { visitPHINodeOrSelectInst(SI); }
+ 
+   /// Disable SROA entirely if there are unhandled users of the alloca.
+   void visitInstruction(Instruction &I) { PI.setAborted(&I); }
diff --git a/third_party/tsl/third_party/llvm/workspace.bzl b/third_party/tsl/third_party/llvm/workspace.bzl
index 0d5a44fe7..a418b02fc 100644
--- a/third_party/tsl/third_party/llvm/workspace.bzl
+++ b/third_party/tsl/third_party/llvm/workspace.bzl
@@ -17,6 +17,8 @@ def repo(name):
         ],
         build_file = "//third_party/llvm:llvm.BUILD",
         patch_file = [
+            "//third_party/llvm:spirv.patch",
+            "//third_party/llvm:opt.patch",
             "//third_party/llvm:generated.patch",  # Autogenerated, don't remove.
             "//third_party/llvm:build.patch",
             "//third_party/llvm:mathextras.patch",
diff --git a/third_party/tsl/tsl/framework/contraction/BUILD b/third_party/tsl/tsl/framework/contraction/BUILD
index 980ca390d..2374e6d10 100644
--- a/third_party/tsl/tsl/framework/contraction/BUILD
+++ b/third_party/tsl/tsl/framework/contraction/BUILD
@@ -104,7 +104,6 @@ cc_library(
         "//tsl:macos_arm64": [],
         "//conditions:default": [
             "TENSORFLOW_USE_CUSTOM_CONTRACTION_KERNEL",
-            "TENSORFLOW_USE_MKLDNN_CONTRACTION_KERNEL",
         ],
     }),
     deps = [
@@ -121,7 +120,7 @@ cc_library(
         "//tsl:linux_ppc64le": [],
         "//tsl:linux_s390x": [],
         "//tsl:macos_arm64": [],
-        "//conditions:default": ["@mkl_dnn_v1//:mkl_dnn"],
+        "//conditions:default": [],
     }),
 )
 
diff --git a/xla/mlir_hlo/lhlo_gpu/IR/lhlo_gpu_ops.td b/xla/mlir_hlo/lhlo_gpu/IR/lhlo_gpu_ops.td
index 72fd56b2a..87451f0c5 100644
--- a/xla/mlir_hlo/lhlo_gpu/IR/lhlo_gpu_ops.td
+++ b/xla/mlir_hlo/lhlo_gpu/IR/lhlo_gpu_ops.td
@@ -306,6 +306,16 @@ def LHLOGPU_AllToAllDoneOp: LHLOGPU_Op<"all_to_all_done"> {
   let arguments = (ins MHLO_Token:$token);
 }
 
+def LHLOGPU_fusedQKVOp : LHLOGPU_Op<"fQKV"> {
+  let arguments = (ins
+    Arg<LHLO_Buffer, "", [MemRead]>:$input,
+    Arg<LHLO_Buffer, "", [MemRead]>:$weight,
+    Arg<LHLO_Buffer, "", [MemWrite]>:$bmm_output1,
+    Arg<LHLO_Buffer, "", [MemWrite]>:$bmm_output2,
+    Arg<LHLO_Buffer, "", [MemWrite]>:$bmm_output3,
+    MHLO_DotDimensionNumbers:$dot_dimension_numbers);
+}
+
 def LHLOGPU_fusedMHAOp : LHLOGPU_Op<"fMHA"> {
   let arguments = (ins
     Arg<LHLO_Buffer, "", [MemRead]>:$lhs_bmm1,
@@ -317,6 +327,7 @@ def LHLOGPU_fusedMHAOp : LHLOGPU_Op<"fMHA"> {
     MHLO_DotDimensionNumbers:$bmm2_dot_dimension_numbers,
     I64ArrayAttr:$intermediate_tensor_dimensions,
     I64ArrayAttr:$intermediate_tensor_layout,
+    F64Attr:$fmha_scale,
     OptionalAttr<F64Attr>:$dropout_rate,
     FusedMhaDagSignatureAttr:$fused_mha_dag,
     FusedMHAAlgorithmConfigAttr:$algorithm_config,
diff --git a/xla/mlir_hlo/lhlo_gpu/IR/lhlo_gpu_ops_enums.td b/xla/mlir_hlo/lhlo_gpu/IR/lhlo_gpu_ops_enums.td
index 2a527ac55..94e0d78be 100644
--- a/xla/mlir_hlo/lhlo_gpu/IR/lhlo_gpu_ops_enums.td
+++ b/xla/mlir_hlo/lhlo_gpu/IR/lhlo_gpu_ops_enums.td
@@ -136,6 +136,7 @@ def FusedMhaDagSoftmaxDropout : I32EnumAttrCase<"SoftmaxDropout", 5>;
 def FusedMhaDagSoftmax : I32EnumAttrCase<"Softmax", 6>;
 def FusedMhaDagScaleBiasSoftmaxDropout : I32EnumAttrCase<"ScaleBiasSoftmaxDropout", 7>;
 def FusedMhaDagScaleBiasSoftmax : I32EnumAttrCase<"ScaleBiasSoftmax", 8>;
+def FusedMhaDagScaleSoftmax : I32EnumAttrCase<"ScaleSoftmax", 9>;
 
 def FusedMhaDagSignature: I32EnumAttr<"FusedMhaDagSignature",
     "DAG configuration for Fused Multi-Headed Attention",
@@ -147,7 +148,8 @@ def FusedMhaDagSignature: I32EnumAttr<"FusedMhaDagSignature",
     FusedMhaDagSoftmaxDropout,
     FusedMhaDagSoftmax,
     FusedMhaDagScaleBiasSoftmaxDropout,
-    FusedMhaDagScaleBiasSoftmax]> {
+    FusedMhaDagScaleBiasSoftmax,
+    FusedMhaDagScaleSoftmax]> {
   let genSpecializedAttr = 0;
   let cppNamespace = "::mlir::lmhlo_gpu";
 }
diff --git a/xla/service/algebraic_simplifier.h b/xla/service/algebraic_simplifier.h
index 32be0a59d..a8dbddd3a 100644
--- a/xla/service/algebraic_simplifier.h
+++ b/xla/service/algebraic_simplifier.h
@@ -400,7 +400,7 @@ class AlgebraicSimplifierVisitor : public DfsHloRewriteVisitor {
   virtual bool IsValidLayout(const Shape& shape) { return true; }
   // Allow backend targets to determine whether a layout is inefficient.
   virtual bool ShouldStrengthReduceDotToReduce(const HloInstruction* hlo) {
-    return true;
+    return false;
   }
 
  protected:
diff --git a/xla/service/dump.cc b/xla/service/dump.cc
index ad67a9e0f..dd6687d38 100644
--- a/xla/service/dump.cc
+++ b/xla/service/dump.cc
@@ -615,6 +615,8 @@ void DumpToFileInDirOrStdout(const HloModule& module, string_view file_prefix,
   if (opts.dumping_to_stdout()) return op->dump();
 
   mlir::OpPrintingFlags print_flags = mlir::OpPrintingFlags().useLocalScope();
+  // Avoid printing large constant weight.
+  print_flags.elideLargeElementsAttrs(8);
   // Enable debug info so that it is easier to see the corresponding HLO node.
   if (file_prefix == "lmhlo") {
     print_flags.enableDebugInfo(/*enable=*/true,
diff --git a/xla/service/gpu/cublas_cudnn.cc b/xla/service/gpu/cublas_cudnn.cc
index 240e69ac4..65ed0bd9b 100644
--- a/xla/service/gpu/cublas_cudnn.cc
+++ b/xla/service/gpu/cublas_cudnn.cc
@@ -39,6 +39,7 @@ bool IsCublasLtMatmulF8(const HloInstruction& hlo) {
          hlo.custom_call_target() == kCublasLtMatmulF8CallTarget;
 }
 
+const absl::string_view kCudnnfQKVCallTarget = "__cudnn$fQKV";
 const absl::string_view kGemmCallTarget = "__cublas$gemm";
 const absl::string_view kCublasLtMatmulCallTarget = "__cublas$lt$matmul";
 const absl::string_view kCublasLtMatmulF8CallTarget = "__cublas$lt$matmul$f8";
@@ -65,6 +66,8 @@ const absl::string_view kCudnnfMHAScaleBiasMaskSoftmaxDropoutCallTarget =
     "__cudnn$fhmaScaleBiasMaskSoftmaxDropout";
 const absl::string_view kCudnnfMHAScaleBiasSoftmaxDropoutCallTarget =
     "__cudnn$fhmaScaleBiasSoftmaxDropout";
+const absl::string_view kCudnnfMHAScaleSoftmaxCallTarget =
+    "__cudnn$fmhaScaleSoftmax";
 const absl::string_view kCudnnfMHAScaleBiasSoftmaxCallTarget =
     "__cudnn$fhmaScaleBiasSoftmax";
 const absl::string_view kCudnnfMHAScaleMaskSoftmaxCallTarget =
@@ -94,6 +97,11 @@ bool IsCudnnConvolutionReorder(const HloInstruction& hlo) {
          target == kCudnnConvReorderFilterAndBiasCallTarget;
 }
 
+bool IsCustomCallTofQKV(const HloInstruction& hlo) {
+  const auto& target = hlo.custom_call_target();
+  return target == kCudnnfQKVCallTarget;
+}
+
 bool IsCustomCallTofMHA(const HloInstruction& hlo) {
   const auto& target = hlo.custom_call_target();
   return target == kCudnnfMHABmmBmmCallTarget ||
@@ -102,6 +110,7 @@ bool IsCustomCallTofMHA(const HloInstruction& hlo) {
          target == kCudnnfMHAScaleBiasMaskSoftmaxDropoutCallTarget ||
          target == kCudnnfMHAScaleMaskSoftmaxCallTarget ||
          target == kCudnnfMHAScaleMaskSoftmaxDropoutCallTarget ||
+         target == kCudnnfMHAScaleSoftmaxCallTarget ||
          target == kCudnnfMHASoftmaxDropoutCallTarget ||
          target == kCudnnfMHAScaleBiasSoftmaxCallTarget ||
          target == kCudnnfMHAScaleBiasSoftmaxDropoutCallTarget;
@@ -150,6 +159,8 @@ StatusOr<CudnnfMHAKind> GetCudnnfMHAKind(
     return CudnnfMHAKind::kScaleMaskSoftmax;
   if (target == kCudnnfMHAScaleMaskSoftmaxDropoutCallTarget)
     return CudnnfMHAKind::kScaleMaskSoftmaxDropout;
+  if (target == kCudnnfMHAScaleSoftmaxCallTarget)
+    return CudnnfMHAKind::kScaleSoftmax;
   if (target == kCudnnfMHASoftmaxDropoutCallTarget)
     return CudnnfMHAKind::kSoftmaxDropout;
   if (target == kCudnnfMHASoftmaxCallTarget) return CudnnfMHAKind::kSoftmax;
@@ -181,6 +192,8 @@ std::string CudnnfMHAKindToString(CudnnfMHAKind kind) {
       return "fmha_bias_softmax_with_dropout";
     case CudnnfMHAKind::kScaleBiasSoftmax:
       return "fmha_bias_softmax";
+    case CudnnfMHAKind::kScaleSoftmax:
+      return "fmha_scale_softmax";
   }
 }
 
diff --git a/xla/service/gpu/cublas_cudnn.h b/xla/service/gpu/cublas_cudnn.h
index 50eddecea..fff26fb91 100644
--- a/xla/service/gpu/cublas_cudnn.h
+++ b/xla/service/gpu/cublas_cudnn.h
@@ -53,6 +53,7 @@ enum class CudnnfMHAKind {
   kScaleMaskSoftmaxDropout,
   kSoftmaxDropout,
   kSoftmax,
+  kScaleSoftmax,
   kScaleBiasSoftmax,
   kScaleBiasSoftmaxDropout,
 };
@@ -152,6 +153,7 @@ bool IsCudnnConvolutionReorder(const HloInstruction& hlo);
 // 6. BMM1 - Softmax - Dropout - BMM2
 // 7. BMM1 - Softmax - BMM2
 // 8. BMM1 - scale - Bias - Softmax - BMM2
+extern const absl::string_view kCudnnfQKVCallTarget;
 extern const absl::string_view kCudnnfMHABmmBmmCallTarget;
 extern const absl::string_view kCudnnfMHASoftmaxCallTarget;
 extern const absl::string_view kCudnnfMHAScaleBiasMaskSoftmaxCallTarget;
@@ -161,9 +163,12 @@ extern const absl::string_view kCudnnfMHAScaleMaskSoftmaxDropoutCallTarget;
 extern const absl::string_view kCudnnfMHASoftmaxDropoutCallTarget;
 extern const absl::string_view kCudnnfMHAScaleBiasSoftmaxDropoutCallTarget;
 extern const absl::string_view kCudnnfMHAScaleBiasSoftmaxCallTarget;
+extern const absl::string_view kCudnnfMHAScaleSoftmaxCallTarget;
 
 bool IsCustomCallTofMHA(const HloInstruction& hlo);
 
+bool IsCustomCallTofQKV(const HloInstruction& hlo);
+
 StatusOr<CudnnfMHAKind> GetCudnnfMHAKind(const HloCustomCallInstruction* instr);
 
 std::string CudnnfMHAKindToString(CudnnfMHAKind kind);
diff --git a/xla/service/gpu/elemental_ir_emitter.cc b/xla/service/gpu/elemental_ir_emitter.cc
index f0709a7f1..ce93976fb 100644
--- a/xla/service/gpu/elemental_ir_emitter.cc
+++ b/xla/service/gpu/elemental_ir_emitter.cc
@@ -323,12 +323,13 @@ StatusOr<llvm::Value*> GpuElementalIrEmitter::EmitTanh(PrimitiveType prim_type,
                 value->getType(), "tanh");
 }
 
-StatusOr<llvm::Value*> GpuElementalIrEmitter::EmitComplexAbs(
-    PrimitiveType prim_type, llvm::Value* value) {
-  return EmitDeviceMathCall(TargetDeviceFunctionID::kHypot,
-                            {EmitExtractReal(value), EmitExtractImag(value)},
-                            {prim_type, prim_type}, prim_type);
-}
+// TODO: hypot has accuracy issue on PVC. Fallback it.
+// StatusOr<llvm::Value*> GpuElementalIrEmitter::EmitComplexAbs(
+//     PrimitiveType prim_type, llvm::Value* value) {
+//   return EmitDeviceMathCall(TargetDeviceFunctionID::kHypot,
+//                             {EmitExtractReal(value), EmitExtractImag(value)},
+//                             {prim_type, prim_type}, prim_type);
+// }
 
 llvm::Value* GpuElementalIrEmitter::EmitThreadId() {
   llvm::Value* block_id = IntCast(
diff --git a/xla/service/gpu/elemental_ir_emitter.h b/xla/service/gpu/elemental_ir_emitter.h
index 12f5f03ac..d9e4a4170 100644
--- a/xla/service/gpu/elemental_ir_emitter.h
+++ b/xla/service/gpu/elemental_ir_emitter.h
@@ -93,8 +93,8 @@ class GpuElementalIrEmitter : public ElementalIrEmitter {
   StatusOr<llvm::Value*> EmitTanh(PrimitiveType prim_type,
                                   llvm::Value* value) override;
 
-  StatusOr<llvm::Value*> EmitComplexAbs(PrimitiveType prim_type,
-                                        llvm::Value* value) override;
+  // StatusOr<llvm::Value*> EmitComplexAbs(PrimitiveType prim_type,
+  //                                       llvm::Value* value) override;
 
   StatusOr<std::vector<llvm::Value*>> EmitThreadLocalCall(
       const HloComputation& callee, absl::Span<llvm::Value* const> parameters,
diff --git a/xla/service/gpu/gpu_layout_assignment.cc b/xla/service/gpu/gpu_layout_assignment.cc
index 590f5ff41..6dcb1939b 100644
--- a/xla/service/gpu/gpu_layout_assignment.cc
+++ b/xla/service/gpu/gpu_layout_assignment.cc
@@ -66,7 +66,7 @@ HeuristicLayoutAssignment(const HloInstruction* instr,
       std::make_tuple(DataLayout::kBatchDepthYX4, FilterLayout::kOutputInputYX4,
                       DataLayout::kBatchDepthYX4);
   constexpr auto kAllNHWC =
-      std::make_tuple(DataLayout::kBatchYXDepth, FilterLayout::kOutputYXInput,
+      std::make_tuple(DataLayout::kBatchYXDepth, FilterLayout::kYXInputOutput,
                       DataLayout::kBatchYXDepth);
 
   // Integer convolution must use NHWC or NCHW_VECT_C.
diff --git a/xla/service/gpu/ir_emission_utils.cc b/xla/service/gpu/ir_emission_utils.cc
index 5603f6b87..9a78219a3 100644
--- a/xla/service/gpu/ir_emission_utils.cc
+++ b/xla/service/gpu/ir_emission_utils.cc
@@ -107,11 +107,11 @@ bool IsMatrixMultiplication(const HloInstruction& dot) {
   const DotDimensionNumbers& dim_numbers = dot.dot_dimension_numbers();
 
   PrimitiveType output_primitive_type = dot.shape().element_type();
+  // Disable F64, C64, C128
   bool type_is_allowed =
       (output_primitive_type == F8E4M3FN || output_primitive_type == F8E5M2 ||
        output_primitive_type == F16 || output_primitive_type == BF16 ||
-       output_primitive_type == F32 || output_primitive_type == F64 ||
-       output_primitive_type == C64 || output_primitive_type == C128) ||
+       output_primitive_type == F32) ||
       (output_primitive_type == S32 && lhs_shape.element_type() == S8 &&
        rhs_shape.element_type() == S8);
   bool shapes_are_valid =
@@ -365,6 +365,31 @@ llvm::Value* EmitNVPTXShflDown(llvm::Value* value, llvm::Value* offset,
       intrinsic, {b->getInt32(-1), value, offset, b->getInt32(WarpSize() - 1)});
 }
 
+// Helper function to emit call to NVPTX shfl_down intrinsic.
+llvm::Value* EmitSPIRShflDown(llvm::Value* value, llvm::Value* offset,
+                              llvm::IRBuilder<>* b) {
+  llvm::Module* module = b->GetInsertBlock()->getModule();
+  llvm::Intrinsic::ID llvm_intrinsic_id;
+  CHECK_EQ(value->getType()->getPrimitiveSizeInBits(), 32);
+  if (value->getType()->isFloatTy()) {
+    return EmitDeviceFunctionCall(
+        "__builtin_spirv_OpSubgroupShuffleDownINTEL_f32_f32_i32",
+        {value, value, offset}, {F32, F32, U32}, F32,
+        llvm::AttrBuilder(b->getContext())
+            .addAttribute(llvm::Attribute::NoUnwind)
+            .addAttribute(llvm::Attribute::Convergent),
+        b);
+  } else {
+    return EmitDeviceFunctionCall(
+        "__builtin_spirv_OpSubgroupShuffleDownINTEL_i32_i32_i32",
+        {value, value, offset}, {U32, U32, U32}, U32,
+        llvm::AttrBuilder(b->getContext())
+            .addAttribute(llvm::Attribute::NoUnwind)
+            .addAttribute(llvm::Attribute::Convergent),
+        b);
+  }
+}
+
 llvm::Value* EmitFullWarpShuffleDown(llvm::Value* value, llvm::Value* offset,
                                      llvm::IRBuilder<>* builder) {
   int bit_width = value->getType()->getPrimitiveSizeInBits();
@@ -377,6 +402,8 @@ llvm::Value* EmitFullWarpShuffleDown(llvm::Value* value, llvm::Value* offset,
       return EmitNVPTXShflDown(value, offset, builder);
     } else if (target_triple.getArch() == llvm::Triple::amdgcn) {
       return EmitAMDGPUShflDown(value, offset, builder);
+    } else if (target_triple.isSPIR()) {
+      return EmitSPIRShflDown(value, offset, builder);
     } else {
       LOG(FATAL) << "Invalid triple " << target_triple.str();
     }
@@ -398,6 +425,9 @@ llvm::Value* EmitFullWarpShuffleDown(llvm::Value* value, llvm::Value* offset,
     } else if (target_triple.getArch() == llvm::Triple::amdgcn) {
       insert_val = EmitAMDGPUShflDown(builder->CreateExtractElement(x, i),
                                       offset, builder);
+    } else if (target_triple.isSPIR()) {
+      insert_val = EmitSPIRShflDown(builder->CreateExtractElement(x, i), offset,
+                                    builder);
     } else {
       LOG(FATAL) << "Invalid triple " << target_triple.str();
     }
diff --git a/xla/service/gpu/matmul_utils.h b/xla/service/gpu/matmul_utils.h
index c46ba6922..dedac9e37 100644
--- a/xla/service/gpu/matmul_utils.h
+++ b/xla/service/gpu/matmul_utils.h
@@ -37,6 +37,23 @@ limitations under the License.
 #include "xla/stream_executor/scratch_allocator.h"
 #endif  // GOOGLE_CUDA
 
+namespace stream_executor {
+namespace cuda {
+namespace BlasLt {
+enum class Epilogue {
+  kDefault = 1,                   // No special postprocessing
+  kReLU = 2,                      // Apply point-wise ReLU function
+  kBias = 4,                      // Add broadcasted bias vector
+  kBiasThenReLU = kBias | kReLU,  // Apply bias and then ReLU transform
+  kGELU = 32,                // Apply GELU point-wise transform to the results
+  kGELUWithAux = 32 | 1024,  // Apply GELU with auxiliary output.
+  kBiasThenGELU = kBias | kGELU,  // Apply bias and then approximate GELU.
+  kBiasThenGELUWithAux = kBiasThenGELU | 1024,
+};
+}
+}  // namespace cuda
+}  // namespace stream_executor
+
 namespace xla {
 namespace gpu {
 
@@ -119,6 +136,7 @@ struct GemmConfig {
   double beta;
   std::optional<int64_t> algorithm;
   int64_t compute_precision;
+  se::cuda::BlasLt::Epilogue epilogue;
 };
 
 StatusOr<se::blas::ComputationType> GetBlasComputationType(
diff --git a/xla/service/gpu/parallel_loop_emitter.cc b/xla/service/gpu/parallel_loop_emitter.cc
index 7577f166f..857f3cf4e 100644
--- a/xla/service/gpu/parallel_loop_emitter.cc
+++ b/xla/service/gpu/parallel_loop_emitter.cc
@@ -178,9 +178,7 @@ ParallelLoopEmitter::EmitIndexAndSetExitBasicBlock(absl::string_view loop_name,
   // for that dimensions.  This helps LLVM generate vectorized codes
   // in that cases.
   llvm::Value* row_index = nullptr;
-  if (!launch_config_.row_vectorized) {
-    array_indices.emplace_back(linear_index_base, shape_, b_);
-  } else {
+  if (launch_config_.row_vectorized) {
     // Simpler index for row computation.
     // This will allow LLVM to vectorize.
     row_index = b_->CreateMul(
@@ -192,19 +190,19 @@ ParallelLoopEmitter::EmitIndexAndSetExitBasicBlock(absl::string_view loop_name,
     array_indices.emplace_back(linear_index_base, multidim, shape_, b_);
   }
 
-  for (int i = 1; i < launch_config_.unroll_factor; ++i) {
+  for (int i = 0; i < launch_config_.unroll_factor; ++i) {
     llvm::Value* linear_index =
         b_->CreateAdd(linear_index_base, llvm::ConstantInt::get(index_type, i),
                       absl::StrCat("linear_index", i),
                       /*HasNUW=*/true, /*HasNSW=*/true);
-    if (!launch_config_.row_vectorized) {
-      array_indices.emplace_back(linear_index, shape_, b_);
-    } else {
+    if (launch_config_.row_vectorized && i > 0) {
       std::vector<llvm::Value*> multidim(shape_.rank(), nullptr);
       multidim.back() = b_->CreateAdd(
           row_index, llvm::ConstantInt::get(index_type, i),
           absl::StrCat("row_index_plus", i), /*HasNUW=*/true, /*HasNSW=*/true);
       array_indices.emplace_back(linear_index, multidim, shape_, b_);
+    } else {
+      array_indices.emplace_back(linear_index, shape_, b_);
     }
   }
 
diff --git a/xla/service/gpu/stream_executor_util.cc b/xla/service/gpu/stream_executor_util.cc
index fb042054f..04121065f 100644
--- a/xla/service/gpu/stream_executor_util.cc
+++ b/xla/service/gpu/stream_executor_util.cc
@@ -140,6 +140,13 @@ StreamExecutorConvLayoutsToXlaLayouts(const ConvolutionDimensionNumbers& dnums,
                            dnums.kernel_spatial_dimensions().end());
       filter_layout.push_back(dnums.kernel_input_feature_dimension());
       break;
+    case FilterLayout::kYXInputOutput:  // HWIO
+      filter_layout.insert(filter_layout.end(),
+                           dnums.kernel_spatial_dimensions().begin(),
+                           dnums.kernel_spatial_dimensions().end());
+      filter_layout.push_back(dnums.kernel_input_feature_dimension());
+      filter_layout.push_back(dnums.kernel_output_feature_dimension());
+      break;
     default:
       return InternalError("Invalid filter layout %s for conv with dnums %s,",
                            FilterLayoutString(filter),
@@ -177,7 +184,7 @@ XlaConvShapesToStreamExecutorLayouts(const ConvolutionDimensionNumbers& dnums,
   Layout nhwc_input, nhwc_filter, nhwc_output;
   std::tie(nhwc_input, nhwc_filter, nhwc_output) =
       StreamExecutorConvLayoutsToXlaLayouts(dnums, DataLayout::kBatchYXDepth,
-                                            FilterLayout::kOutputYXInput,
+                                            FilterLayout::kYXInputOutput,
                                             DataLayout::kBatchYXDepth)
           .value();
 
@@ -226,7 +233,7 @@ XlaConvShapesToStreamExecutorLayouts(const ConvolutionDimensionNumbers& dnums,
           ConvolutionDimensionNumbersToString(dnums), vect_size);
     }
   } else if (LayoutUtil::Equal(filter.layout(), nhwc_filter)) {
-    filter_layout = FilterLayout::kOutputYXInput;
+    filter_layout = FilterLayout::kYXInputOutput;
   } else {
     return InternalError(
         "Invalid filter layout %s for conv with dnums %s, expected one of (%s, "
@@ -329,7 +336,8 @@ StatusOr<std::unique_ptr<se::KernelBase>> CreateKernel(
 
   if (!cubin_data.empty()) {
     loader_spec.AddCudaCubinInMemory(
-        reinterpret_cast<const char*>(cubin_data.data()), kernel_name);
+        reinterpret_cast<const char*>(cubin_data.data()), cubin_data.size(),
+        kernel_name);
   }
 
   auto kernel_base = std::make_unique<se::KernelBase>(stream_exec);
diff --git a/xla/service/gpu/stream_executor_util.h b/xla/service/gpu/stream_executor_util.h
index 4ba827110..0c2167edd 100644
--- a/xla/service/gpu/stream_executor_util.h
+++ b/xla/service/gpu/stream_executor_util.h
@@ -106,6 +106,8 @@ StatusOr<se::dnn::FusedMHAKind> GetDNNFusedMHAKindFromCudnnfMHAKind(
     CudnnfMHAKind kind);
 
 StatusOr<se::dnn::DataType> GetDNNDataTypeFromPrimitiveType(PrimitiveType type);
+StatusOr<se::dnn::FusedMHAKind> GetDNNFusedMHAKindFromCudnnfMHAKind(
+    CudnnfMHAKind kind);
 
 // Returns result with the smallest time which has not failed.
 // If deterministic output is requested, returns first (not failing) result.
diff --git a/xla/service/gpu/target_constants.h b/xla/service/gpu/target_constants.h
index 92f31a6c1..ff12c19d9 100644
--- a/xla/service/gpu/target_constants.h
+++ b/xla/service/gpu/target_constants.h
@@ -54,6 +54,28 @@ inline const char* DataLayout() {
 
 }  // namespace amdgpu
 
+namespace spir {
+// SYCL: The triple that represents our target on SPIR backend.
+inline const char* TargetTriple() {
+  static constexpr char kTargetTriple[] = "spir64-unknown-unknown";
+  return kTargetTriple;
+}
+
+// The data layout of the emitted module.
+inline const char* DataLayout() {
+  // Specifies the address space as global address space
+  // A1: Specifies the address space of objects created by ‘alloca’.
+  // P1: Specifies the address space that corresponds to program memory.
+  // G1: Specifies the address space of global variables.
+  static constexpr char kDataLayout[] =
+      "e-p:64:64:64-i1:8:8-i8:8:8-i16:16:16-i32:32:32-i64:64:64-f32:"
+      "32:32-f64:64:64-v16:16:16-v24:32:32-v32:32:32-v48:64:64-v64:64:64-v96:"
+      "128:128-v128:128:128-v192:256:256-v256:256:256-v512:512:512-v1024:1024:"
+      "1024";
+  return kDataLayout;
+}
+}  // namespace spir
+
 }  // namespace gpu
 }  // namespace xla
 
diff --git a/xla/service/gpu/target_util.cc b/xla/service/gpu/target_util.cc
index 97ca63888..eace46129 100644
--- a/xla/service/gpu/target_util.cc
+++ b/xla/service/gpu/target_util.cc
@@ -45,6 +45,10 @@ struct TargetIntrinsics {
   std::variant<llvm::Intrinsic::ID,
                std::function<llvm::CallInst*(llvm::IRBuilder<>*)>>
       amdgpu_intrinsic_or_function;
+  // SYCL: Target for SPIRV.
+  absl::variant<llvm::Intrinsic::ID,
+                std::function<llvm::CallInst*(llvm::IRBuilder<>*)>>
+      spir_intrinsic_or_function;
 };
 
 // Gets the llvm intrinsic ids on different platforms (NVPTX, AMDGPU)
@@ -52,32 +56,82 @@ struct TargetIntrinsics {
 struct TargetIntrinsics GetIntrinsic(TargetIntrinsicID intrin) {
   switch (intrin) {
     case TargetIntrinsicID::kThreadIdx: {
-      return {llvm::Intrinsic::nvvm_read_ptx_sreg_tid_x,
-              llvm::Intrinsic::amdgcn_workitem_id_x};
+      return {
+          llvm::Intrinsic::nvvm_read_ptx_sreg_tid_x,
+          llvm::Intrinsic::amdgcn_workitem_id_x,
+          [](llvm::IRBuilder<>* b_) -> llvm::CallInst* {
+            return EmitDeviceFunctionCall("_Z12get_local_idj",
+                                          {b_->getInt32(0)}, {U32}, U64,
+                                          {b_->getContext()}, b_);
+          },
+      };
     }
     case TargetIntrinsicID::kThreadIdy: {
-      return {llvm::Intrinsic::nvvm_read_ptx_sreg_tid_y,
-              llvm::Intrinsic::amdgcn_workitem_id_y};
+      return {
+          llvm::Intrinsic::nvvm_read_ptx_sreg_tid_y,
+          llvm::Intrinsic::amdgcn_workitem_id_y,
+          [](llvm::IRBuilder<>* b_) -> llvm::CallInst* {
+            return EmitDeviceFunctionCall("_Z12get_local_idj",
+                                          {b_->getInt32(1)}, {U32}, U64,
+                                          {b_->getContext()}, b_);
+          },
+      };
     }
     case TargetIntrinsicID::kThreadIdz: {
-      return {llvm::Intrinsic::nvvm_read_ptx_sreg_tid_z,
-              llvm::Intrinsic::amdgcn_workitem_id_z};
+      return {
+          llvm::Intrinsic::nvvm_read_ptx_sreg_tid_z,
+          llvm::Intrinsic::amdgcn_workitem_id_z,
+          [](llvm::IRBuilder<>* b_) -> llvm::CallInst* {
+            return EmitDeviceFunctionCall("_Z12get_local_idj",
+                                          {b_->getInt32(2)}, {U32}, U64,
+                                          {b_->getContext()}, b_);
+          },
+      };
     }
     case TargetIntrinsicID::kBlockIdx: {
-      return {llvm::Intrinsic::nvvm_read_ptx_sreg_ctaid_x,
-              llvm::Intrinsic::amdgcn_workgroup_id_x};
+      return {
+          llvm::Intrinsic::nvvm_read_ptx_sreg_ctaid_x,
+          llvm::Intrinsic::amdgcn_workgroup_id_x,
+          [](llvm::IRBuilder<>* b_) -> llvm::CallInst* {
+            return EmitDeviceFunctionCall("_Z12get_group_idj",
+                                          {b_->getInt32(0)}, {U32}, U64,
+                                          {b_->getContext()}, b_);
+          },
+      };
     }
     case TargetIntrinsicID::kBlockIdy: {
-      return {llvm::Intrinsic::nvvm_read_ptx_sreg_ctaid_y,
-              llvm::Intrinsic::amdgcn_workgroup_id_y};
+      return {
+          llvm::Intrinsic::nvvm_read_ptx_sreg_ctaid_y,
+          llvm::Intrinsic::amdgcn_workgroup_id_y,
+          [](llvm::IRBuilder<>* b_) -> llvm::CallInst* {
+            return EmitDeviceFunctionCall("_Z12get_group_idj",
+                                          {b_->getInt32(1)}, {U32}, U64,
+                                          {b_->getContext()}, b_);
+          },
+      };
     }
     case TargetIntrinsicID::kBlockIdz: {
-      return {llvm::Intrinsic::nvvm_read_ptx_sreg_ctaid_z,
-              llvm::Intrinsic::amdgcn_workgroup_id_z};
+      return {
+          llvm::Intrinsic::nvvm_read_ptx_sreg_ctaid_z,
+          llvm::Intrinsic::amdgcn_workgroup_id_z,
+          [](llvm::IRBuilder<>* b_) -> llvm::CallInst* {
+            return EmitDeviceFunctionCall("_Z12get_group_idj",
+                                          {b_->getInt32(2)}, {U32}, U64,
+                                          {b_->getContext()}, b_);
+          },
+      };
     }
     case TargetIntrinsicID::kBarrierId: {
-      return {llvm::Intrinsic::nvvm_barrier0,
-              llvm::Intrinsic::amdgcn_s_barrier};
+      return {llvm::Intrinsic::nvvm_barrier0, llvm::Intrinsic::amdgcn_s_barrier,
+              [](llvm::IRBuilder<>* b_) -> llvm::CallInst* {
+                return EmitDeviceFunctionCall(
+                    "__builtin_spirv_OpControlBarrier_i32_i32_i32",
+                    {b_->getInt32(2), b_->getInt32(2), b_->getInt32(272)},
+                    {U32, U32, U32}, VOID,
+                    llvm::AttrBuilder(b_->getContext())
+                        .addAttribute(llvm::Attribute::Convergent),
+                    b_);
+              }};
     }
     case TargetIntrinsicID::kBlockDimx: {
       return {llvm::Intrinsic::nvvm_read_ptx_sreg_ntid_x,
@@ -85,6 +139,11 @@ struct TargetIntrinsics GetIntrinsic(TargetIntrinsicID intrin) {
                 return EmitDeviceFunctionCall("__ockl_get_local_size",
                                               {b_->getInt32(0)}, {U32}, U64,
                                               {b_->getContext()}, b_);
+              },
+              [](llvm::IRBuilder<>* b_) -> llvm::CallInst* {
+                return EmitDeviceFunctionCall("_Z14get_local_sizej",
+                                              {b_->getInt32(0)}, {U32}, U64,
+                                              {b_->getContext()}, b_);
               }};
     }
     case TargetIntrinsicID::kBlockDimy: {
@@ -93,6 +152,11 @@ struct TargetIntrinsics GetIntrinsic(TargetIntrinsicID intrin) {
                 return EmitDeviceFunctionCall("__ockl_get_local_size",
                                               {b_->getInt32(1)}, {U32}, U64,
                                               {b_->getContext()}, b_);
+              },
+              [](llvm::IRBuilder<>* b_) -> llvm::CallInst* {
+                return EmitDeviceFunctionCall("_Z14get_local_sizej",
+                                              {b_->getInt32(1)}, {U32}, U64,
+                                              {b_->getContext()}, b_);
               }};
     }
     case TargetIntrinsicID::kBlockDimz: {
@@ -101,11 +165,24 @@ struct TargetIntrinsics GetIntrinsic(TargetIntrinsicID intrin) {
                 return EmitDeviceFunctionCall("__ockl_get_local_size",
                                               {b_->getInt32(2)}, {U32}, U64,
                                               {b_->getContext()}, b_);
+              },
+              [](llvm::IRBuilder<>* b_) -> llvm::CallInst* {
+                return EmitDeviceFunctionCall("_Z14get_local_sizej",
+                                              {b_->getInt32(2)}, {U32}, U64,
+                                              {b_->getContext()}, b_);
               }};
     }
     case TargetIntrinsicID::kGroupBarrierId: {
       return {llvm::Intrinsic::nvvm_bar_warp_sync,
-              llvm::Intrinsic::amdgcn_wave_barrier};
+              llvm::Intrinsic::amdgcn_wave_barrier,
+              [](llvm::IRBuilder<>* b_) -> llvm::CallInst* {
+                // TODO: Fix it.
+                return EmitDeviceFunctionCall(
+                    "_Z7barrierj", {b_->getInt32(3)}, {U32}, VOID,
+                    llvm::AttrBuilder(b_->getContext())
+                        .addAttribute(llvm::Attribute::Convergent),
+                    b_);
+              }};
     }
   }
 }
@@ -114,6 +191,7 @@ struct TargetIntrinsics GetIntrinsic(TargetIntrinsicID intrin) {
 struct TargetDeviceFunction {
   const std::string nvptx_root;
   const std::string amdgpu_root;
+  const std::string spir_root;
 };
 
 // Gets the device function name on different platforms (NVPTX, AMDGPU)
@@ -122,52 +200,52 @@ struct TargetDeviceFunction GetDeviceFunctionRoot(
     TargetDeviceFunctionID func_id) {
   switch (func_id) {
     case TargetDeviceFunctionID::kAtan2: {
-      return {"__nv_atan2", "__ocml_atan2"};
+      return {"__nv_atan2", "__ocml_atan2", "_Z5atan2"};
     }
     case TargetDeviceFunctionID::kCos: {
-      return {"__nv_cos", "__ocml_cos"};
+      return {"__nv_cos", "__ocml_cos", "_Z3cos"};
     }
     case TargetDeviceFunctionID::kErfcinv: {
-      return {"__nv_erfcinv", "__ocml_erfcinv"};
+      return {"__nv_erfcinv", "__ocml_erfcinv", "_Z7erfcinv"};
     }
     case TargetDeviceFunctionID::kExp: {
-      return {"__nv_exp", "__ocml_exp"};
+      return {"__nv_exp", "__ocml_exp", "_Z3exp"};
     }
     case TargetDeviceFunctionID::kExpm1: {
-      return {"__nv_expm1", "__ocml_expm1"};
+      return {"__nv_expm1", "__ocml_expm1", "_Z5expm1"};
     }
     case TargetDeviceFunctionID::kFmod: {
-      return {"__nv_fmod", "__ocml_fmod"};
+      return {"__nv_fmod", "__ocml_fmod", "_Z4fmod"};
     }
     case TargetDeviceFunctionID::kHypot: {
-      return {"__nv_hypot", "__ocml_hypot"};
+      return {"__nv_hypot", "__ocml_hypot", "_Z5hypot"};
     }
     case TargetDeviceFunctionID::kLog: {
-      return {"__nv_log", "__ocml_log"};
+      return {"__nv_log", "__ocml_log", "_Z3log"};
     }
     case TargetDeviceFunctionID::kLog1p: {
-      return {"__nv_log1p", "__ocml_log1p"};
+      return {"__nv_log1p", "__ocml_log1p", "_Z5log1p"};
     }
     case TargetDeviceFunctionID::kPow: {
-      return {"__nv_pow", "__ocml_pow"};
+      return {"__nv_pow", "__ocml_pow", "_Z3pow"};
     }
     case TargetDeviceFunctionID::kRound: {
-      return {"__nv_round", "__ocml_round"};
+      return {"__nv_round", "__ocml_round", "_Z5round"};
     }
     case TargetDeviceFunctionID::kRsqrt: {
-      return {"__nv_rsqrt", "__ocml_rsqrt"};
+      return {"__nv_rsqrt", "__ocml_rsqrt", "_Z5rsqrt"};
     }
     case TargetDeviceFunctionID::kSin: {
-      return {"__nv_sin", "__ocml_sin"};
+      return {"__nv_sin", "__ocml_sin", "_Z3sin"};
     }
     case TargetDeviceFunctionID::kSqrt: {
-      return {"__nv_sqrt", "__ocml_sqrt"};
+      return {"__nv_sqrt", "__ocml_sqrt", "_Z4sqrt"};
     }
     case TargetDeviceFunctionID::kTan: {
-      return {"__nv_tan", "__ocml_tan"};
+      return {"__nv_tan", "__ocml_tan", "_Z3tan"};
     }
     case TargetDeviceFunctionID::kTanh: {
-      return {"__nv_tanh", "__ocml_tanh"};
+      return {"__nv_tanh", "__ocml_tanh", "_Z4tanh"};
     }
     case TargetDeviceFunctionID::kCbrt: {
       return {"__nv_cbrt", "__ocml_cbrt"};
@@ -235,6 +313,24 @@ std::string ObtainDeviceFunctionName(TargetDeviceFunctionID func_id,
     } else {
       LOG(FATAL) << "Unexpected type while getting device function name.";
     }
+  } else if (target_triple.isSPIR()) {
+    if (output_type == F32) {
+      if (gpu_root_names.spir_root == "_Z5hypot" ||
+          gpu_root_names.spir_root == "_Z3pow" ||
+          gpu_root_names.spir_root == "_Z5atan2" ||
+          gpu_root_names.spir_root == "_Z4fmod")
+        return StrCat(gpu_root_names.spir_root, "ff");
+      return StrCat(gpu_root_names.spir_root, "f");
+    } else if (output_type == F64) {
+      if (gpu_root_names.spir_root == "_Z5hypot" ||
+          gpu_root_names.spir_root == "_Z3pow" ||
+          gpu_root_names.spir_root == "_Z5atan2" ||
+          gpu_root_names.spir_root == "_Z4fmod")
+        return StrCat(gpu_root_names.spir_root, "dd");
+      return StrCat(gpu_root_names.spir_root, "d");
+    } else {
+      LOG(FATAL) << "Unexpected type while getting device function name.";
+    }
   } else {
     LOG(FATAL) << "Invalid triple " << target_triple.str();
   }
@@ -264,6 +360,11 @@ llvm::CallInst* EmitDeviceFunctionCall(
           .getCallee());
 
   callee->addFnAttrs(attributes);
+  // SYCL: SPIR function with mangling name
+  if (callee_name.find("_Z") != std::string::npos)
+    callee->setCallingConv(llvm::CallingConv::SPIR_FUNC);
+  if (callee_name.find("__builtin_spirv") != std::string::npos)
+    callee->setCallingConv(llvm::CallingConv::SPIR_FUNC);
 
   return b->CreateCall(callee, llvm_ir::AsArrayRef(operands), name.data());
 }
@@ -289,6 +390,18 @@ llvm::CallInst* EmitCallToTargetIntrinsic(
               &gpu_intrinsic_id.amdgpu_intrinsic_or_function);
       return (*builder_func)(b);
     }
+  } else if (target_triple.isSPIR()) {
+    llvm::Intrinsic::ID* llvm_intrinsic_id_ptr =
+        absl::get_if<llvm::Intrinsic::ID>(
+            &gpu_intrinsic_id.spir_intrinsic_or_function);
+    if (llvm_intrinsic_id_ptr) {
+      llvm_intrinsic_id = *llvm_intrinsic_id_ptr;
+    } else {
+      std::function<llvm::CallInst*(llvm::IRBuilder<>*)>* builder_func =
+          absl::get_if<std::function<llvm::CallInst*(llvm::IRBuilder<>*)>>(
+              &gpu_intrinsic_id.spir_intrinsic_or_function);
+      return (*builder_func)(b);
+    }
   } else {
     LOG(FATAL) << "Invalid triple " << target_triple.str();
   }
@@ -316,6 +429,8 @@ void AnnotateFunctionAsGpuKernel(llvm::Module* module, llvm::Function* func,
     // Attach information so AMDGPU can recognize function as a AMDGPU kernel.
     func->setCallingConv(llvm::CallingConv::AMDGPU_KERNEL);
     func->addFnAttr("amdgpu-flat-work-group-size", "1, 1024");
+  } else if (target_triple.isSPIR()) {
+    // Do nothing
   } else {
     LOG(FATAL) << "Invalid triple " << target_triple.str();
   }
diff --git a/xla/service/gpu/thunk.cc b/xla/service/gpu/thunk.cc
index e94af57c6..6ba4429b2 100644
--- a/xla/service/gpu/thunk.cc
+++ b/xla/service/gpu/thunk.cc
@@ -75,6 +75,7 @@ Thunk::ExecuteParams::ExecuteParams(
     CASE(kTriangularSolve);
     CASE(kWhile);
     CASE(kFusedMHA);
+    CASE(kFusedQKV);
   }
 }
 
diff --git a/xla/service/gpu/thunk.h b/xla/service/gpu/thunk.h
index 25803cf7d..c24f8158d 100644
--- a/xla/service/gpu/thunk.h
+++ b/xla/service/gpu/thunk.h
@@ -84,7 +84,8 @@ class Thunk {
     kSequential,
     kTriangularSolve,
     kWhile,
-    kFusedMHA
+    kFusedMHA,
+    kFusedQKV
   };
 
   struct ThunkInfo {
diff --git a/xla/service/llvm_ir/fused_ir_emitter.cc b/xla/service/llvm_ir/fused_ir_emitter.cc
index 006020e41..0add8fbe3 100644
--- a/xla/service/llvm_ir/fused_ir_emitter.cc
+++ b/xla/service/llvm_ir/fused_ir_emitter.cc
@@ -104,7 +104,7 @@ FusedIrEmitter::IndexedGenerator FusedIrEmitter::HandleConstant(
       /*Initializer=*/initializer,
       /*Name=*/"", /*InsertBefore=*/nullptr,
       /*TLMode=*/llvm::GlobalValue::NotThreadLocal,
-      /*AddressSpace=*/0,
+      /*AddressSpace=*/1,  // SYCL: Hardcode to global addrspace
       /*isExternallyInitialized=*/false);
   global->setUnnamedAddr(llvm::GlobalVariable::UnnamedAddr::Global);
 
diff --git a/xla/service/llvm_ir/ir_array.cc b/xla/service/llvm_ir/ir_array.cc
index 7708db56e..6be405a23 100644
--- a/xla/service/llvm_ir/ir_array.cc
+++ b/xla/service/llvm_ir/ir_array.cc
@@ -24,6 +24,7 @@ limitations under the License.
 #include "llvm/IR/Constants.h"
 #include "llvm/IR/Instructions.h"
 #include "llvm/IR/Value.h"
+#include "tsl/platform/logging.h"
 #include "xla/layout_util.h"
 #include "xla/permutation_util.h"
 #include "xla/service/llvm_ir/llvm_type_conversion_util.h"
@@ -32,7 +33,6 @@ limitations under the License.
 #include "xla/statusor.h"
 #include "xla/util.h"
 #include "xla/xla_data.pb.h"
-#include "tsl/platform/logging.h"
 
 namespace xla {
 namespace llvm_ir {
@@ -494,9 +494,24 @@ llvm::Value* IrArray::EmitArrayElementAddress(const IrArray::Index& index,
   if (use_linear_index && index.LinearValidOnShape(shape_)) {
     llvm::Module* module = b->GetInsertBlock()->getParent()->getParent();
     llvm::Type* type = PrimitiveTypeToIrType(shape_.element_type(), module);
-    return b->CreateInBoundsGEP(
-        type, b->CreateBitCast(base_ptr_, type->getPointerTo()), index.linear(),
-        llvm_ir::AsStringRef(name));
+    
+    auto linear_index = dyn_cast<llvm::BinaryOperator>(index.linear());
+    // only separate const offset when having add
+    if (linear_index && (linear_index->getOpcode() == llvm::Instruction::Add)) {
+      llvm::Value* index_operand_0 = linear_index->getOperand(0);
+      // constant index
+      llvm::Value* index_operand_1 = linear_index->getOperand(1);
+      llvm::Value* ptr_address =
+          b->CreateGEP(type, b->CreateBitCast(base_ptr_, type->getPointerTo()),
+                       index_operand_0, "");
+
+      return b->CreateInBoundsGEP(type, ptr_address, index_operand_1,
+                                  llvm_ir::AsStringRef(name));
+    } else {
+      return b->CreateInBoundsGEP(
+          type, b->CreateBitCast(base_ptr_, type->getPointerTo()),
+          index.linear(), llvm_ir::AsStringRef(name));
+    }
   }
 
   std::vector<llvm::Value*> actual_index;
diff --git a/xla/service/llvm_ir/llvm_util.cc b/xla/service/llvm_ir/llvm_util.cc
index 97e6b63e3..a38e2a08c 100644
--- a/xla/service/llvm_ir/llvm_util.cc
+++ b/xla/service/llvm_ir/llvm_util.cc
@@ -235,6 +235,8 @@ llvm::Type* PrimitiveTypeToIrType(PrimitiveType element_type,
       // Tokens do not have a physical representation, but the compiler needs
       // some placeholder type, so use int8_t*.
       return llvm::Type::getInt8PtrTy(module->getContext());
+    case VOID:
+      return llvm::Type::getVoidTy(module->getContext());
     default:
       LOG(FATAL) << "unsupported type " << element_type;
   }
@@ -317,8 +319,9 @@ llvm::AllocaInst* EmitAllocaAtFunctionEntryWithCount(llvm::Type* type,
   llvm::Function* function = b->GetInsertBlock()->getParent();
   b->SetInsertPoint(&function->getEntryBlock(),
                     function->getEntryBlock().getFirstInsertionPt());
+  // SYCL: Fix atomic issue by allocating on private addrspace
   llvm::AllocaInst* alloca =
-      b->CreateAlloca(type, element_count, AsStringRef(name));
+      b->CreateAlloca(type, /*addrspace*/5, element_count, AsStringRef(name));
   if (alignment != 0) {
     alloca->setAlignment(llvm::Align(alignment));
   }
@@ -437,6 +440,7 @@ void SetDereferenceableMetadataForLoad(llvm::LoadInst* load,
 
 llvm::Instruction* AddRangeMetadata(int32_t lower, int32_t upper,
                                     llvm::Instruction* inst) {
+  /* SYCL: This range is for NVPTX target only.
   llvm::LLVMContext& context = inst->getParent()->getContext();
   llvm::IntegerType* i32 = llvm::Type::getInt32Ty(context);
   inst->setMetadata(
@@ -445,6 +449,7 @@ llvm::Instruction* AddRangeMetadata(int32_t lower, int32_t upper,
           context,
           {llvm::ConstantAsMetadata::get(llvm::ConstantInt::get(i32, lower)),
            llvm::ConstantAsMetadata::get(llvm::ConstantInt::get(i32, upper))}));
+  */
   return inst;
 }
 
diff --git a/xla/stream_executor/kernel_spec.cc b/xla/stream_executor/kernel_spec.cc
index a8efbb398..878316ae5 100644
--- a/xla/stream_executor/kernel_spec.cc
+++ b/xla/stream_executor/kernel_spec.cc
@@ -34,9 +34,9 @@ CudaCubinOnDisk::CudaCubinOnDisk(absl::string_view filename,
                                  absl::string_view kernelname)
     : OnDiskKernelLoaderSpec(filename, kernelname) {}
 
-CudaCubinInMemory::CudaCubinInMemory(const char *bytes,
+CudaCubinInMemory::CudaCubinInMemory(const char *bytes, int size,
                                      absl::string_view kernelname)
-    : KernelLoaderSpec(kernelname), bytes_(bytes) {}
+    : KernelLoaderSpec(kernelname), size_(size), bytes_(bytes) {}
 
 bool CompareComputeCapability(const std::tuple<int, int> &lhs,
                               const std::tuple<int, int> &rhs) {
@@ -198,9 +198,9 @@ MultiKernelLoaderSpec *MultiKernelLoaderSpec::AddCudaPtxOnDisk(
 }
 
 MultiKernelLoaderSpec *MultiKernelLoaderSpec::AddCudaCubinInMemory(
-    const char *bytes, absl::string_view kernelname) {
+    const char *bytes, int size, absl::string_view kernelname) {
   CHECK(cuda_cubin_in_memory_ == nullptr);
-  cuda_cubin_in_memory_.reset(new CudaCubinInMemory{bytes, kernelname});
+  cuda_cubin_in_memory_.reset(new CudaCubinInMemory{bytes, size, kernelname});
   return this;
 }
 
diff --git a/xla/stream_executor/kernel_spec.h b/xla/stream_executor/kernel_spec.h
index 8b33a5fe8..9c3ae880b 100644
--- a/xla/stream_executor/kernel_spec.h
+++ b/xla/stream_executor/kernel_spec.h
@@ -258,13 +258,16 @@ class OpenCLTextInMemory : public KernelLoaderSpec {
 // Kernel loader specification for a CUBIN blob that resides in memory.
 class CudaCubinInMemory : public KernelLoaderSpec {
  public:
-  CudaCubinInMemory(const char *bytes, absl::string_view kernelname);
+  CudaCubinInMemory(const char *bytes, int size, absl::string_view kernelname);
   ~CudaCubinInMemory() override {}
 
   const char *bytes() const { return bytes_; }
+  const int size() const { return size_; }
 
  private:
   const char *bytes_;
+  // SYCL: this is needed only for SPIRV
+  int size_;
 
   SE_DISALLOW_COPY_AND_ASSIGN(CudaCubinInMemory);
 };
@@ -338,7 +341,7 @@ class MultiKernelLoaderSpec {
                                           absl::string_view kernelname);
   MultiKernelLoaderSpec *AddCudaCubinOnDisk(absl::string_view filename,
                                             absl::string_view kernelname);
-  MultiKernelLoaderSpec *AddCudaCubinInMemory(const char *cubin_bytes,
+  MultiKernelLoaderSpec *AddCudaCubinInMemory(const char *cubin_bytes, int size,
                                               absl::string_view kernelname);
   MultiKernelLoaderSpec *AddCudaPtxInMemory(absl::string_view ptx,
                                             absl::string_view kernelname);
diff --git a/xla/stream_executor/stream_executor_pimpl.h b/xla/stream_executor/stream_executor_pimpl.h
index c2bb16bce..41a1e3f89 100644
--- a/xla/stream_executor/stream_executor_pimpl.h
+++ b/xla/stream_executor/stream_executor_pimpl.h
@@ -812,7 +812,8 @@ StreamExecutor::CreateTypedKernel(absl::string_view kernel_name,
 
   if (!cubin_data.empty()) {
     loader_spec.AddCudaCubinInMemory(
-        reinterpret_cast<const char*>(cubin_data.data()), kernel_name);
+        reinterpret_cast<const char*>(cubin_data.data()), cubin_data.size(),
+        kernel_name);
   }
 
   TF_RETURN_IF_ERROR(GetKernel(loader_spec, kernel_base.get()));
diff --git a/xla/translate/mhlo_to_lhlo_with_xla/mhlo_to_lhlo_with_xla.cc b/xla/translate/mhlo_to_lhlo_with_xla/mhlo_to_lhlo_with_xla.cc
index 397e7707d..18cdbbb6c 100644
--- a/xla/translate/mhlo_to_lhlo_with_xla/mhlo_to_lhlo_with_xla.cc
+++ b/xla/translate/mhlo_to_lhlo_with_xla/mhlo_to_lhlo_with_xla.cc
@@ -760,6 +760,10 @@ tsl::StatusOr<mlir::Operation*> LhloDialectEmitter::EmitCustomCallOp(
     return EmitDnnfMHA(custom_call_instr);
   }
 
+  if (xla::gpu::IsCustomCallTofQKV(*instr)) {
+    return EmitDnnfQKV(custom_call_instr);
+  }
+
   // For custom call, if there are any token operands or results, they will not
   // be represented in LHLO so we need to remember the mapping. First create
   // operands where each token is replaced with a null Value.
@@ -930,6 +934,8 @@ tsl::StatusOr<lmhlo_gpu::FusedMhaDagSignature> AsLhloFusedMhaDagSignature(
       return lmhlo_gpu::FusedMhaDagSignature::SoftmaxDropout;
     case xla::gpu::CudnnfMHAKind::kSoftmax:
       return lmhlo_gpu::FusedMhaDagSignature::Softmax;
+    case xla::gpu::CudnnfMHAKind::kScaleSoftmax:
+      return lmhlo_gpu::FusedMhaDagSignature::ScaleSoftmax;
     case xla::gpu::CudnnfMHAKind::kScaleBiasSoftmax:
       return lmhlo_gpu::FusedMhaDagSignature::ScaleBiasSoftmax;
     case xla::gpu::CudnnfMHAKind::kScaleBiasSoftmaxDropout:
@@ -1354,12 +1360,16 @@ tsl::StatusOr<Operation*> LhloDialectEmitter::EmitDnnfMHA(
       TF_RETURN_IF_ERROR(GetOrCreateView(custom_call, &operands));
       auto fmha_default =
           CreateOpWithoutAttrs<lmhlo_gpu::fusedMHAOp>(custom_call, operands);
+      fmha_default.setFmhaScaleAttr(
+          builder_.getF64FloatAttr(config.fmha_scale()));
       return set_common_fmha_attributes(fmha_default);
     }
     case xla::gpu::CudnnfMHAKind::kSoftmaxDropout: {
       TF_RETURN_IF_ERROR(GetOrCreateView(custom_call, &operands));
       auto fmha_softmax_dropout =
           CreateOpWithoutAttrs<lmhlo_gpu::fusedMHAOp>(custom_call, operands);
+      fmha_softmax_dropout.setFmhaScaleAttr(
+          builder_.getF64FloatAttr(config.fmha_scale()));
       fmha_softmax_dropout.setDropoutRateAttr(
           builder_.getF64FloatAttr(config.dropout_rate()));
       fmha_softmax_dropout.setSeedAttr(
@@ -1443,9 +1453,39 @@ tsl::StatusOr<Operation*> LhloDialectEmitter::EmitDnnfMHA(
           builder_.getF64FloatAttr(config.fmha_scale()));
       return set_common_fmha_attributes(fmha_bias_softmax);
     }
+    case xla::gpu::CudnnfMHAKind::kScaleSoftmax: {
+      TF_RETURN_IF_ERROR(GetOrCreateView(custom_call, &operands));
+      auto fmha_softmax =
+          CreateOpWithoutAttrs<lmhlo_gpu::fusedMHAOp>(custom_call, operands);
+      fmha_softmax.setFmhaScaleAttr(
+          builder_.getF64FloatAttr(config.fmha_scale()));
+      return set_common_fmha_attributes(fmha_softmax);
+    }
   }
 }
 
+tsl::StatusOr<Operation*> LhloDialectEmitter::EmitDnnfQKV(
+    const HloCustomCallInstruction* custom_call) {
+  
+  TF_ASSIGN_OR_RETURN(
+      auto const config,
+      custom_call->backend_config<xla::gpu::GemmBackendConfig>());
+
+  if (custom_call->operand_count() != 2) {
+    return xla::InvalidArgument("GEMM custom call should have 2 operands");
+  }
+
+  // QKV GEMM have two operands.
+  TF_ASSIGN_OR_RETURN(
+      lmhlo_gpu::fusedQKVOp op,
+      CreateOpWithoutAttrs<lmhlo_gpu::fusedQKVOp>(custom_call,
+                                              /*num_operands=*/2));
+  op.setDotDimensionNumbersAttr(
+      GetDotDimensionNumbersAttr(builder_, config.dot_dimension_numbers()));
+  
+  return op.getOperation();
+}
+
 // Convert an XLA HLO constant to a global_memref + get_global_memref pair.
 tsl::StatusOr<mlir::memref::GetGlobalOp> LhloDialectEmitter::EmitConstant(
     const HloInstruction* instr) {
diff --git a/xla/translate/mhlo_to_lhlo_with_xla/mhlo_to_lhlo_with_xla.h b/xla/translate/mhlo_to_lhlo_with_xla/mhlo_to_lhlo_with_xla.h
index 7022b416d..8e0fa63eb 100644
--- a/xla/translate/mhlo_to_lhlo_with_xla/mhlo_to_lhlo_with_xla.h
+++ b/xla/translate/mhlo_to_lhlo_with_xla/mhlo_to_lhlo_with_xla.h
@@ -84,6 +84,8 @@ class LhloDialectEmitter : public xla::ConstDfsHloVisitorWithDefault {
       const xla::HloCustomCallInstruction* custom_call);
   xla::StatusOr<Operation*> EmitDnnfMHA(
       const xla::HloCustomCallInstruction* custom_call);
+  xla::StatusOr<Operation*> EmitDnnfQKV(
+      const xla::HloCustomCallInstruction* custom_call);    
 
   tsl::StatusOr<memref::GetGlobalOp> EmitConstant(
       const xla::HloInstruction* instr);
diff --git a/xla/xla_data.proto b/xla/xla_data.proto
index c4c0f97d2..33b7e98ed 100644
--- a/xla/xla_data.proto
+++ b/xla/xla_data.proto
@@ -28,6 +28,7 @@ enum PrimitiveType {
   // Invalid primitive type to serve as default.
   PRIMITIVE_TYPE_INVALID = 0;
 
+  VOID = 100;
   // Predicates are two-state booleans.
   PRED = 1;
 
