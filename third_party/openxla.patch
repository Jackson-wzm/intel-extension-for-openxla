diff --git a/third_party/tsl/third_party/eigen3/eigen.patch b/third_party/tsl/third_party/eigen3/eigen.patch
new file mode 100644
index 000000000..8954b1d2e
--- /dev/null
+++ b/third_party/tsl/third_party/eigen3/eigen.patch
@@ -0,0 +1,13 @@
+diff --git a/Eigen/src/Core/util/Macros.h b/Eigen/src/Core/util/Macros.h
+index 69bbf2e73..251053b55 100644
+--- a/Eigen/src/Core/util/Macros.h
++++ b/Eigen/src/Core/util/Macros.h
+@@ -686,7 +686,7 @@
+ // For instance, if compiling with gcc and -std=c++17, then EIGEN_COMP_CXXVER
+ // is defined to 17.
+ #if EIGEN_CPLUSPLUS >= 202002L
+-  #define EIGEN_COMP_CXXVER 20
++  #define EIGEN_COMP_CXXVER 17
+ #elif EIGEN_CPLUSPLUS >= 201703L
+   #define EIGEN_COMP_CXXVER 17
+ #elif EIGEN_CPLUSPLUS >= 201402L
diff --git a/third_party/tsl/third_party/eigen3/workspace.bzl b/third_party/tsl/third_party/eigen3/workspace.bzl
index 027454e46..ca5489940 100644
--- a/third_party/tsl/third_party/eigen3/workspace.bzl
+++ b/third_party/tsl/third_party/eigen3/workspace.bzl
@@ -14,6 +14,7 @@ def repo():
     tf_http_archive(
         name = "eigen_archive",
         build_file = "//third_party/eigen3:eigen_archive.BUILD",
+        patch_file = ["//third_party/eigen3:eigen.patch"],
         sha256 = EIGEN_SHA256,
         strip_prefix = "eigen-{commit}".format(commit = EIGEN_COMMIT),
         urls = tf_mirror_urls("https://gitlab.com/libeigen/eigen/-/archive/{commit}/eigen-{commit}.tar.gz".format(commit = EIGEN_COMMIT)),
diff --git a/third_party/tsl/third_party/gpus/cuda_configure.bzl b/third_party/tsl/third_party/gpus/cuda_configure.bzl
index 7e9c443cd..88bfb2137 100644
--- a/third_party/tsl/third_party/gpus/cuda_configure.bzl
+++ b/third_party/tsl/third_party/gpus/cuda_configure.bzl
@@ -25,9 +25,9 @@
   * `PYTHON_BIN_PATH`: The python binary path
 """
 
-load("//third_party/clang_toolchain:download_clang.bzl", "download_clang")
+load("@xla//third_party/tsl/third_party/clang_toolchain:download_clang.bzl", "download_clang")
 load(
-    "//third_party/remote_config:common.bzl",
+    "@xla//third_party/tsl/third_party/remote_config:common.bzl",
     "config_repo_label",
     "err_out",
     "execute",
diff --git a/third_party/tsl/third_party/grpc/c_ares.patch b/third_party/tsl/third_party/grpc/c_ares.patch
new file mode 100644
index 000000000..752965f93
--- /dev/null
+++ b/third_party/tsl/third_party/grpc/c_ares.patch
@@ -0,0 +1,200 @@
+diff --git a/bazel/grpc_deps.bzl b/bazel/grpc_deps.bzl
+index c2f2f43820..72de3cea94 100644
+--- a/bazel/grpc_deps.bzl
++++ b/bazel/grpc_deps.bzl
+@@ -182,9 +182,9 @@ def grpc_deps():
+         http_archive(
+             name = "com_github_cares_cares",
+             build_file = "@com_github_grpc_grpc//third_party:cares/cares.BUILD",
+-            sha256 = "e8c2751ddc70fed9dc6f999acd92e232d5846f009ee1674f8aee81f19b2b915a",
+-            strip_prefix = "c-ares-e982924acee7f7313b4baa4ee5ec000c5e373c30",
+-            url = "https://github.com/c-ares/c-ares/archive/e982924acee7f7313b4baa4ee5ec000c5e373c30.tar.gz",
++            sha256 = "321700399b72ed0e037d0074c629e7741f6b2ec2dda92956abe3e9671d3e268e",
++            strip_prefix = "c-ares-1.19.1",
++            url = "https://github.com/c-ares/c-ares/releases/download/cares-1_19_1/c-ares-1.19.1.tar.gz",
+         )
+
+     if "com_google_absl" not in native.existing_rules():
+diff --git a/third_party/cares/cares.BUILD b/third_party/cares/cares.BUILD
+index 203712b182..2561b1a4bc 100644
+--- a/third_party/cares/cares.BUILD
++++ b/third_party/cares/cares.BUILD
+@@ -109,84 +109,95 @@ genrule(
+ cc_library(
+     name = "ares",
+     srcs = [
+-        "ares__close_sockets.c",
+-        "ares__get_hostent.c",
+-        "ares__read_line.c",
+-        "ares__timeval.c",
+-        "ares_cancel.c",
+-        "ares_create_query.c",
+-        "ares_data.c",
+-        "ares_destroy.c",
+-        "ares_expand_name.c",
+-        "ares_expand_string.c",
+-        "ares_fds.c",
+-        "ares_free_hostent.c",
+-        "ares_free_string.c",
+-        "ares_getenv.c",
+-        "ares_gethostbyaddr.c",
+-        "ares_gethostbyname.c",
+-        "ares_getnameinfo.c",
+-        "ares_getopt.c",
+-        "ares_getsock.c",
+-        "ares_init.c",
+-        "ares_library_init.c",
+-        "ares_llist.c",
+-        "ares_mkquery.c",
+-        "ares_nowarn.c",
+-        "ares_options.c",
+-        "ares_parse_a_reply.c",
+-        "ares_parse_aaaa_reply.c",
+-        "ares_parse_mx_reply.c",
+-        "ares_parse_naptr_reply.c",
+-        "ares_parse_ns_reply.c",
+-        "ares_parse_ptr_reply.c",
+-        "ares_parse_soa_reply.c",
+-        "ares_parse_srv_reply.c",
+-        "ares_parse_txt_reply.c",
+-        "ares_platform.c",
+-        "ares_process.c",
+-        "ares_query.c",
+-        "ares_search.c",
+-        "ares_send.c",
+-        "ares_strcasecmp.c",
+-        "ares_strdup.c",
+-        "ares_strsplit.c",
+-        "ares_strerror.c",
+-        "ares_timeout.c",
+-        "ares_version.c",
+-        "ares_writev.c",
+-        "bitncmp.c",
+-        "inet_net_pton.c",
+-        "inet_ntop.c",
+-        "windows_port.c",
++        "src/lib/ares__read_line.c",
++        "src/lib/ares__get_hostent.c",
++        "src/lib/ares__close_sockets.c",
++        "src/lib/ares__timeval.c",
++        "src/lib/ares_gethostbyaddr.c",
++        "src/lib/ares_getenv.c",
++        "src/lib/ares_free_string.c",
++        "src/lib/ares_free_hostent.c",
++        "src/lib/ares_fds.c",
++        "src/lib/ares_expand_string.c",
++        "src/lib/ares_create_query.c",
++        "src/lib/ares_cancel.c",
++        "src/lib/ares_android.c",
++        "src/lib/ares_parse_txt_reply.c",
++        "src/lib/ares_parse_srv_reply.c",
++        "src/lib/ares_parse_soa_reply.c",
++        "src/lib/ares_parse_ptr_reply.c",
++        "src/lib/ares_parse_ns_reply.c",
++        "src/lib/ares_parse_naptr_reply.c",
++        "src/lib/ares_parse_mx_reply.c",
++        "src/lib/ares_parse_caa_reply.c",
++        "src/lib/ares_options.c",
++        "src/lib/ares_nowarn.c",
++        "src/lib/ares_mkquery.c",
++        "src/lib/ares_llist.c",
++        "src/lib/ares_getsock.c",
++        "src/lib/ares_getnameinfo.c",
++        "src/lib/bitncmp.c",
++        "src/lib/ares_writev.c",
++        "src/lib/ares_version.c",
++        "src/lib/ares_timeout.c",
++        "src/lib/ares_strerror.c",
++        "src/lib/ares_strcasecmp.c",
++        "src/lib/ares_search.c",
++        "src/lib/ares_platform.c",
++        "src/lib/windows_port.c",
++        "src/lib/inet_ntop.c",
++        "src/lib/ares__sortaddrinfo.c",
++        "src/lib/ares__readaddrinfo.c",
++        "src/lib/ares_parse_uri_reply.c",
++        "src/lib/ares__parse_into_addrinfo.c",
++        "src/lib/ares_parse_a_reply.c",
++        "src/lib/ares_parse_aaaa_reply.c",
++        "src/lib/ares_library_init.c",
++        "src/lib/ares_init.c",
++        "src/lib/ares_gethostbyname.c",
++        "src/lib/ares_getaddrinfo.c",
++        "src/lib/ares_freeaddrinfo.c",
++        "src/lib/ares_expand_name.c",
++        "src/lib/ares_destroy.c",
++        "src/lib/ares_data.c",
++        "src/lib/ares__addrinfo_localhost.c",
++        "src/lib/ares__addrinfo2hostent.c",
++        "src/lib/inet_net_pton.c",
++        "src/lib/ares_strsplit.c",
++        "src/lib/ares_strdup.c",
++        "src/lib/ares_send.c",
++        "src/lib/ares_rand.c",
++        "src/lib/ares_query.c",
++        "src/lib/ares_process.c",
+     ],
+     hdrs = [
+-        "ares.h",
+         "ares_build.h",
+         "ares_config.h",
+-        "ares_data.h",
+-        "ares_dns.h",
+-        "ares_getenv.h",
+-        "ares_getopt.h",
+-        "ares_inet_net_pton.h",
+-        "ares_iphlpapi.h",
+-        "ares_ipv6.h",
+-        "ares_library_init.h",
+-        "ares_llist.h",
+-        "ares_nowarn.h",
+-        "ares_platform.h",
+-        "ares_private.h",
+-        "ares_rules.h",
+-        "ares_setup.h",
+-        "ares_strcasecmp.h",
+-        "ares_strdup.h",
+-        "ares_strsplit.h",
+-        "ares_version.h",
+-        "ares_writev.h",
+-        "bitncmp.h",
+-        "config-win32.h",
+-        "nameser.h",
+-        "setup_once.h",
++        "include/ares_version.h",
++        "include/ares.h",
++        "include/ares_rules.h",
++        "include/ares_dns.h",
++        "include/ares_nameser.h",
++        "src/tools/ares_getopt.h",
++        "src/lib/ares_strsplit.h",
++        "src/lib/ares_android.h",
++        "src/lib/ares_private.h",
++        "src/lib/ares_llist.h",
++        "src/lib/ares_platform.h",
++        "src/lib/ares_ipv6.h",
++        "src/lib/config-dos.h",
++        "src/lib/bitncmp.h",
++        "src/lib/ares_strcasecmp.h",
++        "src/lib/setup_once.h",
++        "src/lib/ares_inet_net_pton.h",
++        "src/lib/ares_data.h",
++        "src/lib/ares_getenv.h",
++        "src/lib/config-win32.h",
++        "src/lib/ares_strdup.h",
++        "src/lib/ares_iphlpapi.h",
++        "src/lib/ares_setup.h",
++        "src/lib/ares_writev.h",
++        "src/lib/ares_nowarn.h",
+     ],
+     copts = [
+         "-D_GNU_SOURCE",
+@@ -202,7 +213,7 @@ cc_library(
+         "//conditions:default": [],
+     }),
+     defines = ["CARES_STATICLIB"],
+-    includes = ["."],
++    includes = ["include", "."],
+     linkopts = select({
+         ":windows": ["-defaultlib:ws2_32.lib"],
+         "//conditions:default": [],
diff --git a/third_party/tsl/third_party/grpc/upb_platform_fix.patch b/third_party/tsl/third_party/grpc/upb_platform_fix.patch
index 6edd66067..022c9d155 100644
--- a/third_party/tsl/third_party/grpc/upb_platform_fix.patch
+++ b/third_party/tsl/third_party/grpc/upb_platform_fix.patch
@@ -11,3 +11,12 @@ index ad85b202..2311b2e4 100644
  )
 
  config_setting(
+@@ -24,7 +24,7 @@ exports_files([
+
+ CPPOPTS = [
+     # copybara:strip_for_google3_begin
+-    "-Werror",
++    # "-Werror",
+     "-Wno-long-long",
+     # copybara:strip_end
+ ]
diff --git a/third_party/tsl/third_party/llvm/build.patch b/third_party/tsl/third_party/llvm/build.patch
index bbf8f587a..c2a3c6bd3 100644
--- a/third_party/tsl/third_party/llvm/build.patch
+++ b/third_party/tsl/third_party/llvm/build.patch
@@ -44,3 +44,24 @@ index 2b88729d748b..e12d979b4908 100644
          "//conditions:default": [
              "BLAKE3_NO_AVX2",
              "BLAKE3_NO_AVX512",
+diff --git a/mlir/lib/Bytecode/Reader/BytecodeReader.cpp b/mlir/lib/Bytecode/Reader/BytecodeReader.cpp
+index 177372c68046..40d49dc13b2f 100644
+--- a/mlir/lib/Bytecode/Reader/BytecodeReader.cpp
++++ b/mlir/lib/Bytecode/Reader/BytecodeReader.cpp
+@@ -1549,6 +1549,8 @@ private:
+   const std::shared_ptr<llvm::SourceMgr> &bufferOwnerRef;
+ };
+
++// TODO: Disable clang opt since it crashes with clang-17 compiler.
++#pragma clang optimize off
+ LogicalResult BytecodeReader::Impl::read(
+     Block *block, llvm::function_ref<bool(Operation *)> lazyOpsCallback) {
+   EncodingReader reader(buffer.getBuffer(), fileLoc);
+@@ -1628,6 +1630,7 @@ LogicalResult BytecodeReader::Impl::read(
+   // Finally, process the IR section.
+   return parseIRSection(*sectionDatas[bytecode::Section::kIR], block);
+ }
++#pragma clang optimize on
+
+ LogicalResult BytecodeReader::Impl::parseVersion(EncodingReader &reader) {
+   if (failed(reader.parseVarInt(version)))
diff --git a/third_party/tsl/third_party/llvm/spirv.patch b/third_party/tsl/third_party/llvm/spirv.patch
new file mode 100644
index 000000000..8643e9181
--- /dev/null
+++ b/third_party/tsl/third_party/llvm/spirv.patch
@@ -0,0 +1,76 @@
+diff --git a/llvm/lib/Passes/PassBuilderPipelines.cpp b/llvm/lib/Passes/PassBuilderPipelines.cpp
+index 78e0e6353056..4f9f51164bfe 100644
+--- a/llvm/lib/Passes/PassBuilderPipelines.cpp
++++ b/llvm/lib/Passes/PassBuilderPipelines.cpp
+@@ -183,6 +183,10 @@ static cl::opt<bool> EnableGlobalAnalyses(
+     "enable-global-analyses", cl::init(true), cl::Hidden,
+     cl::desc("Enable inter-procedural analyses"));
+ 
++static cl::opt<bool>
++    SYCLOptimizationMode("sycl-opt", cl::init(false), cl::Hidden,
++                         cl::desc("Enable SYCL optimization mode."));
++
+ static cl::opt<bool>
+     RunPartialInlining("enable-partial-inlining", cl::init(false), cl::Hidden,
+                        cl::desc("Run Partial inlinining pass"));
+@@ -406,6 +410,7 @@ PassBuilder::buildO1FunctionSimplificationPipeline(OptimizationLevel Level,
+   // Form canonically associated expression trees, and simplify the trees using
+   // basic mathematical properties. For example, this will form (nearly)
+   // minimal multiplication trees.
++  if (!SYCLOptimizationMode) {
+   FPM.addPass(ReassociatePass());
+ 
+   // Add the primary loop simplification pipeline.
+@@ -477,7 +482,7 @@ PassBuilder::buildO1FunctionSimplificationPipeline(OptimizationLevel Level,
+   FPM.addPass(createFunctionToLoopPassAdaptor(std::move(LPM2),
+                                               /*UseMemorySSA=*/false,
+                                               /*UseBlockFrequencyInfo=*/false));
+-
++  }
+   // Delete small array after loop unroll.
+   FPM.addPass(SROAPass(SROAOptions::ModifyCFG));
+ 
+@@ -580,6 +585,7 @@ PassBuilder::buildFunctionSimplificationPipeline(OptimizationLevel Level,
+   // Form canonically associated expression trees, and simplify the trees using
+   // basic mathematical properties. For example, this will form (nearly)
+   // minimal multiplication trees.
++  if (!SYCLOptimizationMode) {
+   FPM.addPass(ReassociatePass());
+ 
+   if (EnableConstraintElimination)
+@@ -657,7 +663,7 @@ PassBuilder::buildFunctionSimplificationPipeline(OptimizationLevel Level,
+   FPM.addPass(createFunctionToLoopPassAdaptor(std::move(LPM2),
+                                               /*UseMemorySSA=*/false,
+                                               /*UseBlockFrequencyInfo=*/false));
+-
++  }
+   // Delete small array after loop unroll.
+   FPM.addPass(SROAPass(SROAOptions::ModifyCFG));
+ 
+@@ -715,6 +721,9 @@ PassBuilder::buildFunctionSimplificationPipeline(OptimizationLevel Level,
+ 
+   invokeScalarOptimizerLateEPCallbacks(FPM, Level);
+ 
++  if (SYCLOptimizationMode)
++    FPM.addPass(SimplifyCFGPass());
++  else
+   FPM.addPass(SimplifyCFGPass(SimplifyCFGOptions()
+                                   .convertSwitchRangeToICmp(true)
+                                   .hoistCommonInsts(true)
+@@ -1385,6 +1394,7 @@ PassBuilder::buildModuleOptimizationPipeline(OptimizationLevel Level,
+ 
+   invokeVectorizerStartEPCallbacks(OptimizePM, Level);
+ 
++  if (!SYCLOptimizationMode) {
+   LoopPassManager LPM;
+   // First rotate loops that may have been un-rotated by prior passes.
+   // Disable header duplication at -Oz.
+@@ -1408,7 +1418,7 @@ PassBuilder::buildModuleOptimizationPipeline(OptimizationLevel Level,
+   OptimizePM.addPass(InjectTLIMappings());
+ 
+   addVectorPasses(Level, OptimizePM, /* IsFullLTO */ false);
+-
++  }
+   // LoopSink pass sinks instructions hoisted by LICM, which serves as a
+   // canonicalization pass that enables other optimizations. As a result,
+   // LoopSink pass needs to be a very late IR pass to avoid undoing LICM
diff --git a/third_party/tsl/third_party/llvm/workspace.bzl b/third_party/tsl/third_party/llvm/workspace.bzl
index e3f1876db..b9debef80 100644
--- a/third_party/tsl/third_party/llvm/workspace.bzl
+++ b/third_party/tsl/third_party/llvm/workspace.bzl
@@ -17,6 +17,7 @@ def repo(name):
         ],
         build_file = "//third_party/llvm:llvm.BUILD",
         patch_file = [
+            "//third_party/llvm:spirv.patch",
             "//third_party/llvm:generated.patch",  # Autogenerated, don't remove.
             "//third_party/llvm:build.patch",
             "//third_party/llvm:mathextras.patch",
diff --git a/third_party/tsl/tsl/framework/bfc_allocator.cc b/third_party/tsl/tsl/framework/bfc_allocator.cc
index 9e4447108..2de2402c9 100644
--- a/third_party/tsl/tsl/framework/bfc_allocator.cc
+++ b/third_party/tsl/tsl/framework/bfc_allocator.cc
@@ -133,6 +133,7 @@ bool BFCAllocator::Extend(size_t alignment, size_t rounded_bytes) {
 
   // Try allocating.
   size_t bytes = std::min(curr_region_allocation_bytes_, available_bytes);
+  bytes = std::min(bytes, limit_byte_);
   size_t bytes_received;
   void* mem_addr = sub_allocator_->Alloc(alignment, bytes, &bytes_received);
   if (mem_addr == nullptr && !started_backpedal_) {
diff --git a/third_party/tsl/tsl/framework/bfc_allocator.h b/third_party/tsl/tsl/framework/bfc_allocator.h
index 47619856a..afd88700f 100644
--- a/third_party/tsl/tsl/framework/bfc_allocator.h
+++ b/third_party/tsl/tsl/framework/bfc_allocator.h
@@ -97,6 +97,8 @@ class BFCAllocator : public Allocator {
 
   bool ClearStats() override;
 
+  void SetAllocateLimit(uint64_t limit_byte) { limit_byte_ = limit_byte; }
+
   void SetTimingCounter(SharedCounter* sc) { timing_counter_ = sc; }
 
   void SetSafeFrontier(uint64 count) override;
@@ -108,6 +110,7 @@ class BFCAllocator : public Allocator {
   MemoryDump RecordMemoryMap();
 
  private:
+  uint64_t limit_byte_ = std::numeric_limits<uint64_t>::max();
   struct Bin;
 
   void* AllocateRawInternal(size_t alignment, size_t num_bytes,
diff --git a/third_party/tsl/workspace2.bzl b/third_party/tsl/workspace2.bzl
index 1343f9e0f..839e84bdd 100644
--- a/third_party/tsl/workspace2.bzl
+++ b/third_party/tsl/workspace2.bzl
@@ -370,6 +370,7 @@ def _tf_repositories():
         patch_file = [
             "//third_party/grpc:generate_cc_env_fix.patch",
             "//third_party/grpc:register_go_toolchain.patch",
+            "//third_party/grpc:c_ares.patch",
         ],
         system_link_files = {
             "//third_party/systemlibs:BUILD": "bazel/BUILD",
diff --git a/xla/pjrt/event_pool.cc b/xla/pjrt/event_pool.cc
index 4a07a6ee7..b7f264c01 100644
--- a/xla/pjrt/event_pool.cc
+++ b/xla/pjrt/event_pool.cc
@@ -53,8 +53,11 @@ StatusOr<EventPool::Handle> EventPool::AllocateEvent(
 }
 
 void EventPool::ThenRecordEvent(se::Stream* stream, EventPool::Handle& handle) {
-  absl::MutexLock lock(&mu_);
+  // We should not lock event pool mutex before submitting a sycl barrier to a
+  // stream, otherwise it may lead to a dead lock if there is a host task
+  // requiring this mutex in the same stream.
   stream->ThenRecordEvent(handle.event_.get());
+  absl::MutexLock lock(&mu_);
   handle.sequence_number_ = next_sequence_number_++;
 }
 
diff --git a/xla/pjrt/gpu/gpu_helpers.cc b/xla/pjrt/gpu/gpu_helpers.cc
index 449b43cb6..423a153ad 100644
--- a/xla/pjrt/gpu/gpu_helpers.cc
+++ b/xla/pjrt/gpu/gpu_helpers.cc
@@ -37,11 +37,12 @@ namespace xla {
 StatusOr<LocalClient*> GetGpuXlaClient(
     const std::optional<std::string>& platform_name,
     const std::optional<std::set<int>>& allowed_devices) {
+  // SYCL: hardcode to xpu
   TF_ASSIGN_OR_RETURN(
       se::Platform * platform,
-      PlatformUtil::GetPlatform(platform_name ? *platform_name : "gpu"));
+      PlatformUtil::GetPlatform(platform_name ? *platform_name : "xpu"));
   if (platform->VisibleDeviceCount() <= 0) {
-    return FailedPrecondition("No visible GPU devices.");
+    return FailedPrecondition("No visible XPU devices.");
   }
   LocalClientOptions options;
   options.set_platform(platform);
@@ -113,7 +114,8 @@ StatusOr<std::unique_ptr<tsl::BFCAllocator>> CreateBFCAllocator(
   opts.allow_growth = !preallocate;
   return std::make_unique<tsl::BFCAllocator>(
       std::move(sub_allocator), allocator_memory,
-      absl::StrCat("GPU_", device_ordinal, "_bfc"), opts);
+      // SYCL: hardcode to xpu
+      absl::StrCat("XPU_", device_ordinal, "_bfc"), opts);
 }
 
 // Returns a GPU pinned host memory allocator to use when staging host->GPU
@@ -131,7 +133,8 @@ std::unique_ptr<tsl::BFCAllocator> GetGpuHostAllocator(
   opts.allow_growth = true;
   return std::make_unique<tsl::BFCAllocator>(std::move(sub_allocator),
                                              kGpuHostMemoryLimitBytes,
-                                             /*name=*/"xla_gpu_host_bfc", opts);
+                                             // SYCL: hardcode to xpu
+                                             /*name=*/"xla_xpu_host_bfc", opts);
 }
 
 }  // namespace xla
diff --git a/xla/service/BUILD b/xla/service/BUILD
index bc4000eea..d8cae9dc8 100644
--- a/xla/service/BUILD
+++ b/xla/service/BUILD
@@ -3855,6 +3855,7 @@ cc_library(
         "//xla/stream_executor/cuda:cuda_platform_id",
         "//xla/stream_executor/host:host_platform_id",
         "//xla/stream_executor/rocm:rocm_platform_id",
+        "@intel_extension_for_openxla//xla/stream_executor/sycl:sycl_platform_id",
         "@com_google_absl//absl/container:flat_hash_map",
         "@com_google_absl//absl/memory",
         "@com_google_absl//absl/strings",
diff --git a/xla/service/algebraic_simplifier.h b/xla/service/algebraic_simplifier.h
index 232074497..8b304b6e2 100644
--- a/xla/service/algebraic_simplifier.h
+++ b/xla/service/algebraic_simplifier.h
@@ -27,6 +27,7 @@ limitations under the License.
 #include <vector>
 
 #include "absl/container/inlined_vector.h"
+#include "tsl/util/env_var.h"
 #include "xla/hlo/ir/dfs_hlo_visitor_with_default.h"
 #include "xla/hlo/ir/hlo_instruction.h"
 #include "xla/hlo/ir/hlo_module.h"
@@ -432,7 +433,9 @@ class AlgebraicSimplifierVisitor : public DfsHloRewriteVisitor {
   virtual bool IsValidLayout(const Shape& shape) { return true; }
   // Allow backend targets to determine whether a layout is inefficient.
   virtual bool ShouldStrengthReduceDotToReduce(const HloInstruction* hlo) {
-    return true;
+    bool llm_flag = false;
+    tsl::ReadBoolFromEnvVar("LLM", false, &llm_flag);
+    return !llm_flag;
   }
 
  protected:
diff --git a/xla/service/computation_placer.cc b/xla/service/computation_placer.cc
index 9410c30d6..fe4c15705 100644
--- a/xla/service/computation_placer.cc
+++ b/xla/service/computation_placer.cc
@@ -31,6 +31,7 @@ limitations under the License.
 #include "xla/stream_executor/cuda/cuda_platform_id.h"
 #include "xla/stream_executor/host/host_platform_id.h"
 #include "xla/stream_executor/rocm/rocm_platform_id.h"
+#include "xla/stream_executor/sycl/sycl_platform_id.h"
 #include "xla/types.h"
 #include "xla/util.h"
 #include "tsl/platform/errors.h"
@@ -216,6 +217,8 @@ static bool InitModule() {
       stream_executor::cuda::kCudaPlatformId, &CreateComputationPlacer);
   xla::ComputationPlacer::RegisterComputationPlacer(
       stream_executor::rocm::kROCmPlatformId, &CreateComputationPlacer);
+  xla::ComputationPlacer::RegisterComputationPlacer(
+      stream_executor::sycl::kSyclPlatformId, &CreateComputationPlacer);
   return true;
 }
 static bool module_initialized = InitModule();
diff --git a/xla/service/dump.cc b/xla/service/dump.cc
index a490f9af9..659e5659e 100644
--- a/xla/service/dump.cc
+++ b/xla/service/dump.cc
@@ -622,6 +622,8 @@ void DumpToFileInDirOrStdout(const HloModule& module, string_view file_prefix,
   if (opts.dumping_to_stdout()) return op->dump();
 
   mlir::OpPrintingFlags print_flags = mlir::OpPrintingFlags().useLocalScope();
+  // Avoid printing large constant weight.
+  print_flags.elideLargeElementsAttrs(8);
   // Enable debug info so that it is easier to see the corresponding HLO node.
   if (file_prefix == "lmhlo") {
     print_flags.enableDebugInfo(/*enable=*/true,
diff --git a/xla/service/gpu/BUILD b/xla/service/gpu/BUILD
index af77ce507..992ab3000 100644
--- a/xla/service/gpu/BUILD
+++ b/xla/service/gpu/BUILD
@@ -34,6 +34,7 @@ load(
     "@tsl//tsl/platform/default:cuda_build_defs.bzl",
     "if_cuda_is_configured",
 )
+load("@local_config_sycl//sycl:build_defs.bzl", "if_sycl_is_configured")
 
 package(
     # copybara:uncomment default_applicable_licenses = ["//tensorflow:license"],
@@ -939,7 +940,7 @@ cc_library(
         ":kernel_arguments",
         ":launch_dimensions",
         ":matmul_utils",
-        ":nccl_collective_thunks",
+        # ":nccl_collective_thunks",
         ":non_atomically_upgradeable_rw_lock",
         ":stream_executor_util",
         ":thunk",
@@ -955,11 +956,11 @@ cc_library(
         "//xla:util",
         "//xla:xla_data_proto_cc",
         "//xla/hlo/ir:hlo",
-        "//xla/mlir/runtime/ir:rt",
-        "//xla/mlir/runtime/transforms:compilation_pipeline_gpu",
-        "//xla/mlir/runtime/transforms:type_converter",
+        # "//xla/mlir/runtime/ir:rt",
+        # "//xla/mlir/runtime/transforms:compilation_pipeline_gpu",
+        # "//xla/mlir/runtime/transforms:type_converter",
         "//xla/mlir_hlo:lhlo_gpu",
-        "//xla/runtime:executable",
+        # "//xla/runtime:executable",
         "//xla/service:buffer_assignment",
         "//xla/service:custom_call_status_internal",
         "//xla/service:executable",
@@ -969,8 +970,8 @@ cc_library(
         "//xla/service:shaped_buffer",
         "//xla/service:stream_pool",
         "//xla/service:xla_debug_info_manager",
-        "//xla/service/gpu/runtime:executable",
-        "//xla/service/gpu/runtime:support",
+        # "//xla/service/gpu/runtime:executable",
+        # "//xla/service/gpu/runtime:support",
         "//xla/service/gpu/runtime3:custom_call_thunk",
         "//xla/service/gpu/runtime3:fft_thunk",
         "//xla/stream_executor",
@@ -1009,6 +1010,8 @@ cc_library(
         "@tsl//tsl/platform:status",
         "@tsl//tsl/profiler/lib:scoped_annotation",
         "@tsl//tsl/profiler/lib:traceme",
+        "@intel_extension_for_openxla//xla/service/gpu:xetla_gpu_fused_mha_runner",
+        "@intel_extension_for_openxla//xla/service/gpu:onednn_gpu_conv_runner",
     ] + if_gpu_is_configured([
         ":precompiled_kernels",
         "//xla/service/gpu/runtime3:cholesky_thunk",
@@ -1352,6 +1355,7 @@ cc_library(
         "//xla/service:buffer_assignment",
         "//xla/stream_executor",
         "//xla/stream_executor:device_memory",
+        "@intel_extension_for_openxla//xla/service/gpu:onednn_matmul_utils",
         "@tsl//tsl/platform:logging",
     ],
 )
@@ -1364,6 +1368,7 @@ cc_library(
         "TENSORFLOW_USE_ROCM=1",
     ]),
     deps = if_gpu_is_configured([
+        "@intel_extension_for_openxla//xla/service/gpu:onednn_matmul_utils",
         ":matmul_utils",
         ":thunk",
         "//xla/service:buffer_assignment",
@@ -1886,6 +1891,8 @@ cc_library(
         "//xla/stream_executor/rocm:rocblas_wrapper",
         "//xla/stream_executor/rocm:rocsolver_wrapper",
         "//xla/stream_executor/rocm:hipsolver_wrapper",
+    ]) + if_sycl_is_configured([
+        "@local_config_sycl//sycl:sycl_headers",
     ]),
 )
 
@@ -2407,6 +2414,7 @@ cc_library(
         "//xla/stream_executor/cuda:cuda_platform_id",
         "//xla/stream_executor/host:host_platform_id",
         "//xla/stream_executor/rocm:rocm_platform_id",
+        "@intel_extension_for_openxla//xla/stream_executor/sycl:sycl_platform_id",
         "@com_google_absl//absl/cleanup",
         "@llvm-project//llvm:Core",
         "@tsl//tsl/platform:errors",
diff --git a/xla/service/gpu/buffer_sharing.cc b/xla/service/gpu/buffer_sharing.cc
index 64421596d..8908ba761 100644
--- a/xla/service/gpu/buffer_sharing.cc
+++ b/xla/service/gpu/buffer_sharing.cc
@@ -198,6 +198,14 @@ std::optional<bool> CanShareBufferHint(const HloInstruction* user,
             std::move(user->backend_config<GemmBackendConfig>()).value();
         return (config.beta() != 0.) && user->operand(2) == operand;
       }
+      // SYCL
+      if (user->custom_call_target() ==
+          kCudnnConvBiasActivationForwardCallTarget) {
+        CudnnConvBackendConfig config =
+            std::move(user->backend_config<CudnnConvBackendConfig>()).value();
+        return (config.side_input_scale() != 0.) &&
+               (user->operand(user->operand_count() - 1) == operand);
+      }
       // The operand of cholesky can be shared with the first output.
       if (user->custom_call_target() == kCusolverCholeskyCallTarget) {
         return user_index.size() == 1 && user_index[0] == 0;
diff --git a/xla/service/gpu/convolution_thunk.cc b/xla/service/gpu/convolution_thunk.cc
index 17096e890..4ddf43c02 100644
--- a/xla/service/gpu/convolution_thunk.cc
+++ b/xla/service/gpu/convolution_thunk.cc
@@ -28,11 +28,20 @@ limitations under the License.
 #include "xla/util.h"
 #include "tsl/platform/logging.h"
 
+#if GOOGLE_SYCL
+#include "xla/service/gpu/onednn_gpu_conv_runner.h"
+#include "xla/stream_executor/scratch_allocator.h"
+#endif
+
 namespace xla {
 namespace gpu {
 
 ConvolutionThunk::ConvolutionThunk(
+#if GOOGLE_SYCL
+    ThunkInfo thunk_info, GpuConvDescriptor descriptor,
+#else
     ThunkInfo thunk_info, GpuConvConfig config,
+#endif
     std::vector<BufferAllocation::Slice> operand_slices,
     std::vector<BufferAllocation::Slice> result_slices,
     BufferAllocation::Slice scratch_slice)
@@ -40,8 +49,13 @@ ConvolutionThunk::ConvolutionThunk(
       operand_buffers_(std::move(operand_slices)),
       result_buffers_(std::move(result_slices)),
       scratch_buffer_(scratch_slice),
+#if GOOGLE_SYCL
+      descriptor_(std::move(descriptor)) {}
+#else
       config_(std::move(config)) {}
+#endif
 
+#if !GOOGLE_SYCL
 GenericConvRunner& ConvolutionThunk::GetOrCreateRunner(
     const stream_executor::Stream* stream) {
   absl::MutexLock lock(&mu_);
@@ -53,6 +67,7 @@ GenericConvRunner& ConvolutionThunk::GetOrCreateRunner(
   }
   return *it->second;
 }
+#endif
 
 Status ConvolutionThunk::ExecuteOnStream(const ExecuteParams& params) {
   const auto& buffer_allocations = *params.buffer_allocations;
@@ -71,13 +86,26 @@ Status ConvolutionThunk::ExecuteOnStream(const ExecuteParams& params) {
   se::DeviceMemoryBase scratch =
       buffer_allocations.GetDeviceAddress(scratch_buffer_);
 
+#if GOOGLE_SYCL
+  auto stream = params.stream;
+  se::OwningScratchAllocator<2> scratch_allocator(
+      buffer_allocations.device_ordinal(),
+      buffer_allocations.memory_allocator());
+  TF_ASSIGN_OR_RETURN(auto conv_primitive,
+                      GetOrCreateOneDnnConvPrimitive(stream, descriptor_, operand_se_buffers,
+                                                     result_se_buffers[0], params, &scratch_allocator));
+
+  TF_RETURN_IF_ERROR(RunGpuConv(conv_primitive, descriptor_,
+                                absl::MakeSpan(operand_se_buffers),
+                                result_se_buffers[0], params));
+#else
   RunConvOptions opts;
   opts.runner_cache = &GetOrCreateRunner(params.stream);
 
   TF_RETURN_IF_ERROR(RunGpuConv(config_, absl::MakeSpan(operand_se_buffers),
                                 absl::MakeSpan(result_se_buffers), scratch,
                                 params.stream, opts));
-
+#endif
   // Note: Convolution has a tuple buffer as an output, but we don't need to
   // populate it as no one should be reading from the tuple directly.
   if (!params.stream->ok()) {
diff --git a/xla/service/gpu/convolution_thunk.h b/xla/service/gpu/convolution_thunk.h
index d7f731302..063638587 100644
--- a/xla/service/gpu/convolution_thunk.h
+++ b/xla/service/gpu/convolution_thunk.h
@@ -44,7 +44,12 @@ class ConvolutionThunk : public Thunk {
   // Constructs a thunk for launching a DNN convolution.
   //
   // operand_slices should be in the same order as cudnn_call->operands().
-  ConvolutionThunk(ThunkInfo thunk_info, GpuConvConfig config,
+  ConvolutionThunk(ThunkInfo thunk_info,
+#if GOOGLE_SYCL
+                   GpuConvDescriptor descriptor,
+#else
+                   GpuConvConfig config,
+#endif
                    std::vector<BufferAllocation::Slice> operand_slices,
                    std::vector<BufferAllocation::Slice> result_slices,
                    BufferAllocation::Slice scratch_slice);
@@ -61,7 +66,11 @@ class ConvolutionThunk : public Thunk {
   GenericConvRunner& GetOrCreateRunner(const stream_executor::Stream* stream);
 
   // Convolution config
+#if GOOGLE_SYCL
+  const GpuConvDescriptor descriptor_;
+#else
   const GpuConvConfig config_;
+#endif
   absl::Mutex mu_;
   absl::flat_hash_map<const stream_executor::Stream*,
                       std::unique_ptr<GenericConvRunner>>
diff --git a/xla/service/gpu/cudnn_fused_conv_rewriter.cc b/xla/service/gpu/cudnn_fused_conv_rewriter.cc
index 2cd279b68..0fcb36c90 100644
--- a/xla/service/gpu/cudnn_fused_conv_rewriter.cc
+++ b/xla/service/gpu/cudnn_fused_conv_rewriter.cc
@@ -824,6 +824,13 @@ StatusOr<bool> FuseBiasOrSideInput(HloComputation* comp) {
         conv->CloneWithNewOperands(conv->shape(), new_operands));
     comp->parent()->SetAndUniquifyInstrName(new_conv, conv->name());
     TF_RETURN_IF_ERROR(new_conv->set_backend_config(config));
+#if GOOGLE_SYCL
+    if (can_accept_side_input) {
+      xla::Cast<HloCustomCallInstruction>(new_conv)
+          ->set_output_to_operand_aliasing(
+              {{{}, {static_cast<long>(new_operands.size()) - 1, {}}}});
+    }
+#endif
     TF_ASSIGN_OR_RETURN(HloInstruction * new_instr,
                         MakeGetTupleElementHlo(new_conv, 0));
     TF_RETURN_IF_ERROR(comp->ReplaceInstruction(instr, new_instr));
diff --git a/xla/service/gpu/cudnn_fused_mha_rewriter.cc b/xla/service/gpu/cudnn_fused_mha_rewriter.cc
index 099e6776a..1d76a0708 100644
--- a/xla/service/gpu/cudnn_fused_mha_rewriter.cc
+++ b/xla/service/gpu/cudnn_fused_mha_rewriter.cc
@@ -295,14 +295,22 @@ bool IsSupportedPrimitiveType(const HloInstruction* bmm) {
 }
 
 bool IsContractingDimSupported(absl::Span<const int64_t> contracting_dims) {
+#if GOOGLE_SYCL
+  return true;
+#else
   return absl::c_all_of(contracting_dims,
                         [](int64_t dim) { return dim == 64; });
+#endif
 }
 
 bool IsNonContractingDimSupported(
     const std::vector<int64_t>& non_contracting_dims) {
+#if GOOGLE_SYCL
+  return true;
+#else
   return absl::c_all_of(non_contracting_dims,
                         [](int64_t dim) { return dim <= 512; });
+#endif
 }
 
 std::vector<int64_t> GetDimensionVector(absl::Span<const int64_t> dimensions,
@@ -388,6 +396,7 @@ StatusOr<bool> IsSupportedBMM2(const HloInstruction* bmm_2,
   std::vector<int64_t> non_contracting_dims_bmm2 =
       GetDimensionVector(bmm_2->operand(operand_index)->shape().dimensions(),
                          non_contracting_dim_nums_bmm2);
+#if !GOOGLE_SYCL
   // The non contracting dimension for BMM2 needs to be 64 for the input matrix.
   // The input matrix is the second argument to BMM2 i.e, rhs.
   if (!absl::c_all_of(non_contracting_dims_bmm2,
@@ -399,6 +408,7 @@ StatusOr<bool> IsSupportedBMM2(const HloInstruction* bmm_2,
     }
     return false;
   }
+#endif
   return true;
 }
 
@@ -523,6 +533,7 @@ MatchFwdResult MatchBmm1UnfusedBiasSoftmaxBmm2(MatchFwdResult previous_result,
 
   if (Match(softmax_input,
             OptionalConvert(OptionalBitcast(first_bmm_pattern)))) {
+    // BMM + softmax
     match_result.matched_bmm_1 = bmm_1;
     match_result.matched_custom_call_name =
         has_dropout ? kCudnnfMHASoftmaxDropoutCallTarget
@@ -533,6 +544,8 @@ MatchFwdResult MatchBmm1UnfusedBiasSoftmaxBmm2(MatchFwdResult previous_result,
                        OptionalConvert(OptionalBitcast(m::AnyOf<HloInstruction>(
                            unfused_scaled_bmm_subpattern, first_bmm_pattern))),
                        m::Op(&bias))))) {
+    // BMM + add + softmax
+    // BMM + scale + add + softmax
     match_result.matched_bmm_1 = bmm_1;
     match_result.matched_scale = scale;
     match_result.matched_bias = bias;
@@ -540,6 +553,15 @@ MatchFwdResult MatchBmm1UnfusedBiasSoftmaxBmm2(MatchFwdResult previous_result,
         has_dropout ? kCudnnfMHAScaleBiasSoftmaxDropoutCallTarget
                     : kCudnnfMHAScaleBiasSoftmaxCallTarget;
     match_result.has_match = true;
+  } else if (Match(softmax_input, unfused_scaled_bmm_subpattern)) {
+    // SYCL: BMM + scale + softmax
+    match_result.matched_bmm_1 = bmm_1;
+    match_result.matched_scale = scale;
+    match_result.matched_custom_call_name =
+        has_dropout ? kCudnnfMHASoftmaxDropoutCallTarget
+                    : kCudnnfMHASoftmaxCallTarget;
+
+    match_result.has_match = true;
   } else {
     match_result.has_match = false;
   }
@@ -1138,6 +1160,9 @@ StatusOr<HloInstruction*> ChangeCheckedDimToFastest(
                 rhs_minor_to_major_bmm),
             operand_bmm, perm),
         &operand_bmm->metadata());
+#if GOOGLE_SYCL
+    bmm->ReplaceOperandWithDifferentShape(bmm_operand, operand_bmm);
+#endif
     *((DynCast<HloDotInstruction>(bmm))->mutable_dot_dimension_numbers()) =
         new_dot_dims_bmm;
   }
@@ -1522,12 +1547,14 @@ StatusOr<bool> CudnnFusedMHARewriter::Run(
        module->MakeNonfusionComputations(execution_threads)) {
     const DebugOptions& debug_options =
         comp->parent()->config().debug_options();
+#if !GOOGLE_SYCL
     if (!debug_options.xla_gpu_enable_cudnn_fmha() ||
         !IsComputeCapabilityAndCudnnSupported(
             compute_capability_, cudnn_version_, stream_executor_,
             stream_executor::dnn::VersionInfo(8, 8, 0))) {
       return false;
     }
+#endif  // !GOOGLE_SYCL
     for (HloInstruction* instr : comp->MakeInstructionPostOrder()) {
       bool v_transposed = false;
       MatchFwdResult matched_result =
@@ -1564,6 +1591,7 @@ StatusOr<bool> CudnnFusedMHARewriter::Run(
               matched_result.is_training, changed, v_transposed));
       any_changed |= changed;
 
+#if !GOOGLE_SYCL
       if (matched_result.is_training) {
         // if fwd uses mask input, then bwd needs cudnn 8.9.1 to take in a mask
         // input if cudnn version < 8.9.1 we won't lower the bwd pass
@@ -1636,6 +1664,7 @@ StatusOr<bool> CudnnFusedMHARewriter::Run(
                 matched_bwd_result.bmm_2_grad_1_need_canonicalization));
         any_changed |= changed;
       }
+#endif  // !GOOGLE_SYCL
     }
   }
 
diff --git a/xla/service/gpu/cusolver_context.cc b/xla/service/gpu/cusolver_context.cc
index 282bfa295..fdfae3890 100644
--- a/xla/service/gpu/cusolver_context.cc
+++ b/xla/service/gpu/cusolver_context.cc
@@ -26,6 +26,7 @@ limitations under the License.
 namespace xla {
 namespace gpu {
 
+#if !GOOGLE_SYCL
 namespace {
 
 // Type traits to get CUDA complex types from std::complex<T>.
@@ -414,6 +415,19 @@ Status GpuSolverContext::PotrfBatched(
 #endif
       ToDevicePointer(lapack_info), batch_size));
 }
+#else // !GOOGLE_SYCL
 
+StatusOr<GpuSolverContext> GpuSolverContext::Create() {
+  return GpuSolverContext();
+}
+
+Status GpuSolverContext::SetStream(se::Stream* stream) {
+  gpu_stream_ = stream_executor::gpu::AsGpuStreamValue(stream);
+  return OkStatus();
+}
+
+GpuSolverContext::GpuSolverContext() {}
+
+#endif // !GOOGLE_SYCL
 }  // namespace gpu
 }  // namespace xla
diff --git a/xla/service/gpu/cusolver_context.h b/xla/service/gpu/cusolver_context.h
index 63138f5fd..1d85de4e4 100644
--- a/xla/service/gpu/cusolver_context.h
+++ b/xla/service/gpu/cusolver_context.h
@@ -26,6 +26,12 @@ limitations under the License.
 #define TENSORFLOW_USE_CUSOLVER_OR_HIPSOLVER \
   (!TENSORFLOW_USE_ROCM || TENSORFLOW_USE_HIPSOLVER)
 
+#if GOOGLE_SYCL
+#include "oneapi/mkl/blas.hpp"
+#include "oneapi/mkl/lapack.hpp"
+#include "oneapi/mkl/dfti.hpp"
+#include "oneapi/mkl/exceptions.hpp"
+#else // GOOGLE_SYCL
 #if !TENSORFLOW_USE_ROCM
 #include "third_party/gpus/cuda/include/cusolverDn.h"
 using gpusolverHandle_t = cusolverDnHandle_t;
@@ -41,16 +47,18 @@ using gpusolverHandle_t = hipsolverHandle_t;
 using gpusolverHandle_t = rocblas_handle;
 #endif  // TF_ROCM_VERSION >= 40500
 #endif  // TENSORFLOW_USE_ROCM
+#endif  // GOOGLE_SYCL
 
 #include "xla/statusor.h"
 #include "xla/stream_executor/blas.h"
 #include "xla/stream_executor/stream_executor.h"
 #include "xla/xla_data.pb.h"
+#include "xla/stream_executor/gpu/gpu_types.h"
 
 namespace xla {
 namespace gpu {
-
 namespace se = ::stream_executor;
+#if !GOOGLE_SYCL
 
 class GpuSolverContext {
  public:
@@ -104,6 +112,59 @@ class GpuSolverContext {
   std::unique_ptr<std::remove_pointer_t<gpusolverHandle_t>, Deleter> handle_;
 };
 
+#else // !GOOGLE_SYCL
+class GpuSolverContext {
+ public:
+  static StatusOr<GpuSolverContext> Create();
+  Status SetStream(se::Stream* stream);
+
+  template<typename T>
+  Status PotrfBatched(se::blas::UpperLower uplo, int n,
+                      int lda,
+                      int batch_size, se::DeviceMemoryBase* workspace_buffer, T* a_base) {
+    T* scratch_data = static_cast<T*>((*workspace_buffer).opaque());
+    int64_t scratchpad_size = (*workspace_buffer).size() / sizeof(T);
+
+    const int64_t stride_a = n * n;
+
+    oneapi::mkl::uplo params_uplo;
+    switch(uplo){
+      case se::blas::UpperLower::kLower:
+        params_uplo = oneapi::mkl::uplo::L;
+        break;
+      case se::blas::UpperLower::kUpper:
+        params_uplo = oneapi::mkl::uplo::U;
+        break;
+      default:
+        params_uplo = static_cast<oneapi::mkl::uplo>(uplo);
+    }
+
+    try {
+      oneapi::mkl::lapack::potrf_batch(*gpu_stream_, params_uplo, n,
+                                       a_base, lda, stride_a, batch_size,
+                                       scratch_data, scratchpad_size);
+    } catch (oneapi::mkl::lapack::batch_error const& be) {
+      int i = 0;
+      auto& ids = be.ids();
+      for (auto const& e : be.exceptions()) {
+        try {
+          std::rethrow_exception(e);
+        } catch (oneapi::mkl::lapack::exception& e) {
+          LOG(ERROR) << "Exception " << ids[i++]
+                     << " in a batch says: " << e.what()
+                     << " (info code: " << e.info() << ")";
+        }
+      }
+    }
+    return OkStatus();
+  }
+
+ private:
+  explicit GpuSolverContext();
+  stream_executor::gpu::GpuStreamHandle gpu_stream_;
+};
+
+#endif // !GOOGLE_SYCL
 }  // namespace gpu
 }  // namespace xla
 
diff --git a/xla/service/gpu/cusolver_rewriter.cc b/xla/service/gpu/cusolver_rewriter.cc
index 904673038..eac7e0b99 100644
--- a/xla/service/gpu/cusolver_rewriter.cc
+++ b/xla/service/gpu/cusolver_rewriter.cc
@@ -65,6 +65,7 @@ StatusOr<HloInstruction*> CreateCholesky(GpuSolverContext* context,
   absl::c_iota(batch_dim_ids, 0);
   int64_t batch_size = absl::c_accumulate(batch_dims, 1, std::multiplies<>{});
 
+#if !GOOGLE_SYCL
   // Find the workspace size.
   se::blas::UpperLower uplo = options.lower() ? se::blas::UpperLower::kLower
                                               : se::blas::UpperLower::kUpper;
@@ -72,7 +73,36 @@ StatusOr<HloInstruction*> CreateCholesky(GpuSolverContext* context,
   TF_ASSIGN_OR_RETURN(
       workspace_size,
       context->PotrfBufferSize(a_shape.element_type(), uplo, n, n, batch_size));
-
+#else
+  // Find the workspace size.
+  int64_t workspace_size = 0;
+  oneapi::mkl::uplo uplo =
+      options.lower() ? oneapi::mkl::uplo::L : oneapi::mkl::uplo::U;
+  sycl::property_list propList{sycl::property::queue::in_order()};
+  sycl::queue queue(sycl::gpu_selector{}, propList);
+  switch (a_shape.element_type()) {
+    case F32:
+      workspace_size = oneapi::mkl::lapack::potrf_batch_scratchpad_size<float>(
+          queue, uplo, n, n, n * n, batch_size);
+      break;
+    case F64:
+      workspace_size = oneapi::mkl::lapack::potrf_batch_scratchpad_size<double>(
+          queue, uplo, n, n, n * n, batch_size);
+      break;
+    case C64:
+      workspace_size =
+          oneapi::mkl::lapack::potrf_batch_scratchpad_size<std::complex<float>>(
+              queue, uplo, n, n, n * n, batch_size);
+      break;
+    case C128:
+      workspace_size = oneapi::mkl::lapack::potrf_batch_scratchpad_size<
+          std::complex<double>>(queue, uplo, n, n, n * n, batch_size);
+      break;
+    default:
+      return InvalidArgument("Invalid type for cholesky %s",
+                             PrimitiveType_Name(a_shape.element_type()));
+  }
+#endif
   // TODO(phawkins): Ideally we would relax this constraint. What we actually
   // want is that:
   // a) the batch dimensions are major, in no particular order.
@@ -99,6 +129,9 @@ StatusOr<HloInstruction*> CreateCholesky(GpuSolverContext* context,
   TF_RETURN_IF_ERROR(custom_call->set_backend_config(options));
   HloInstruction* out = computation->AddInstruction(
       HloInstruction::CreateGetTupleElement(a_shape, custom_call, 0));
+#if GOOGLE_SYCL
+  return out;
+#else
   HloInstruction* info = computation->AddInstruction(
       HloInstruction::CreateGetTupleElement(info_shape, custom_call, 2));
 
@@ -128,6 +161,7 @@ StatusOr<HloInstruction*> CreateCholesky(GpuSolverContext* context,
       computation->AddInstruction(HloInstruction::CreateTernary(
           a_shape, HloOpcode::kSelect, ok, out, nans));
   return select;
+#endif
 }
 
 // Tries to rewrite a single convolution into a call to cudnn.
diff --git a/xla/service/gpu/fused_mha_thunk.cc b/xla/service/gpu/fused_mha_thunk.cc
index 96562a8e2..e8aa10851 100644
--- a/xla/service/gpu/fused_mha_thunk.cc
+++ b/xla/service/gpu/fused_mha_thunk.cc
@@ -28,6 +28,10 @@ limitations under the License.
 #include "xla/util.h"
 #include "tsl/platform/logging.h"
 
+#if GOOGLE_SYCL
+#include "xla/service/gpu/xetla_gpu_fused_mha_runner.h"
+#endif
+
 namespace xla {
 namespace gpu {
 
@@ -88,12 +92,19 @@ Status FusedMHAThunk::ExecuteOnStream(const ExecuteParams& params) {
     activation_buffer = buffer_allocations.GetDeviceAddress(activation_buffer_);
   }
 
+#if GOOGLE_SYCL
+  TF_RETURN_IF_ERROR(RunXetlaGpuFMHA(config_, lhs_bmm1_buffer, rhs_bmm1_buffer,
+                                     rhs_bmm2_buffer, output_buffer, scratch_buffer,
+                                     mask_buffer, bias_buffer,
+                                     activation_buffer, params.stream));
+#else
   RunFusedMHAOptions opts;
   opts.runner_cache = &GetOrCreateRunner(params.stream);
   TF_RETURN_IF_ERROR(RunGpuFMHA(config_, lhs_bmm1_buffer, rhs_bmm1_buffer,
                                 rhs_bmm2_buffer, output_buffer, scratch_buffer,
                                 mask_buffer, bias_buffer, activation_buffer,
                                 params.stream, opts));
+#endif
 
   if (!params.stream->ok()) {
     return InternalError("FusedMHAThunk::ExecuteOnStream failed.");
diff --git a/xla/service/gpu/fusions/copy.cc b/xla/service/gpu/fusions/copy.cc
index a04885f9f..bdff1fc0d 100644
--- a/xla/service/gpu/fusions/copy.cc
+++ b/xla/service/gpu/fusions/copy.cc
@@ -18,6 +18,10 @@ limitations under the License.
 
 #include "xla/service/gpu/copy_thunk.h"
 
+#if GOOGLE_SYCL
+#include "xla/service/gpu/ir_emission_utils.h"
+#endif
+
 namespace xla {
 namespace gpu {
 
diff --git a/xla/service/gpu/fusions/fusion_emitter.cc b/xla/service/gpu/fusions/fusion_emitter.cc
index fd3d73035..413595982 100644
--- a/xla/service/gpu/fusions/fusion_emitter.cc
+++ b/xla/service/gpu/fusions/fusion_emitter.cc
@@ -36,6 +36,10 @@ limitations under the License.
 #include "tsl/platform/errors.h"
 #include "tsl/platform/statusor.h"
 
+#if GOOGLE_SYCL
+#include "xla/service/gpu/ir_emission_utils.h"
+#endif
+
 namespace xla {
 namespace gpu {
 namespace {
@@ -112,9 +116,13 @@ BuildKernelPrototype(IrEmitterContext& ir_emitter_context,
   // Create the kernel and add it to the module.
   auto* llvm_module = ir_emitter_context.llvm_module();
   llvm::LLVMContext& context = llvm_module->getContext();
+
+  // SYCL: Hardcode to global device addrspace.
+  llvm::Type* arg_type =
+      IsSPIR(llvm_module)? builder->getInt8PtrTy(1) : builder->getInt8PtrTy();
   llvm::FunctionType* kernel_type = llvm::FunctionType::get(
       /*Result=*/llvm::Type::getVoidTy(context),
-      std::vector<llvm::Type*>(kNumLlvmArgs, builder->getInt8PtrTy()),
+      std::vector<llvm::Type*>(kNumLlvmArgs, arg_type),
       /*isVarArg=*/false);
   llvm::Function* kernel =
       llvm::Function::Create(kernel_type, llvm::GlobalValue::ExternalLinkage,
@@ -123,6 +131,17 @@ BuildKernelPrototype(IrEmitterContext& ir_emitter_context,
   AnnotateFunctionAsGpuKernel(llvm_module, kernel, builder);
   AnnotateKernelLaunchDimensions(launch_dimensions, kernel_name, llvm_module);
 
+  // SYCL: Set function metadata
+  if (IsSPIR(llvm_module)) {
+    llvm::LLVMContext& context = llvm_module->getContext();
+    llvm::IntegerType* i32 = llvm::Type::getInt32Ty(context);
+    kernel->setMetadata(
+        "intel_reqd_sub_group_size",
+        llvm::MDNode::get(context,
+                          {llvm::ConstantAsMetadata::get(
+                              llvm::ConstantInt::get(i32, WarpSize()))}));
+  }
+
   // TODO(b/65380986): Investigate if adding fast math flags for generated
   // kernels makes sense.
 
diff --git a/xla/service/gpu/fusions/reduction.cc b/xla/service/gpu/fusions/reduction.cc
index 318e34fea..e55b1eeb5 100644
--- a/xla/service/gpu/fusions/reduction.cc
+++ b/xla/service/gpu/fusions/reduction.cc
@@ -1021,10 +1021,14 @@ StatusOr<FusionEmissionResult> ReductionFusion::Emit(
     Shape reduce_operand_shape =
         reduction_codegen_info->GetReduceOperandShape();
 
-    llvm::CallInst* raw_block_id_y = gpu::EmitCallToTargetIntrinsic(
+    llvm::Value* raw_block_id_y = gpu::EmitCallToTargetIntrinsic(
         gpu::TargetIntrinsicID::kBlockIdy, {}, {}, builder);
     llvm_ir::AddRangeMetadata(0, instr_index_groups.size(),
                               llvm::cast<llvm::Instruction>(raw_block_id_y));
+    // SYCL: Cast to int32 output.
+    if (IsSPIR(builder->GetInsertBlock()->getModule()))
+      raw_block_id_y = builder->CreateZExtOrTrunc(
+          raw_block_id_y, builder->getInt32Ty(), "raw_block_id_y");
     for (int i = 0; i < instr_index_groups.size(); ++i) {
       TF_RETURN_IF_ERROR(ksl.IfWithStatus(
           absl::StrCat("reduce-group-", i),
diff --git a/xla/service/gpu/gemm_rewriter.cc b/xla/service/gpu/gemm_rewriter.cc
index 0e41cfdcb..96c21f8ba 100644
--- a/xla/service/gpu/gemm_rewriter.cc
+++ b/xla/service/gpu/gemm_rewriter.cc
@@ -561,6 +561,7 @@ class GemmRewriterVisitor : public DfsHloRewriteVisitor {
 
   Status HandleMultiply(HloInstruction *instr) override {
     HloInstruction *alpha, *existing_gemm;
+#if !GOOGLE_SYCL
     if (Match(instr,
               m::MultiplyAnyOrder(
                   GemmOrCublasLtMatmulMaybeF8(&existing_gemm).WithOneUser(),
@@ -584,11 +585,13 @@ class GemmRewriterVisitor : public DfsHloRewriteVisitor {
         return ReplaceInstruction(instr, existing_gemm);
       }
     }
+#endif  // !GOOGLE_SYCL
 
     // Attempt to match approximate GELU activation
     // (https://arxiv.org/abs/1606.08415), where:
     // approx_gelu(x) = x * cdf(x)
     // cdf(x) = 0.5 * (1 + tanh(sqrt(2 / pi) * (x + 0.044715 * x**3))
+    // SYCL: OptionalBitcast?
     HloInstruction *cdf, *slice_or_bitcast = nullptr;
     if (Match(instr, m::MultiplyAnyOrder(
                          m::AnyOf<HloInstruction>(
@@ -628,6 +631,14 @@ class GemmRewriterVisitor : public DfsHloRewriteVisitor {
                                   .WithOneUser())
                               .WithOneUser())
                           .WithOneUser())))) {
+      // SYCL
+      // gemm - bitcast - gelu - bitcast
+      if (instr->user_count() == 1) {
+        auto bitcast = instr->users()[0];
+        if (bitcast->opcode() == HloOpcode::kBitcast &&
+            ShapeUtil::Compatible(bitcast->shape(), existing_gemm->shape()))
+          return FuseGeluActivation(bitcast, existing_gemm);
+      }
       return FuseGeluActivation(instr, existing_gemm, slice_or_bitcast);
     }
     return OkStatus();
@@ -1266,8 +1277,8 @@ class GemmRewriterVisitor : public DfsHloRewriteVisitor {
       return in_out_alias_config.ParameterHasAlias(bias->parameter_number(),
                                                    /*param_index=*/{});
     }();
-    bool want_to_fuse_bias = IsCublasLtMatmulF8(*gemm) ||
-                             IsCublasLtMatmul(*gemm) || can_overwrite_bias;
+    // SYCL: cannot always fuse bias.
+    bool want_to_fuse_bias = can_overwrite_bias;
 
     auto config = gemm->backend_config<GemmBackendConfig>().value();
 
@@ -1558,13 +1569,14 @@ class GemmRewriterVisitor : public DfsHloRewriteVisitor {
   StatusOr<absl::string_view> GetNonFp8GemmCustomCallTarget(
       const HloInstruction &instr,
       const GemmBackendConfig &gemm_backend_config) const {
-    if (!instr.GetModule()
-             ->config()
-             .debug_options()
-             .xla_gpu_enable_cublaslt()) {
-      // cublasLt is not enabled.
-      return absl::string_view(kGemmCallTarget);
-    }
+    // SYCL: disable fallback
+    // if (!instr.GetModule()
+    //          ->config()
+    //          .debug_options()
+    //          .xla_gpu_enable_cublaslt()) {
+    //   // cublasLt is not enabled.
+    //   return absl::string_view(kGemmCallTarget);
+    // }
 
     // cublasLt is enabled, check if other internal conditions are met.
     const HloInstruction *lhs = instr.operand(0);
@@ -1861,11 +1873,6 @@ class GemmRewriterVisitor : public DfsHloRewriteVisitor {
     for (auto batch_dimension : batch_dimensions) {
       batch_count *= lhs->shape().dimensions(batch_dimension);
     }
-    if (batch_count > kMaxBatchCount) {
-      // This is not supported by cublasLt.
-      return false;
-    }
-
     TF_ASSIGN_OR_RETURN(bool output_is_column_major,
                         MatrixIsColumnMajor(instr, gemm_backend_config));
 
diff --git a/xla/service/gpu/gemm_thunk.cc b/xla/service/gpu/gemm_thunk.cc
index b774280a2..d7fbb3006 100644
--- a/xla/service/gpu/gemm_thunk.cc
+++ b/xla/service/gpu/gemm_thunk.cc
@@ -23,6 +23,10 @@ limitations under the License.
 #include "xla/stream_executor/device_memory.h"
 #include "tsl/platform/logging.h"
 
+#if GOOGLE_SYCL
+#include "xla/service/gpu/onednn_matmul_utils.h"
+#endif
+
 namespace xla {
 namespace gpu {
 
@@ -38,6 +42,7 @@ GemmThunk::GemmThunk(ThunkInfo thunk_info, GemmConfig config,
       output_buffer_(output_buffer),
       deterministic_(deterministic) {}
 
+#if !GOOGLE_SYCL
 Status GemmThunk::ExecuteOnStream(const ExecuteParams& params) {
   VLOG(3) << "Running GEMM thunk";
   const BufferAllocations& allocs = *params.buffer_allocations;
@@ -49,6 +54,26 @@ Status GemmThunk::ExecuteOnStream(const ExecuteParams& params) {
                  allocs.GetDeviceAddress(output_buffer_), workspace,
                  deterministic_, params.stream);
 }
+#else // !GOOGLE_SYCL
+Status GemmThunk::ExecuteOnStream(const ExecuteParams& params) {
+  VLOG(3) << "Running GEMM thunk";
+  const BufferAllocations& allocs = *params.buffer_allocations;
+  se::DeviceMemoryBase lhs_data = allocs.GetDeviceAddress(lhs_buffer_);
+  se::DeviceMemoryBase rhs_data = allocs.GetDeviceAddress(rhs_buffer_);
+  se::DeviceMemoryBase output_data = allocs.GetDeviceAddress(output_buffer_);
+  se::DeviceMemoryBase add_data;
+  se::DeviceMemoryBase bias_data;
+
+  auto& buffer_allocations = *params.buffer_allocations;
+  se::OwningScratchAllocator<> scratch_allocator(
+      buffer_allocations.device_ordinal(),
+      buffer_allocations.memory_allocator());
+
+  se::gpu::BlasLt::Epilogue epilogue = se::gpu::BlasLt::Epilogue::kDefault;
+  return RunGemm(config_, lhs_data, rhs_data, add_data, output_data, bias_data,
+                 params.stream, epilogue, &scratch_allocator);
+}
+#endif // !GOOGLE_SYCL
 
 Status GemmThunk::Initialize(se::StreamExecutor* executor,
                              ExecutableSource src) {
diff --git a/xla/service/gpu/gpu_executable.cc b/xla/service/gpu/gpu_executable.cc
index a576d8922..1846cd6a8 100644
--- a/xla/service/gpu/gpu_executable.cc
+++ b/xla/service/gpu/gpu_executable.cc
@@ -33,15 +33,15 @@ limitations under the License.
 #include "mlir/Parser/Parser.h"  // from @llvm-project
 #include "xla/hlo/ir/hlo_instruction.h"
 #include "xla/map_util.h"
-#include "xla/mlir/runtime/ir/rt_ops.h"
-#include "xla/mlir/runtime/transforms/compilation_pipeline_gpu.h"
-#include "xla/mlir/runtime/transforms/type_converter.h"
-#include "xla/runtime/executable.h"
+// #include "xla/mlir/runtime/ir/rt_ops.h"
+// #include "xla/mlir/runtime/transforms/compilation_pipeline_gpu.h"
+// #include "xla/mlir/runtime/transforms/type_converter.h"
+// #include "xla/runtime/executable.h"
 #include "xla/service/buffer_assignment.h"
 #include "xla/service/gpu/buffer_allocations.h"
 #include "xla/service/gpu/gpu_constants.h"
 #include "xla/service/gpu/non_atomically_upgradeable_rw_lock.h"
-#include "xla/service/gpu/runtime/executable.h"
+// #include "xla/service/gpu/runtime/executable.h"
 #include "xla/service/gpu/stream_executor_util.h"
 #include "xla/service/gpu/thunk.h"
 #include "xla/service/hlo_parser.h"
@@ -78,7 +78,11 @@ namespace xla {
 namespace gpu {
 
 bool IsXlaRuntimeExecutableEnabled(const HloModuleConfig& config) {
+#if GOOGLE_SYCL
+  return false;
+#else
   return config.debug_options().xla_gpu_enable_xla_runtime_executable();
+#endif
 }
 
 namespace {
@@ -106,7 +110,7 @@ StatusOr<std::unique_ptr<GpuExecutable>> GpuExecutable::Create(Params params) {
     result->thunks_ = std::move(std::get<OwnedThunkSequence>(executable));
     return result;
   }
-
+#if !GOOGLE_SYCL
   if (std::holds_alternative<OwnedGpuRuntimeProgram>(executable)) {
     auto& program = std::get<OwnedGpuRuntimeProgram>(executable);
     TF_ASSIGN_OR_RETURN(
@@ -114,7 +118,7 @@ StatusOr<std::unique_ptr<GpuExecutable>> GpuExecutable::Create(Params params) {
         GpuRuntimeExecutable::Create(result->module_name_, std::move(program)));
     return result;
   }
-
+#endif
   return InternalError("No XLA gpu executable was provided");
 }
 
@@ -186,7 +190,9 @@ Status GpuExecutable::CheckCompatibilityWithServiceExecutableRunOptions(
         << "}, but was {" << std::get<se::CudaComputeCapability>(cc).ToString()
         << "}";
   } else {
+#if !GOOGLE_SYCL
     return InternalError("Unknown platform");
+#endif
   }
 
   return OkStatus();
@@ -207,6 +213,9 @@ Status ExecuteThunks(const std::string& module_name, ModuleIdentifier module_id,
   se::StreamExecutor* executor = main_stream->parent();
   stream_executor::StreamPriority stream_priority =
       stream_executor::StreamPriority::Default;
+#if GOOGLE_SYCL
+  use_highest_priority_for_async_stream = false;
+#endif
   if (use_highest_priority_for_async_stream) {
     stream_priority = stream_executor::StreamPriority::Highest;
   }
@@ -310,10 +319,16 @@ GpuExecutable::ResolveConstantGlobals(se::Stream* stream) {
   // The CUDA driver isn't able to load a PTX and a binary which are both empty.
   // It's okay if we skip loading in this case; if the module isn't loaded, all
   // symbol lookups will fail, just as they should for an empty module.
+#if !GOOGLE_SYCL
   if (!(executor->platform()->id() == stream_executor::cuda::kCudaPlatformId &&
         binary().empty() && text().empty())) {
     TF_RETURN_IF_ERROR(executor->LoadModule(module_spec, &module_handle));
   }
+#else
+  if (module_spec.has_cuda_cubin_in_memory()) {
+    TF_RETURN_IF_ERROR(executor->LoadModule(module_spec, &module_handle));
+  }
+#endif
 
   // A flag signalling if constant initialization submitted memcpy operations
   // to the `stream`.
@@ -341,6 +356,26 @@ GpuExecutable::ResolveConstantGlobals(se::Stream* stream) {
         submitted_mem_copies = true;
       }
     } else {
+#if GOOGLE_SYCL
+      // SYCL: content may be empty
+      if (info.content.empty()) {
+        // LLVM module contains the const variable, but it still fails to look
+        // up symbol. So allocate an empty buffer here.
+        void* opaque = nullptr;
+        size_t bytes = 0;
+        global = se::DeviceMemoryBase(opaque, bytes);
+      } else {
+        TF_ASSIGN_OR_RETURN(
+            auto shared, executor->CreateOrShareConstant(stream, info.content));
+        global = *shared;
+        VLOG(3) << "Allocated (or shared) global " << info.symbol_name << " at "
+                << global.opaque();
+        // XLA will continue to own this global at least until this executable
+        // is destroyed (longer if another, longer-lived executable shares the
+        // same constant).
+        shared_constants_.push_back(std::move(shared));
+      }
+#else
       // The constant was not defined in the PTX and therefore must be both
       // allocated and initialized by XLA here.
       CHECK(!info.content.empty());
@@ -354,6 +389,7 @@ GpuExecutable::ResolveConstantGlobals(se::Stream* stream) {
       // destroyed (longer if another, longer-lived executable shares the same
       // constant).
       shared_constants_.push_back(std::move(shared));
+#endif
     }
 
     if (info.allocation_index != -1) {
@@ -494,7 +530,7 @@ StatusOr<ScopedShapedBuffer> GpuExecutable::ExecuteAsyncOnStream(
                       ExecuteAsyncOnStreamImpl(run_options, arguments));
   return out.ConsumeResult();
 }
-
+#if !GOOGLE_SYCL
 static Status ExecuteXlaRuntime(const std::string& module_name,
                                 ModuleIdentifier module_id,
                                 GpuRuntimeExecutable& gpu_runtime_executable,
@@ -528,7 +564,7 @@ static Status ExecuteXlaRuntime(const std::string& module_name,
       run_options, start_nanos,
       block_host_until_done ? run_options->stream() : nullptr);
 }
-
+#endif
 Status GpuExecutable::PopulatePersistentTempBuffers(
     se::StreamExecutor* executor) {
   auto search = persistent_temp_buffers_.find(executor);
@@ -789,13 +825,13 @@ Status GpuExecutable::ExecuteThunksOrXlaRuntime(
       if (temp_buffer == nullptr) temp_buffer = &alloc;
     }
   }
-
+#if !GOOGLE_SYCL
   if (gpu_runtime_executable_) {
     return ExecuteXlaRuntime(module_name_, unique_id, *gpu_runtime_executable_,
                              run_options, text_, binary_, buffer_allocations,
                              temp_buffer, block_host_until_done, gpu_lock);
   }
-
+#endif
   return FailedPrecondition("Expected XLA gpu executable is not supplied.");
 }
 
@@ -928,7 +964,7 @@ GetOutputInfo(const HloModule& hlo_module, const BufferAssignment& assignment) {
       }));
   return output;
 }
-
+#if !GOOGLE_SYCL
 GpuExecutable::GpuExecutable(
     std::shared_ptr<HloModule> hlo_module, std::string asm_text,
     std::vector<uint8_t> binary, std::vector<ConstantInfo> constants,
@@ -1137,6 +1173,6 @@ StatusOr<std::string_view> GpuExecutable::GetMlirModule() const {
     return Internal("gpu_runtime_executable is null");
   return gpu_runtime_executable_->GetMlirModule();
 }
-
+#endif
 }  // namespace gpu
 }  // namespace xla
diff --git a/xla/service/gpu/gpu_executable.h b/xla/service/gpu/gpu_executable.h
index bef637f59..029d60184 100644
--- a/xla/service/gpu/gpu_executable.h
+++ b/xla/service/gpu/gpu_executable.h
@@ -37,7 +37,7 @@ limitations under the License.
 #include "xla/service/executable.h"
 #include "xla/service/gpu/buffer_allocations.h"
 #include "xla/service/gpu/non_atomically_upgradeable_rw_lock.h"
-#include "xla/service/gpu/runtime/executable.h"
+// #include "xla/service/gpu/runtime/executable.h"
 #include "xla/service/gpu/thunk.h"
 #include "xla/service/hlo_execution_profile.h"
 #include "xla/service/shaped_buffer.h"
@@ -52,6 +52,25 @@ namespace gpu {
 // Returns whether GpuExecutable runs with Xla Runtime.
 bool IsXlaRuntimeExecutableEnabled(const HloModuleConfig& config);
 
+// SYCL: dummy GpuRuntimeProgram for compilation
+#if GOOGLE_SYCL
+struct GpuRuntimeProgram {
+  GpuRuntimeProgram(std::string entry_point, std::string module,
+                    std::vector<int64_t> buffer_sizes,
+                    DebugOptions debug_options)
+      : entry_point(std::move(entry_point)),
+        module(std::move(module)),
+        buffer_sizes(std::move(buffer_sizes)),
+        debug_options(std::move(debug_options)) {}
+
+  std::string entry_point;
+  std::string module;
+  std::vector<int64_t> buffer_sizes;
+  DebugOptions debug_options;
+};
+class GpuRuntimeExecutable {};
+#endif
+
 // GPU-targeting implementation of the XLA Executable interface.
 //
 // Launches the given GPU kernel via the StreamExecutor.
@@ -118,7 +137,7 @@ class GpuExecutable : public Executable {
       std::vector<BufferAllocation>* allocations,
       absl::flat_hash_map<ShapeIndex, OutputInfo>* output_info,
       Shape* output_shape);
-
+#if !GOOGLE_SYCL
   // Returns an Executable that is loaded from an object file (XLA program
   // compiled to a native function using the XLA Runtime stack).
   static StatusOr<std::unique_ptr<Executable>> LoadFromObjFile(
@@ -128,7 +147,7 @@ class GpuExecutable : public Executable {
       absl::string_view asm_text, absl::string_view binary,
       std::vector<ConstantInfo> constants, se::GpuComputeCapability gpu_version,
       stream_executor::StreamExecutor* executor);
-
+#endif
   // Constructor to use when loading a GpuExecutable from an object file (native
   // function compiled for XLA Runtime). Omits setting class members that aren't
   // used in XLA Runtime execution mode.
diff --git a/xla/service/gpu/gpu_fused_mha_runner.cc b/xla/service/gpu/gpu_fused_mha_runner.cc
index 74c3d8397..293dbccd7 100644
--- a/xla/service/gpu/gpu_fused_mha_runner.cc
+++ b/xla/service/gpu/gpu_fused_mha_runner.cc
@@ -153,6 +153,8 @@ void AssignScale(GpufMHAConfig &config,
   double fmha_scale = 0.0;
 
   switch (config.kind) {
+    // SYCL: supports bias + softmax
+    case CudnnfMHAKind::kSoftmax:
     case CudnnfMHAKind::kScaleBiasMaskSoftmax:
     case CudnnfMHAKind::kScaleBiasMaskSoftmaxDropout:
     case CudnnfMHAKind::kScaleMaskSoftmax:
diff --git a/xla/service/gpu/gpu_fusible.cc b/xla/service/gpu/gpu_fusible.cc
index 744dd1ce1..f1df13275 100644
--- a/xla/service/gpu/gpu_fusible.cc
+++ b/xla/service/gpu/gpu_fusible.cc
@@ -462,6 +462,13 @@ FusionDecision IsProducerConsumerFusible(const HloInstruction& producer,
   }
 
   if (IsInputFusibleReduction(producer)) {
+#if GOOGLE_SYCL
+    // TODO: Check on latest XLA. Reduction epilogue fusion may cause
+    // regression for row reductions cases with large dim.
+    // It changes fusion kind from kInput to kLoop, and
+    // will fail to enter the row vectorization pass.
+    return "Reduction epilogue fusion is not enabled.";
+#endif
     if (!producer.GetModule()
              ->config()
              .debug_options()
diff --git a/xla/service/gpu/gpu_layout_assignment.cc b/xla/service/gpu/gpu_layout_assignment.cc
index ad2d7ffa6..06c8ab79b 100644
--- a/xla/service/gpu/gpu_layout_assignment.cc
+++ b/xla/service/gpu/gpu_layout_assignment.cc
@@ -69,7 +69,7 @@ HeuristicLayoutAssignment(const HloInstruction* instr,
       std::make_tuple(DataLayout::kBatchDepthYX4, FilterLayout::kOutputInputYX4,
                       DataLayout::kBatchDepthYX4);
   constexpr auto kAllNHWC =
-      std::make_tuple(DataLayout::kBatchYXDepth, FilterLayout::kOutputYXInput,
+      std::make_tuple(DataLayout::kBatchYXDepth, FilterLayout::kYXInputOutput,
                       DataLayout::kBatchYXDepth);
 
   // Integer convolution must use NHWC or NCHW_VECT_C.
diff --git a/xla/service/gpu/gpu_transfer_manager.cc b/xla/service/gpu/gpu_transfer_manager.cc
index 5bd1c557e..f99b9b4c4 100644
--- a/xla/service/gpu/gpu_transfer_manager.cc
+++ b/xla/service/gpu/gpu_transfer_manager.cc
@@ -34,6 +34,7 @@ limitations under the License.
 #include "xla/stream_executor/host/host_platform_id.h"
 #include "xla/stream_executor/multi_platform_manager.h"
 #include "xla/stream_executor/rocm/rocm_platform_id.h"
+#include "xla/stream_executor/sycl/sycl_platform_id.h"
 #include "xla/stream_executor/stream_executor.h"
 #include "xla/types.h"
 #include "xla/util.h"
@@ -207,11 +208,20 @@ static std::unique_ptr<xla::TransferManager> CreateAMDGPUTransferManager() {
           .getPointerSize(0 /* default address space */));
 }
 
+static std::unique_ptr<xla::TransferManager> CreateSYCLTransferManager() {
+  return std::make_unique<xla::gpu::GpuTransferManager>(
+      /*id=*/stream_executor::sycl::kSyclPlatformId,
+      /*pointer_size=*/llvm::DataLayout(xla::gpu::spir::DataLayout())
+          .getPointerSize(0 /* default address space */));
+}
+
 static bool InitModule() {
   xla::TransferManager::RegisterTransferManager(
       stream_executor::cuda::kCudaPlatformId, &CreateNVPTXTransferManager);
   xla::TransferManager::RegisterTransferManager(
       stream_executor::rocm::kROCmPlatformId, &CreateAMDGPUTransferManager);
+  xla::TransferManager::RegisterTransferManager(
+      stream_executor::sycl::kSyclPlatformId, &CreateSYCLTransferManager);
   return true;
 }
 static bool module_initialized = InitModule();
diff --git a/xla/service/gpu/gpublas_lt_matmul_thunk.cc b/xla/service/gpu/gpublas_lt_matmul_thunk.cc
index 4b3d4f1fc..be716e5b6 100644
--- a/xla/service/gpu/gpublas_lt_matmul_thunk.cc
+++ b/xla/service/gpu/gpublas_lt_matmul_thunk.cc
@@ -25,6 +25,10 @@ limitations under the License.
 #include "xla/stream_executor/scratch_allocator.h"
 #include "tsl/platform/logging.h"
 
+#if GOOGLE_SYCL
+#include "xla/service/gpu/onednn_matmul_utils.h"
+#endif
+
 namespace xla {
 namespace gpu {
 
@@ -53,6 +57,36 @@ CublasLtMatmulThunk::CublasLtMatmulThunk(
       d_scale_buffer_(d_scale),
       d_amax_buffer_(d_amax) {}
 
+#if GOOGLE_SYCL
+Status CublasLtMatmulThunk::ExecuteOnStream(const ExecuteParams& params) {
+  VLOG(3) << "Running cublas_lt matmul thunk";
+  const BufferAllocations& allocs = *params.buffer_allocations;
+
+  se::DeviceMemoryBase a, b, c, d;
+  if (a_buffer_.allocation() != nullptr) {
+    a = allocs.GetDeviceAddress(a_buffer_);
+  }
+  if (b_buffer_.allocation() != nullptr) {
+    b = allocs.GetDeviceAddress(b_buffer_);
+  }
+  if (c_buffer_.allocation() != nullptr) {
+    c = allocs.GetDeviceAddress(c_buffer_);
+  }
+  if (d_buffer_.allocation() != nullptr) {
+    d = allocs.GetDeviceAddress(d_buffer_);
+  }
+
+  se::DeviceMemoryBase bias, a_scale, b_scale, c_scale, d_scale, d_amax;
+  if (bias_buffer_.allocation() != nullptr) {
+    bias = allocs.GetDeviceAddress(bias_buffer_);
+  }
+
+  se::OwningScratchAllocator<> scratch_allocator(allocs.device_ordinal(),
+                                                 allocs.memory_allocator());
+  return RunGemm(gemm_config_, a, b, c, d, bias, params.stream, epilogue_,
+                 &scratch_allocator);
+}
+#else  // GOOGLE_SYCL
 Status CublasLtMatmulThunk::ExecuteOnStream(const ExecuteParams& params) {
   TF_ASSIGN_OR_RETURN(auto plan, GetMatmulPlan(params.stream));
   TF_ASSIGN_OR_RETURN(auto algorithm, GetMatmulAlgorithm(plan));
@@ -119,6 +153,6 @@ CublasLtMatmulThunk::GetMatmulAlgorithm(
   }
   return it->second;
 }
-
+#endif  // GOOGLE_SYCL
 }  // namespace gpu
 }  // namespace xla
diff --git a/xla/service/gpu/gpublas_lt_matmul_thunk.h b/xla/service/gpu/gpublas_lt_matmul_thunk.h
index 5a394dbf8..2516e0604 100644
--- a/xla/service/gpu/gpublas_lt_matmul_thunk.h
+++ b/xla/service/gpu/gpublas_lt_matmul_thunk.h
@@ -48,6 +48,7 @@ class CublasLtMatmulThunk : public Thunk {
   Status ExecuteOnStream(const ExecuteParams& params) override;
 
  private:
+#if !GOOGLE_SYCL
   StatusOr<se::gpu::BlasLt::MatmulPlan*> GetMatmulPlan(
       const stream_executor::Stream* stream);
   StatusOr<std::optional<se::gpu::BlasLt::MatmulAlgorithm> > GetMatmulAlgorithm(
@@ -62,7 +63,7 @@ class CublasLtMatmulThunk : public Thunk {
   absl::flat_hash_map<const se::gpu::BlasLt::MatmulPlan*,
                       se::gpu::BlasLt::MatmulAlgorithm>
       matmul_algorithm_cache_ ABSL_GUARDED_BY(matmul_algorithm_cache_mutex_);
-
+#endif
   GemmConfig gemm_config_;
   se::gpu::BlasLt::Epilogue epilogue_;
   int64_t algorithm_idx_;
diff --git a/xla/service/gpu/hlo_fusion_analysis.cc b/xla/service/gpu/hlo_fusion_analysis.cc
index 0605af7c2..e6d04d737 100644
--- a/xla/service/gpu/hlo_fusion_analysis.cc
+++ b/xla/service/gpu/hlo_fusion_analysis.cc
@@ -784,6 +784,8 @@ int HloFusionAnalysis::CalculateVirtualThreadScalingFactorForReduction(
   int64_t dimx = reduction_dimensions.dimensions[kDimX];
   if (reduction_dimensions.is_row_reduction && dimx <= 128) {
     int rows_per_warp = RowReductionGetRowsPerWarp(dimx);
+    // SYCL: larger thread number causes hang.
+    return rows_per_warp;
     const auto* cuda_cc = std::get_if<se::CudaComputeCapability>(
         &device_info_->gpu_compute_capability());
     if (cuda_cc != nullptr &&
diff --git a/xla/service/gpu/ir_emission_utils.cc b/xla/service/gpu/ir_emission_utils.cc
index a28628c14..84a780506 100644
--- a/xla/service/gpu/ir_emission_utils.cc
+++ b/xla/service/gpu/ir_emission_utils.cc
@@ -83,11 +83,11 @@ bool IsMatrixMultiplication(const HloInstruction& dot) {
   const DotDimensionNumbers& dim_numbers = dot.dot_dimension_numbers();
 
   PrimitiveType output_primitive_type = dot.shape().element_type();
+  // Disable F64, C64, C128
   bool type_is_allowed =
       (output_primitive_type == F8E4M3FN || output_primitive_type == F8E5M2 ||
        output_primitive_type == F16 || output_primitive_type == BF16 ||
-       output_primitive_type == F32 || output_primitive_type == F64 ||
-       output_primitive_type == C64 || output_primitive_type == C128) ||
+       output_primitive_type == F32) ||
       (output_primitive_type == S32 && lhs_shape.element_type() == S8 &&
        rhs_shape.element_type() == S8);
   bool shapes_are_valid =
@@ -235,6 +235,31 @@ llvm::Value* EmitNVPTXShflDown(llvm::Value* value, llvm::Value* offset,
       intrinsic, {b->getInt32(-1), value, offset, b->getInt32(WarpSize() - 1)});
 }
 
+// Helper function to emit call to SPIR shfl_down intrinsic.
+llvm::Value* EmitSPIRShflDown(llvm::Value* value, llvm::Value* offset,
+                              llvm::IRBuilder<>* b) {
+  llvm::Module* module = b->GetInsertBlock()->getModule();
+  llvm::Intrinsic::ID llvm_intrinsic_id;
+  CHECK_EQ(value->getType()->getPrimitiveSizeInBits(), 32);
+  if (value->getType()->isFloatTy()) {
+    return EmitDeviceFunctionCall(
+        "_Z32__spirv_SubgroupShuffleDownINTELffj",
+        {value, value, offset}, {F32, F32, U32}, F32,
+        llvm::AttrBuilder(b->getContext())
+            .addAttribute(llvm::Attribute::NoUnwind)
+            .addAttribute(llvm::Attribute::Convergent),
+        b);
+  } else {
+    return EmitDeviceFunctionCall(
+        "_Z32__spirv_SubgroupShuffleDownINTELjjj",
+        {value, value, offset}, {U32, U32, U32}, U32,
+        llvm::AttrBuilder(b->getContext())
+            .addAttribute(llvm::Attribute::NoUnwind)
+            .addAttribute(llvm::Attribute::Convergent),
+        b);
+  }
+}
+
 llvm::Value* EmitFullWarpShuffleDown(llvm::Value* value, llvm::Value* offset,
                                      llvm::IRBuilder<>* builder) {
   int bit_width = value->getType()->getPrimitiveSizeInBits();
@@ -247,6 +272,8 @@ llvm::Value* EmitFullWarpShuffleDown(llvm::Value* value, llvm::Value* offset,
       return EmitNVPTXShflDown(value, offset, builder);
     } else if (target_triple.getArch() == llvm::Triple::amdgcn) {
       return EmitAMDGPUShflDown(value, offset, builder);
+    } else if (target_triple.isSPIR()) {
+      return EmitSPIRShflDown(value, offset, builder);
     } else {
       LOG(FATAL) << "Invalid triple " << target_triple.str();
     }
@@ -268,6 +295,9 @@ llvm::Value* EmitFullWarpShuffleDown(llvm::Value* value, llvm::Value* offset,
     } else if (target_triple.getArch() == llvm::Triple::amdgcn) {
       insert_val = EmitAMDGPUShflDown(builder->CreateExtractElement(x, i),
                                       offset, builder);
+    } else if (target_triple.isSPIR()) {
+      insert_val = EmitSPIRShflDown(builder->CreateExtractElement(x, i), offset,
+                                    builder);
     } else {
       LOG(FATAL) << "Invalid triple " << target_triple.str();
     }
@@ -954,5 +984,9 @@ bool IsAMDGPU(const llvm::Module* module) {
   return llvm::Triple(module->getTargetTriple()).isAMDGPU();
 }
 
+bool IsSPIR(const llvm::Module* module) {
+  return llvm::Triple(module->getTargetTriple()).isSPIR();
+}
+
 }  // namespace gpu
 }  // namespace xla
diff --git a/xla/service/gpu/ir_emission_utils.h b/xla/service/gpu/ir_emission_utils.h
index 890353151..ec9430494 100644
--- a/xla/service/gpu/ir_emission_utils.h
+++ b/xla/service/gpu/ir_emission_utils.h
@@ -222,6 +222,9 @@ std::string GetIrNameFromLoc(mlir::Location loc);
 // Whether the module's target is an AMD GPU.
 bool IsAMDGPU(const llvm::Module* module);
 
+// Whether the module's target is an AMD GPU.
+bool IsSPIR(const llvm::Module* module);
+
 }  // namespace gpu
 }  // namespace xla
 
diff --git a/xla/service/gpu/ir_emitter_context.cc b/xla/service/gpu/ir_emitter_context.cc
index af3a8530b..7a74c6fa2 100644
--- a/xla/service/gpu/ir_emitter_context.cc
+++ b/xla/service/gpu/ir_emitter_context.cc
@@ -20,6 +20,7 @@ limitations under the License.
 #include <utility>
 #include <vector>
 
+#include "llvm/TargetParser/Triple.h"
 #include "xla/service/gpu/gpu_constants.h"
 
 namespace xla {
@@ -54,6 +55,8 @@ void IrEmitterContext::emit_constant(int64_t num_elements,
       return llvm::ConstantAggregateZero::get(global_type);
     }
 
+    // SYCL: always set info.content.
+    info.content = content;
     std::vector<uint8_t> padded(kMinConstAllocationInBytes, 0);
     absl::c_copy(content, padded.begin());
     return llvm::ConstantDataArray::get<uint8_t>(
@@ -69,14 +72,38 @@ void IrEmitterContext::emit_constant(int64_t num_elements,
   //
   // We may have to be more clever here in the future if we notice that we're
   // keeping around too many globals because of their linkage.
+  // SYCL: Hardcode to global addrspace
+  bool is_spir = llvm::Triple(llvm_module_->getTargetTriple()).isSPIR();
+  int addrspace = is_spir ? 1 : 0;
   llvm::GlobalVariable* global_for_const = new llvm::GlobalVariable(
       global_type, /*isConstant=*/should_emit_initializer,
       llvm::GlobalValue::ExternalLinkage,
       /*Initializer=*/initializer, symbol_name,
       /*TLMode=*/llvm::GlobalValue::NotThreadLocal,
-      /*AddressSpace=*/0,
+      /*AddressSpace=*/addrspace,
       /*isExternallyInitialized=*/false);
   global_for_const->setAlignment(llvm::Align(kConstantBufferAlignBytes));
+
+  if (is_spir) {
+    // SYCL: Add spirv.Decorations for global variable. See document about the
+    // annotation:
+    // https://github.com/intel/llvm/blob/sycl/sycl/doc/design/spirv-extensions/SPV_INTEL_global_variable_decorations.asciidoc
+    llvm::LLVMContext& context = llvm_module_->getContext();
+    llvm::SmallVector<llvm::Metadata*, 4> metadatas;
+    std::vector<llvm::Metadata*> ops;
+
+    auto* kind = llvm::ConstantAsMetadata::get(llvm::ConstantInt::get(
+        llvm::Type::getInt32Ty(context), /*IDecHostAccessINTEL*/ 6147));
+    ops.push_back(kind);
+    auto* const acc_mode = llvm::ConstantAsMetadata::get(llvm::ConstantInt::get(
+        llvm::Type::getInt32Ty(context), /*AccessMode*/ 2));
+    ops.push_back(acc_mode);
+    ops.push_back(llvm::MDString::get(context, symbol_name));
+    metadatas.push_back(llvm::MDNode::get(context, ops));
+
+    llvm::MDNode* md_list = llvm::MDNode::get(context, metadatas);
+    global_for_const->setMetadata("spirv.Decorations", md_list);
+  }
   llvm_module_->insertGlobalVariable(global_for_const);
 
   info.symbol_name.assign(symbol_name);
diff --git a/xla/service/gpu/ir_emitter_nested.cc b/xla/service/gpu/ir_emitter_nested.cc
index ad83ba15a..114e2765d 100644
--- a/xla/service/gpu/ir_emitter_nested.cc
+++ b/xla/service/gpu/ir_emitter_nested.cc
@@ -33,6 +33,11 @@ limitations under the License.
 #include "xla/service/llvm_ir/llvm_util.h"
 #include "xla/service/llvm_ir/tuple_ops.h"
 
+#if GOOGLE_SYCL
+#include "xla/service/gpu/ir_emission_utils.h"
+#include "llvm/TargetParser/Triple.h"
+#endif
+
 namespace xla {
 namespace gpu {
 namespace {
@@ -380,6 +385,14 @@ bool MaybeEmitDirectAtomicOperation(llvm::IRBuilder<>* builder,
       return true;
     }
 
+    if (target_triple.isSPIR() &&
+        element_type == F32) {
+      builder->CreateAtomicRMW(llvm::AtomicRMWInst::FAdd, output_address,
+                               source, llvm::MaybeAlign(),
+                               llvm::AtomicOrdering::SequentiallyConsistent);
+      return true;
+    }
+
     if (is_atomic_integral) {
       // integral + integral
       builder->CreateAtomicRMW(
diff --git a/xla/service/gpu/ir_emitter_unnested.cc b/xla/service/gpu/ir_emitter_unnested.cc
index 53d02c62d..cf72aa8b0 100644
--- a/xla/service/gpu/ir_emitter_unnested.cc
+++ b/xla/service/gpu/ir_emitter_unnested.cc
@@ -896,6 +896,17 @@ Status IrEmitterUnnested::EmitConvolutionThunk(mlir::Operation* op) {
     TF_RETURN_IF_ERROR(set_activation_mode(conv));
     descriptor.backend_config.set_side_input_scale(
         conv.getSideInputScale().convertToDouble());
+
+    // SYCL: OneDNN requires inplace sum
+    if (operand_slices[3] != conv_result_slice) {
+      thunks.push_back(std::make_unique<DeviceToDeviceCopyThunk>(
+          GetThunkInfo(op),
+          /*side_input_buffer=*/operand_slices[3],
+          /*destination_buffer=*/conv_result_slice,
+          /*mem_size=*/ShapeUtil::ByteSizeOf(GetShape(op->getOperand(3))),
+          /*source_value=*/op->getOperand(3),
+          /*destination_value=*/op->getOperand(4)));
+    }
   } else {
     return InternalError("EmitConvolutionThunk: Unexpected operation");
   }
diff --git a/xla/service/gpu/launch_dimensions.cc b/xla/service/gpu/launch_dimensions.cc
index 47762ba19..6b5dcfa32 100644
--- a/xla/service/gpu/launch_dimensions.cc
+++ b/xla/service/gpu/launch_dimensions.cc
@@ -93,6 +93,9 @@ struct BlockSizes {
 BlockSizes GetBlockSizes(LaunchDimensionsConfig dim_config,
                          const se::DeviceDescription& gpu_device_info,
                          const Shape& shape, int64_t num_elements) {
+#if !GOOGLE_SYCL
+  // TODO: It will set 128 threads per block by default. We prefer to use
+  // the max value (1024 on PVC). It can benefit instructions like scatter.
   if (!dim_config.row_vectorized && !dim_config.few_waves) {
     BlockSizes result;
     const int kWarpSchedulers = 4;
@@ -103,6 +106,7 @@ BlockSizes GetBlockSizes(LaunchDimensionsConfig dim_config,
         num_elements, result.threads_per_block_x * result.threads_per_block_y);
     return result;
   }
+#endif
 
   int64_t threads_per_block_row_vectorized =
       ThreadsPerBlockRowVectorized(shape, gpu_device_info, dim_config);
diff --git a/xla/service/gpu/llvm_gpu_backend/BUILD b/xla/service/gpu/llvm_gpu_backend/BUILD
index 619acb37a..d1abf8fc2 100644
--- a/xla/service/gpu/llvm_gpu_backend/BUILD
+++ b/xla/service/gpu/llvm_gpu_backend/BUILD
@@ -3,6 +3,10 @@ load(
     "@local_config_rocm//rocm:build_defs.bzl",
     "if_rocm_is_configured",
 )
+load(
+    "@local_config_sycl//sycl:build_defs.bzl",
+    "if_sycl_is_configured",
+)
 
 package(
     # copybara:uncomment default_applicable_licenses = ["//tensorflow:license"],
@@ -66,6 +70,8 @@ cc_library(
     ] + if_rocm_is_configured([
         "@local_config_rocm//rocm:rocm_headers",
         "@llvm-project//llvm:AMDGPUCodeGen",
+    ]) + if_sycl_is_configured([
+        "@llvm_spir//:llvm_spir_translator",
     ]),
 )
 
diff --git a/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc b/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc
index 20c22245c..87e3a600c 100644
--- a/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc
+++ b/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc
@@ -73,6 +73,9 @@ limitations under the License.
 #include "rocm/rocm_config.h"
 #endif
 
+#include "LLVMSPIRVLib.h"
+#include "LLVMSPIRVOpts.h"
+
 namespace xla {
 namespace gpu {
 namespace {
@@ -153,7 +156,6 @@ std::unique_ptr<llvm::TargetMachine> GetTargetMachine(
                << " -- " << error;
     return nullptr;
   }
-
   llvm::TargetOptions target_options =
       llvm::codegen::InitTargetOptionsFromCodeGenFlags(llvm::Triple());
 
@@ -365,10 +367,13 @@ Status LinkAndOptimizeModule(
   llvm::CGSCCAnalysisManager cgam;
   llvm::ModuleAnalysisManager mam;
 
-  fam.registerPass([&] { return target_machine->getTargetIRAnalysis(); });
+  if (target_machine)
+    fam.registerPass([&] { return target_machine->getTargetIRAnalysis(); });
 
+  // SYCL: customized config
   llvm::PipelineTuningOptions pto;
-  pto.SLPVectorization = true;
+  pto.SLPVectorization = false;
+  pto.LoopVectorization = false;
   pto.InlinerThreshold = inline_threshold;
 
   llvm::PassInstrumentationCallbacks pic;
@@ -1001,5 +1006,103 @@ StatusOr<std::vector<uint8_t>> CompileToHsaco(
 
 }  // namespace amdgpu
 
+namespace {
+std::unique_ptr<llvm::TargetMachine> SPIRGetTargetMachine(
+    llvm::Triple target_triple, se::GpuComputeCapability gpu_version,
+    const DebugOptions& debug_options) {
+  return nullptr;
+}
+
+Status SPIRTargetModuleLinker(llvm::Module* module,
+                              se::GpuComputeCapability gpu_version,
+                              const DebugOptions& debug_options,
+                              const std::string& device_bitcode_dir_path) {
+  return OkStatus();
+}
+
+StatusOr<std::string> EmitModuleToSpir(llvm::Module* module,
+                                       se::GpuComputeCapability gpu_version,
+                                       const DebugOptions& debug_options) {
+  SPIRV::TranslatorOpts::ExtensionsStatusMap ExtensionsStatus;
+  SPIRV::TranslatorOpts opts(SPIRV::VersionNumber::MaximumVersion,
+                             ExtensionsStatus);
+  opts.enableAllExtensions();  // enable all SPIR-V extension first
+
+  std::ostringstream oss;
+  std::string err;
+  bool success = llvm::writeSpirv(module, opts, oss, err);
+  if (!success) {
+    return xla::InternalError("Fails to convert LLVM as SPIR-V: %s", err);
+  }
+  return oss.str();
+}
+
+void SPIRBackendInit(const DebugOptions& debug_options) {
+
+  FeedLLVMWithFlags({"-slp-vectorize-hor=false"});
+
+  bool vec = true;
+  tsl::ReadBoolFromEnvVar("VECTORIZE", true, &vec);
+  if (vec) {
+    FeedLLVMWithFlags({
+        "-slp-min-reg-size=64",
+        "-slp-max-reg-size=64",
+    });
+  } else {
+    // TODO: sycl-opt disables all LLVM vectorization passes. Evaluate if it is
+    // needed.
+    FeedLLVMWithFlags({"-sycl-opt=1"});
+  }
+
+  llvm_ir::InitializeLLVMCommandLineOptions(
+      debug_options.xla_backend_extra_options());
+
+  llvm::PassRegistry* registry = llvm::PassRegistry::getPassRegistry();
+  InitializePasses(registry);
+}
+}  // namespace
+
+namespace spir {
+StatusOr<std::vector<uint8_t>> CompileToSpir(
+    llvm::Module* module, se::GpuComputeCapability gpu_version,
+    const DebugOptions& debug_options) {
+  std::string libdevice_dir_path;
+  static absl::once_flag backend_init_flag;
+  absl::call_once(backend_init_flag, SPIRBackendInit, debug_options);
+
+  std::string spir;
+  {
+    XLA_SCOPED_LOGGING_TIMER("Compile module " + module->getName().str());
+
+    // If the module has no functions or globals, there's nothing to compile.
+    if (module->empty() && module->global_empty()) {
+      VLOG(2) << "Module '" << module->getName().str()
+              << "' is empty. Skipping compilation.";
+      return std::vector<uint8_t>();
+    }
+
+    // No SPIR target machine?
+    llvm::Triple default_target_triple("spir64-unknown-unknown");
+    std::unique_ptr<llvm::TargetMachine> target_machine =
+        SPIRGetTargetMachine(default_target_triple, gpu_version, debug_options);
+
+    bool opt = true;
+    tsl::ReadBoolFromEnvVar("SYCL_LLVM_OPT", true, &opt);
+    if (opt) {
+      // Link with libdevice, and optimize the LLVM module.
+      TF_RETURN_IF_ERROR(LinkAndOptimizeModule(
+          module, gpu_version, debug_options, libdevice_dir_path,
+          SPIRTargetModuleLinker, default_target_triple, target_machine.get(),
+          kDefaultInlineThreshold));
+    }
+
+    // Lower optimized LLVM module to SPIR.
+    TF_ASSIGN_OR_RETURN(spir,
+                        EmitModuleToSpir(module, gpu_version, debug_options));
+  }
+  return std::vector<uint8_t>(spir.begin(), spir.end());
+}
+}  // namespace spir
+
 }  // namespace gpu
 }  // namespace xla
diff --git a/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.h b/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.h
index 9ff362c2d..13b6e24a9 100644
--- a/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.h
+++ b/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.h
@@ -66,6 +66,12 @@ StatusOr<std::vector<uint8_t>> CompileToHsaco(
     const std::string& module_config_cache_key);
 }  // namespace amdgpu
 
+namespace spir {
+StatusOr<std::vector<uint8_t>> CompileToSpir(
+    llvm::Module* module, se::GpuComputeCapability gpu_version,
+    const DebugOptions& debug_options);
+}  // namespace spir
+
 }  // namespace gpu
 }  // namespace xla
 
diff --git a/xla/service/gpu/matmul_utils.h b/xla/service/gpu/matmul_utils.h
index e6d318df2..59543ed8e 100644
--- a/xla/service/gpu/matmul_utils.h
+++ b/xla/service/gpu/matmul_utils.h
@@ -40,6 +40,23 @@ limitations under the License.
 #include "rocm/rocm_config.h"
 #endif
 
+namespace stream_executor {
+namespace cuda {
+namespace BlasLt {
+enum class Epilogue {
+  kDefault = 1,                   // No special postprocessing
+  kReLU = 2,                      // Apply point-wise ReLU function
+  kBias = 4,                      // Add broadcasted bias vector
+  kBiasThenReLU = kBias | kReLU,  // Apply bias and then ReLU transform
+  kGELU = 32,                // Apply GELU point-wise transform to the results
+  kGELUWithAux = 32 | 1024,  // Apply GELU with auxiliary output.
+  kBiasThenGELU = kBias | kGELU,  // Apply bias and then approximate GELU.
+  kBiasThenGELUWithAux = kBiasThenGELU | 1024,
+};
+}
+}  // namespace cuda
+}  // namespace stream_executor
+
 namespace xla {
 namespace gpu {
 
@@ -155,6 +172,7 @@ struct GemmConfig : public se::gpu::GemmConfig {
         op.getAlgorithm(), compute_precision, /*grad_x=*/false,
         /*grad_y=*/false);
   }
+
 };
 
 // Run the given GEMM instruction `gemm` subject to the configuration
diff --git a/xla/service/gpu/model/gpu_performance_model.cc b/xla/service/gpu/model/gpu_performance_model.cc
index 51579cd34..610ce12cc 100644
--- a/xla/service/gpu/model/gpu_performance_model.cc
+++ b/xla/service/gpu/model/gpu_performance_model.cc
@@ -230,7 +230,12 @@ LaunchDimensions EstimateFusionLaunchDimensions(
     auto launch_dimensions = fusion_analysis->GetLaunchDimensions();
     if (launch_dimensions.ok()) return *launch_dimensions;
   }
-  int64_t block_size = 128;  // Result for default LaunchDimensionsConfig.
+  // Result for default LaunchDimensionsConfig.
+#if GOOGLE_SYCL
+  int64_t block_size = RoundUpTo(device_info.threads_per_block_limit(),int64_t{32});
+#else
+  int64_t block_size = 128;
+#endif
   int64_t num_blocks = CeilOfRatio(estimated_num_threads, block_size);
   return LaunchDimensions(num_blocks, block_size);
 }
diff --git a/xla/service/gpu/parallel_loop_emitter.cc b/xla/service/gpu/parallel_loop_emitter.cc
index c6b76fd2b..1e85a5259 100644
--- a/xla/service/gpu/parallel_loop_emitter.cc
+++ b/xla/service/gpu/parallel_loop_emitter.cc
@@ -179,9 +179,7 @@ ParallelLoopEmitter::EmitIndexAndSetExitBasicBlock(absl::string_view loop_name,
   // for that dimensions.  This helps LLVM generate vectorized codes
   // in that cases.
   llvm::Value* row_index = nullptr;
-  if (!launch_config_.row_vectorized) {
-    array_indices.emplace_back(linear_index_base, shape_, b_);
-  } else {
+  if (launch_config_.row_vectorized) {
     // Simpler index for row computation.
     // This will allow LLVM to vectorize.
     row_index = b_->CreateMul(
@@ -193,19 +191,19 @@ ParallelLoopEmitter::EmitIndexAndSetExitBasicBlock(absl::string_view loop_name,
     array_indices.emplace_back(linear_index_base, multidim, shape_, b_);
   }
 
-  for (int i = 1; i < launch_config_.unroll_factor; ++i) {
+  for (int i = 0; i < launch_config_.unroll_factor; ++i) {
     llvm::Value* linear_index =
         b_->CreateAdd(linear_index_base, llvm::ConstantInt::get(index_type, i),
                       absl::StrCat("linear_index", i),
                       /*HasNUW=*/true, /*HasNSW=*/true);
-    if (!launch_config_.row_vectorized) {
-      array_indices.emplace_back(linear_index, shape_, b_);
-    } else {
+    if (launch_config_.row_vectorized && i > 0) {
       std::vector<llvm::Value*> multidim(shape_.rank(), nullptr);
       multidim.back() = b_->CreateAdd(
           row_index, llvm::ConstantInt::get(index_type, i),
           absl::StrCat("row_index_plus", i), /*HasNUW=*/true, /*HasNSW=*/true);
       array_indices.emplace_back(linear_index, multidim, shape_, b_);
+    } else {
+      array_indices.emplace_back(linear_index, shape_, b_);
     }
   }
 
diff --git a/xla/service/gpu/precompiled_kernels.cc b/xla/service/gpu/precompiled_kernels.cc
index d380a496c..c4f165ef9 100644
--- a/xla/service/gpu/precompiled_kernels.cc
+++ b/xla/service/gpu/precompiled_kernels.cc
@@ -155,6 +155,7 @@ class LazyKernel {
 
 }  // anonymous namespace
 
+#if !GOOGLE_SYCL
 Status MakeBatchPointers(se::Stream* stream, const se::GpuAsmOpts& asm_opts,
                          se::DeviceMemoryBase base_ptr, int stride_bytes, int n,
                          se::DeviceMemoryBase ptrs_out) {
@@ -179,6 +180,13 @@ Status MakeBatchPointers(se::Stream* stream, const se::GpuAsmOpts& asm_opts,
 #endif
   return OkStatus();
 }
-
+#else
+Status MakeBatchPointers(se::Stream* stream, const se::GpuAsmOpts& asm_opts,
+                         const se::DeviceMemoryBase& base_ptr, int stride_bytes, int n,
+                         se::DeviceMemoryBase& ptrs_out) {
+  ptrs_out = base_ptr;
+  return OkStatus();
+}
+#endif // !GOOGLE_SYCL
 }  // namespace gpu
 }  // namespace xla
diff --git a/xla/service/gpu/precompiled_kernels.h b/xla/service/gpu/precompiled_kernels.h
index 3d1e96afa..5edcd750f 100644
--- a/xla/service/gpu/precompiled_kernels.h
+++ b/xla/service/gpu/precompiled_kernels.h
@@ -51,10 +51,15 @@ namespace gpu {
 //    driver and slow down *all* work on the GPU.  So to do this right, we'd
 //    need to allocate the host memory as pinned, one alloc per stream.  Then
 //    we'd need to manage this memory without leaks.  This becomes complex!
+#if !GOOGLE_SYCL
 Status MakeBatchPointers(se::Stream* stream, const se::GpuAsmOpts& asm_opts,
                          se::DeviceMemoryBase base_ptr, int stride_bytes, int n,
                          se::DeviceMemoryBase ptrs_out);
-
+#else
+Status MakeBatchPointers(se::Stream* stream, const se::GpuAsmOpts& asm_opts,
+                         const se::DeviceMemoryBase& base_ptr, int stride_bytes, int n,
+                         se::DeviceMemoryBase& ptrs_out);
+#endif
 }  // namespace gpu
 }  // namespace xla
 #endif  // XLA_SERVICE_GPU_PRECOMPILED_KERNELS_H_
diff --git a/xla/service/gpu/runtime3/BUILD b/xla/service/gpu/runtime3/BUILD
index 6a66dd62f..6865d21aa 100644
--- a/xla/service/gpu/runtime3/BUILD
+++ b/xla/service/gpu/runtime3/BUILD
@@ -1,6 +1,7 @@
 load("//xla/tests:build_defs.bzl", "xla_test")
 load("//xla/stream_executor:build_defs.bzl", "if_gpu_is_configured")
 load("@tsl//tsl/platform/default:cuda_build_defs.bzl", "if_cuda_is_configured")
+load("@local_config_sycl//sycl:build_defs.bzl", "if_sycl_is_configured")
 
 package(
     # copybara:uncomment default_applicable_licenses = ["//tensorflow:license"],
@@ -176,8 +177,10 @@ cc_library(
         "//xla/hlo/ir:hlo",
         "//xla/service:buffer_assignment",
         "//xla/service/gpu:buffer_allocations",
+        "//xla/service/gpu:cusolver_context",
         "//xla/service/gpu:thunk",
         "//xla/stream_executor",
+        "//xla/stream_executor/gpu:gpu_helpers_header",
         "@com_google_absl//absl/container:flat_hash_map",
         "@com_google_absl//absl/strings",
         "@com_google_absl//absl/strings:str_format",
@@ -194,6 +197,7 @@ cc_library(
         "@com_google_absl//absl/strings:str_format",
         "//xla/service/gpu:buffer_allocations",
         "//xla/service/gpu:precompiled_kernels",
+        "//xla/service/gpu:cusolver_context",
         "//xla/service/gpu:thunk",
         "//xla:types",
         "//xla:util",
diff --git a/xla/service/gpu/runtime3/cholesky_thunk.cc b/xla/service/gpu/runtime3/cholesky_thunk.cc
index b00d8ee29..13e0b57c7 100644
--- a/xla/service/gpu/runtime3/cholesky_thunk.cc
+++ b/xla/service/gpu/runtime3/cholesky_thunk.cc
@@ -33,11 +33,11 @@ namespace xla {
 namespace gpu {
 
 namespace {
-
 template <typename T>
 Status DoPotrfBatched(const se::GpuAsmOpts& asm_opts, CholeskyParams* params,
                       se::Stream* stream, GpuSolverContext& context) {
   T* a_base = static_cast<T*>(params->a_buffer.opaque());
+#if !GOOGLE_SYCL
   se::DeviceMemory<int> infos(params->info_buffer);
 #if TENSORFLOW_USE_ROCSOLVER
   // hipsolver is not supported so allocate a GPU buffer
@@ -60,8 +60,11 @@ Status DoPotrfBatched(const se::GpuAsmOpts& asm_opts, CholeskyParams* params,
   // Now that we've set up the `as` array, we can call cusolver.
   return context.PotrfBatched(params->uplo, params->n, as, params->n, infos,
                               params->batch_size);
+#else // !GOOGLE_SYCL
+  return context.PotrfBatched(params->uplo, params->n, params->n,
+                              params->batch_size, &(params->workspace_buffer), a_base);  
+#endif // !GOOGLE_SYCL
 }
-
 }  // namespace
 
 CholeskyThunk::CholeskyThunk(ThunkInfo thunk_info,
diff --git a/xla/service/gpu/runtime3/cholesky_thunk.h b/xla/service/gpu/runtime3/cholesky_thunk.h
index f226a25f7..bab773e8b 100644
--- a/xla/service/gpu/runtime3/cholesky_thunk.h
+++ b/xla/service/gpu/runtime3/cholesky_thunk.h
@@ -30,6 +30,11 @@ limitations under the License.
 #include "xla/xla_data.pb.h"
 #include "tsl/platform/status.h"
 
+#if GOOGLE_SYCL
+#include "xla/service/gpu/cusolver_context.h"
+#include "xla/stream_executor/gpu/gpu_stream.h"
+#endif
+
 namespace xla {
 namespace gpu {
 
diff --git a/xla/service/gpu/runtime3/custom_call_thunk.cc b/xla/service/gpu/runtime3/custom_call_thunk.cc
index b53a258a5..d6471080a 100644
--- a/xla/service/gpu/runtime3/custom_call_thunk.cc
+++ b/xla/service/gpu/runtime3/custom_call_thunk.cc
@@ -34,7 +34,7 @@ limitations under the License.
 #include "xla/stream_executor/device_memory.h"
 #include "xla/util.h"
 
-#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM
+#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM || GOOGLE_SYCL
 #include "xla/stream_executor/gpu/gpu_stream.h"
 #endif
 
@@ -85,7 +85,7 @@ Status CustomCallThunk::ExecuteCustomCall(const ExecuteParams& params) {
     }
   }
 
-#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM
+#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM || GOOGLE_SYCL
   auto gpu_stream = se::gpu::AsGpuStreamValue(params.stream);
   XlaCustomCallStatus custom_call_status;
   call_target_(gpu_stream, buffers.data(), opaque_.data(), opaque_.size(),
@@ -96,11 +96,11 @@ Status CustomCallThunk::ExecuteCustomCall(const ExecuteParams& params) {
   } else {
     return OkStatus();
   }
-#else   //  GOOGLE_CUDA || TENSORFLOW_USE_ROCM
+#else   //  GOOGLE_CUDA || TENSORFLOW_USE_ROCM || GOOGLE_SYCL
   return Unavailable(
       "Custom calls on GPU are not supported in this configuration. Please "
       "build with --config=cuda or --config=rocm");
-#endif  //   GOOGLE_CUDA || TENSORFLOW_USE_ROCM
+#endif  //   GOOGLE_CUDA || TENSORFLOW_USE_ROCM || GOOGLE_SYCL
 }
 
 Status CustomCallThunk::ExecuteFfiHandler(const ExecuteParams& params) {
diff --git a/xla/service/gpu/runtime3/custom_call_thunk.h b/xla/service/gpu/runtime3/custom_call_thunk.h
index f544485ed..44868f856 100644
--- a/xla/service/gpu/runtime3/custom_call_thunk.h
+++ b/xla/service/gpu/runtime3/custom_call_thunk.h
@@ -32,7 +32,7 @@ limitations under the License.
 #include "xla/shape.h"
 #include "xla/status.h"
 
-#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM
+#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM || GOOGLE_SYCL
 #include "xla/stream_executor/gpu/gpu_types.h"
 #endif
 
@@ -52,11 +52,11 @@ namespace gpu {
 // compiler is allowed to create.
 class CustomCallThunk : public Thunk {
  public:
-#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM
+#if GOOGLE_CUDA || TENSORFLOW_USE_ROCM || GOOGLE_SYCL
   using Stream = stream_executor::gpu::GpuStreamHandle;
-#else   //  GOOGLE_CUDA || TENSORFLOW_USE_ROCM
+#else   //  GOOGLE_CUDA || TENSORFLOW_USE_ROCM || GOOGLE_SYCL
   using Stream = void*;
-#endif  //  GOOGLE_CUDA || TENSORFLOW_USE_ROCM
+#endif  //  GOOGLE_CUDA || TENSORFLOW_USE_ROCM || GOOGLE_SYCL
 
   using CustomCallTarget = std::function<void(Stream, void**, const char*,
                                               size_t, XlaCustomCallStatus*)>;
diff --git a/xla/service/gpu/runtime3/fft_thunk.cc b/xla/service/gpu/runtime3/fft_thunk.cc
index 711ae9907..37075a15f 100644
--- a/xla/service/gpu/runtime3/fft_thunk.cc
+++ b/xla/service/gpu/runtime3/fft_thunk.cc
@@ -65,6 +65,42 @@ std::string FftTypeToString(se::fft::Type type) {
   }
 }
 
+#if GOOGLE_SYCL
+template <typename T>
+class XLAFillCongugateSymmetry;
+
+template <typename T>
+void LaunchXLAFillConjugateSymmetry(se::Stream* stream, T* tensor_ptr,
+                                    T* tmp_tensor_ptr, int64_t stride,
+                                    int64_t tmp_stride, int64_t tensor_size) {
+  auto gpu_stream = stream_executor::gpu::AsGpuStreamValue(stream);
+  const auto num_elements = tensor_size / stride;
+  const int work_group_size =
+      (*gpu_stream)
+          .get_device()
+          .template get_info<sycl::info::device::max_work_group_size>();
+  const auto num_work_groups =
+      (num_elements + work_group_size - 1) / work_group_size;
+
+  gpu_stream->submit([&](sycl::handler& cgh) {
+    cgh.parallel_for<XLAFillCongugateSymmetry<T>>(
+        sycl::nd_range<1>(sycl::range<1>(num_work_groups * work_group_size),
+                          sycl::range<1>(work_group_size)),
+        [=](sycl::nd_item<1> item) {
+          auto idx = item.get_global_linear_id();
+          if (idx >= num_elements) {
+            return;
+          }
+
+          T* tensor_begin = tensor_ptr + idx * stride;
+          T* tmp_tensor_begin = tmp_tensor_ptr + idx * tmp_stride;
+          for (int i = 0; i < stride; ++i) {
+            tensor_begin[i] = tmp_tensor_begin[i];
+          }
+        });
+  });
+}
+#endif // GOOGLE_SYCL
 }  // namespace
 
 FftThunk::FftThunk(ThunkInfo thunk_info, FftType fft_type,
@@ -82,6 +118,283 @@ FftThunk::FftThunk(ThunkInfo thunk_info, FftType fft_type,
       input_shape_(input_shape),
       output_shape_(output_shape) {}
 
+#if GOOGLE_SYCL
+Status FftThunk::ExecuteOnStream(const ExecuteParams& params) {
+  se::Stream* stream = params.stream;
+  auto& buffer_allocations = *params.buffer_allocations;
+
+  VLOG(3) << "FFT type: " << FftTypeToString(fft_type_);
+  VLOG(3) << "Input shape: " << ShapeUtil::HumanStringWithLayout(input_shape_);
+  VLOG(3) << "Output shape: "
+          << ShapeUtil::HumanStringWithLayout(output_shape_);
+
+  se::OwningScratchAllocator<> scratch_allocator(
+      buffer_allocations.device_ordinal(),
+      buffer_allocations.memory_allocator());
+
+  switch (fft_type_) {
+    case se::fft::Type::kC2CForward:
+    case se::fft::Type::kC2CInverse: {
+      se::DeviceMemory<complex64> input_data(
+          buffer_allocations.GetDeviceAddress(input_buffer_));
+      se::DeviceMemory<complex64> output_data(
+          buffer_allocations.GetDeviceAddress(output_buffer_));
+      float* in = reinterpret_cast<float*>(
+          stream_executor::gpu::GpuMemoryMutable(&input_data));
+      float* out = reinterpret_cast<float*>(
+          stream_executor::gpu::GpuMemoryMutable(&output_data));
+      DoFFTInternal<oneapi::mkl::dft::precision::SINGLE,
+                    oneapi::mkl::dft::domain::COMPLEX, float>(
+          stream, in, out, scratch_allocator);
+      break;
+    }
+    case se::fft::Type::kZ2ZForward:
+    case se::fft::Type::kZ2ZInverse: {
+      se::DeviceMemory<complex128> input_data(
+          buffer_allocations.GetDeviceAddress(input_buffer_));
+      se::DeviceMemory<complex128> output_data(
+          buffer_allocations.GetDeviceAddress(output_buffer_));
+      double* in = reinterpret_cast<double*>(
+          stream_executor::gpu::GpuMemoryMutable(&input_data));
+      double* out = reinterpret_cast<double*>(
+          stream_executor::gpu::GpuMemoryMutable(&output_data));
+      DoFFTInternal<oneapi::mkl::dft::precision::DOUBLE,
+                    oneapi::mkl::dft::domain::COMPLEX, double>(
+          stream, in, out, scratch_allocator);
+      break;
+    }
+    case se::fft::Type::kR2C: {
+      se::DeviceMemory<float> input_data(
+          buffer_allocations.GetDeviceAddress(input_buffer_));
+      se::DeviceMemory<complex64> output_data(
+          buffer_allocations.GetDeviceAddress(output_buffer_));
+      float* in = reinterpret_cast<float*>(
+          stream_executor::gpu::GpuMemoryMutable(&input_data));
+      float* out = reinterpret_cast<float*>(
+          stream_executor::gpu::GpuMemoryMutable(&output_data));
+      DoFFTInternal<oneapi::mkl::dft::precision::SINGLE,
+                    oneapi::mkl::dft::domain::REAL, float>(stream, in, out,
+                                                           scratch_allocator);
+      break;
+    }
+    case se::fft::Type::kC2R: {
+      se::DeviceMemory<complex64> input_data(
+          buffer_allocations.GetDeviceAddress(input_buffer_));
+      se::DeviceMemory<float> output_data(
+          buffer_allocations.GetDeviceAddress(output_buffer_));
+      float* in = reinterpret_cast<float*>(
+          stream_executor::gpu::GpuMemoryMutable(&input_data));
+      float* out = reinterpret_cast<float*>(
+          stream_executor::gpu::GpuMemoryMutable(&output_data));
+      DoFFTInternal<oneapi::mkl::dft::precision::SINGLE,
+                    oneapi::mkl::dft::domain::REAL, float>(stream, in, out,
+                                                           scratch_allocator);
+      break;
+    }
+    case se::fft::Type::kD2Z: {
+      se::DeviceMemory<double> input_data(
+          buffer_allocations.GetDeviceAddress(input_buffer_));
+      se::DeviceMemory<complex128> output_data(
+          buffer_allocations.GetDeviceAddress(output_buffer_));
+      double* in = reinterpret_cast<double*>(
+          stream_executor::gpu::GpuMemoryMutable(&input_data));
+      double* out = reinterpret_cast<double*>(
+          stream_executor::gpu::GpuMemoryMutable(&output_data));
+      DoFFTInternal<oneapi::mkl::dft::precision::DOUBLE,
+                    oneapi::mkl::dft::domain::REAL, double>(stream, in, out,
+                                                            scratch_allocator);
+      break;
+    }
+    case se::fft::Type::kZ2D: {
+      se::DeviceMemory<complex128> input_data(
+          buffer_allocations.GetDeviceAddress(input_buffer_));
+      se::DeviceMemory<double> output_data(
+          buffer_allocations.GetDeviceAddress(output_buffer_));
+      double* in = reinterpret_cast<double*>(
+          stream_executor::gpu::GpuMemoryMutable(&input_data));
+      double* out = reinterpret_cast<double*>(
+          stream_executor::gpu::GpuMemoryMutable(&output_data));
+      DoFFTInternal<oneapi::mkl::dft::precision::DOUBLE,
+                    oneapi::mkl::dft::domain::REAL, double>(stream, in, out,
+                                                            scratch_allocator);
+      break;
+    }
+    default:
+      LOG(FATAL) << "unsupported fft type";
+  }
+  return OkStatus();
+}
+
+template <oneapi::mkl::dft::precision P, oneapi::mkl::dft::domain D, typename T>
+Status FftThunk::DoFFTInternal(se::Stream* stream, T* in_buffer, T* out_buffer,
+                               se::ScratchAllocator& allocator) {
+  const int64_t fft_rank = fft_length_.size();
+  CHECK_LE(fft_rank, 3);
+
+  int64_t fft_length[3];
+  int64_t input_embed[3];
+  const uint64_t input_stride = 1;
+  uint64_t input_distance = 1;
+  int64_t output_embed[3];
+  const uint64_t output_stride = 1;
+  uint64_t output_distance = 1;
+
+  for (int i = 0; i < fft_rank; ++i) {
+    auto dim_offset = input_shape_.dimensions_size() - fft_rank + i;
+    fft_length[i] = fft_length_[i];
+    input_embed[i] = input_shape_.dimensions(dim_offset);
+    input_distance *= input_shape_.dimensions(dim_offset);
+    output_embed[i] = output_shape_.dimensions(dim_offset);
+    output_distance *= output_shape_.dimensions(dim_offset);
+  }
+
+  int batch_size = 1;
+  for (int i = 0; i < input_shape_.dimensions_size() - fft_rank; ++i) {
+    batch_size *= input_shape_.dimensions(i);
+  }
+  bool is_input_complex = false;
+  bool is_output_complex = false;
+  bool is_forward = false;
+  bool is_real = false;
+
+  switch (fft_type_) {
+    case se::fft::Type::kC2CForward:
+    case se::fft::Type::kZ2ZForward: {
+      is_forward = true;
+      is_real = false;
+      break;
+    }
+    case se::fft::Type::kR2C:
+    case se::fft::Type::kD2Z: {
+      is_forward = true;
+      is_real = true;
+      break;
+    }
+    case se::fft::Type::kC2CInverse:
+    case se::fft::Type::kZ2ZInverse: {
+      is_forward = false;
+      is_real = false;
+      break;
+    }
+    case se::fft::Type::kC2R:
+    case se::fft::Type::kZ2D: {
+      is_forward = false;
+      is_real = true;
+      break;
+    }
+    default:
+      LOG(FATAL) << "unsupported fft type";
+  }
+
+  double scale_factor;
+  std::vector<std::int64_t> dims_vec(fft_rank, 0);
+  switch (fft_type_) {
+    case se::fft::Type::kC2CForward:
+    case se::fft::Type::kZ2ZForward:
+    case se::fft::Type::kR2C:
+    case se::fft::Type::kD2Z: {
+      scale_factor = 1.0 / input_distance;
+      for (int i = 0; i < fft_rank; ++i) {
+        dims_vec[i] = input_embed[i];
+      }
+      break;
+    }
+    case se::fft::Type::kC2CInverse:
+    case se::fft::Type::kZ2ZInverse:
+    case se::fft::Type::kC2R:
+    case se::fft::Type::kZ2D: {
+      scale_factor = 1.0 / output_distance;
+      for (int i = 0; i < fft_rank; ++i) {
+        dims_vec[i] = output_embed[i];
+      }
+      break;
+    }
+    default:
+      LOG(FATAL) << "unsupported fft type";
+  }
+
+  oneapi::mkl::dft::descriptor<P, D> desc(dims_vec);
+  desc.set_value(oneapi::mkl::dft::config_param::PLACEMENT, DFTI_NOT_INPLACE);
+  desc.set_value(oneapi::mkl::dft::config_param::NUMBER_OF_TRANSFORMS,
+                 batch_size);
+
+  if (is_real) {
+    std::vector<int64_t> mkl_istrides(1 + fft_rank, 0);
+    std::vector<int64_t> mkl_ostrides(1 + fft_rank, 0);
+    desc.set_value(oneapi::mkl::dft::config_param::CONJUGATE_EVEN_STORAGE,
+                   DFTI_COMPLEX_COMPLEX);
+
+    int64_t tmp_istride = 1, tmp_ostride = 1;
+    for (int64_t i = fft_rank; i > 0; --i) {
+      if (is_input_complex && !is_output_complex) {
+        if (i == (fft_rank - 1)) {
+          tmp_istride = input_embed[i];
+          tmp_ostride = (output_embed[i] / 2 + 1) * 2;
+        }
+      }
+      mkl_istrides[i] = tmp_istride;
+      mkl_ostrides[i] = tmp_ostride;
+      tmp_istride *= input_embed[i - 1];
+      tmp_ostride *= output_embed[i - 1];
+    }
+    desc.set_value(oneapi::mkl::dft::config_param::INPUT_STRIDES,
+                   mkl_istrides.data());
+    desc.set_value(oneapi::mkl::dft::config_param::OUTPUT_STRIDES,
+                   mkl_ostrides.data());
+    if (is_forward) {
+      desc.set_value(oneapi::mkl::dft::config_param::FWD_DISTANCE, tmp_istride);
+      desc.set_value(oneapi::mkl::dft::config_param::BWD_DISTANCE, tmp_ostride);
+    } else {
+      desc.set_value(oneapi::mkl::dft::config_param::FWD_DISTANCE, tmp_ostride);
+      desc.set_value(oneapi::mkl::dft::config_param::BWD_DISTANCE, tmp_istride);
+      desc.set_value(oneapi::mkl::dft::config_param::BACKWARD_SCALE,
+                     static_cast<T>(scale_factor));
+    }
+  } else {
+    desc.set_value(oneapi::mkl::dft::config_param::FWD_DISTANCE,
+                   input_distance);
+    desc.set_value(oneapi::mkl::dft::config_param::BWD_DISTANCE,
+                   output_distance);
+    desc.set_value(oneapi::mkl::dft::config_param::BACKWARD_SCALE,
+                   static_cast<T>(scale_factor));
+  }
+
+  desc.commit(*stream_executor::gpu::AsGpuStreamValue(stream));
+
+  sycl::event fft_event;
+  if (is_forward) {
+    fft_event = oneapi::mkl::dft::compute_forward(desc, in_buffer, out_buffer);
+  } else {
+    if (is_input_complex && !is_output_complex && fft_rank > 1) {
+      int workspace_size = 1;
+      for (int i = 0; i < output_shape_.dimensions_size() - 1; ++i) {
+        workspace_size *= output_shape_.dimensions(i);
+      }
+      int out_size = workspace_size * output_shape_.dimensions(
+                                          output_shape_.dimensions_size() - 1);
+      workspace_size =
+          workspace_size * (output_embed[fft_rank - 1] / 2 + 1) * 2;
+
+      TF_ASSIGN_OR_RETURN(stream_executor::DeviceMemory<tsl::uint8> workspace_bytes,
+                          allocator.AllocateBytes(workspace_size * sizeof(T)));
+      void* workspace = static_cast<void*>(workspace_bytes.opaque());
+      T* tmp_out_buffer = reinterpret_cast<T*>(workspace);
+
+      fft_event =
+          oneapi::mkl::dft::compute_backward(desc, in_buffer, tmp_out_buffer);
+      LaunchXLAFillConjugateSymmetry<T>(
+          stream, out_buffer, tmp_out_buffer,
+          output_shape_.dimensions(output_shape_.dimensions_size() - 1),
+          (output_embed[fft_rank - 1] / 2 + 1) * 2, out_size);
+    } else {
+      fft_event =
+          oneapi::mkl::dft::compute_backward(desc, in_buffer, out_buffer);
+    }
+  }
+  fft_event.wait();
+  return OkStatus();
+}
+#else // GOOGLE_SYCL
 Status FftThunk::ExecuteOnStream(const ExecuteParams& params) {
   auto& buffer_allocations = *params.buffer_allocations;
 
@@ -242,6 +555,6 @@ Status RunFft(se::DeviceMemoryBase input, const Shape& input_shape,
   return InternalError("Unable to launch fft with type %s",
                        FftTypeToString(fft_type));
 }
-
+#endif // GOOGLE_SYCL
 }  // namespace gpu
 }  // namespace xla
diff --git a/xla/service/gpu/runtime3/fft_thunk.h b/xla/service/gpu/runtime3/fft_thunk.h
index 4e0de39e1..14ae7fdf5 100644
--- a/xla/service/gpu/runtime3/fft_thunk.h
+++ b/xla/service/gpu/runtime3/fft_thunk.h
@@ -28,6 +28,12 @@ limitations under the License.
 #include "xla/xla_data.pb.h"
 #include "tsl/platform/status.h"
 
+#if GOOGLE_SYCL
+#include "xla/service/gpu/cusolver_context.h"
+#include "xla/stream_executor/gpu/gpu_stream.h"
+#include "xla/stream_executor/gpu/gpu_helpers.h"
+#endif
+
 namespace xla {
 namespace gpu {
 
@@ -76,6 +82,13 @@ class FftThunk : public Thunk {
   Status ExecuteOnStream(const ExecuteParams& params) override;
 
  private:
+#if GOOGLE_SYCL
+  template <oneapi::mkl::dft::precision P, oneapi::mkl::dft::domain D,
+            typename T>
+  Status DoFFTInternal(se::Stream* stream, T* in_buffer, T* out_buffer,
+                       se::ScratchAllocator& allocator);
+#endif  // GOOGLE_SYCL
+
   const se::fft::Type fft_type_;
   const std::vector<int64_t> fft_length_;
 
diff --git a/xla/service/gpu/runtime3/triangular_solve_thunk.h b/xla/service/gpu/runtime3/triangular_solve_thunk.h
index 8f582db16..4fc7bad4b 100644
--- a/xla/service/gpu/runtime3/triangular_solve_thunk.h
+++ b/xla/service/gpu/runtime3/triangular_solve_thunk.h
@@ -29,6 +29,11 @@ limitations under the License.
 #include "xla/xla_data.pb.h"
 #include "tsl/platform/status.h"
 
+#if GOOGLE_SYCL
+#include "xla/service/gpu/cusolver_context.h"
+#include "xla/stream_executor/gpu/gpu_stream.h"
+#endif
+
 namespace xla {
 namespace gpu {
 
@@ -71,7 +76,6 @@ class TriangularSolveThunk : public Thunk {
   const int64_t a_batch_stride_;
   const int64_t b_batch_stride_;
 };
-
 Status RunTriangularSolve(se::DeviceMemoryBase a_data,
                           se::DeviceMemoryBase b_data,
                           se::DeviceMemoryBase temp_data,
@@ -81,7 +85,6 @@ Status RunTriangularSolve(se::DeviceMemoryBase a_data,
                           int64_t batch_size, int64_t m, int64_t n,
                           int64_t a_batch_stride, int64_t b_batch_stride,
                           se::Stream* stream);
-
 }  // namespace gpu
 }  // namespace xla
 
diff --git a/xla/service/gpu/stream_executor_util.cc b/xla/service/gpu/stream_executor_util.cc
index 7904e6d5e..1d8305c04 100644
--- a/xla/service/gpu/stream_executor_util.cc
+++ b/xla/service/gpu/stream_executor_util.cc
@@ -139,6 +139,13 @@ StreamExecutorConvLayoutsToXlaLayouts(const ConvolutionDimensionNumbers& dnums,
                            dnums.kernel_spatial_dimensions().end());
       filter_layout.push_back(dnums.kernel_input_feature_dimension());
       break;
+    case FilterLayout::kYXInputOutput:  // HWIO
+      filter_layout.insert(filter_layout.end(),
+                           dnums.kernel_spatial_dimensions().begin(),
+                           dnums.kernel_spatial_dimensions().end());
+      filter_layout.push_back(dnums.kernel_input_feature_dimension());
+      filter_layout.push_back(dnums.kernel_output_feature_dimension());
+      break;
     default:
       return InternalError("Invalid filter layout %s for conv with dnums %s,",
                            FilterLayoutString(filter),
@@ -176,7 +183,7 @@ XlaConvShapesToStreamExecutorLayouts(const ConvolutionDimensionNumbers& dnums,
   Layout nhwc_input, nhwc_filter, nhwc_output;
   std::tie(nhwc_input, nhwc_filter, nhwc_output) =
       StreamExecutorConvLayoutsToXlaLayouts(dnums, DataLayout::kBatchYXDepth,
-                                            FilterLayout::kOutputYXInput,
+                                            FilterLayout::kYXInputOutput,
                                             DataLayout::kBatchYXDepth)
           .value();
 
@@ -225,7 +232,7 @@ XlaConvShapesToStreamExecutorLayouts(const ConvolutionDimensionNumbers& dnums,
           ConvolutionDimensionNumbersToString(dnums), vect_size);
     }
   } else if (LayoutUtil::Equal(filter.layout(), nhwc_filter)) {
-    filter_layout = FilterLayout::kOutputYXInput;
+    filter_layout = FilterLayout::kYXInputOutput;
   } else {
     return InternalError(
         "Invalid filter layout %s for conv with dnums %s, expected one of (%s, "
@@ -328,7 +335,8 @@ StatusOr<std::unique_ptr<se::KernelBase>> CreateKernel(
 
   if (!cubin_data.empty()) {
     loader_spec.AddCudaCubinInMemory(
-        reinterpret_cast<const char*>(cubin_data.data()), kernel_name);
+        reinterpret_cast<const char*>(cubin_data.data()), cubin_data.size(),
+        kernel_name);
   }
 
   auto kernel_base = std::make_unique<se::KernelBase>(stream_exec);
diff --git a/xla/service/gpu/target_constants.h b/xla/service/gpu/target_constants.h
index 92f31a6c1..aead65821 100644
--- a/xla/service/gpu/target_constants.h
+++ b/xla/service/gpu/target_constants.h
@@ -54,6 +54,24 @@ inline const char* DataLayout() {
 
 }  // namespace amdgpu
 
+namespace spir {
+// The triple that represents our target on SPIR backend.
+inline const char* TargetTriple() {
+  static constexpr char kTargetTriple[] = "spir64-unknown-unknown";
+  return kTargetTriple;
+}
+
+// The data layout of the emitted module.
+inline const char* DataLayout() {
+  static constexpr char kDataLayout[] =
+      "e-p:64:64:64-i1:8:8-i8:8:8-i16:16:16-i32:32:32-i64:64:64-f32:"
+      "32:32-f64:64:64-v16:16:16-v24:32:32-v32:32:32-v48:64:64-v64:64:64-v96:"
+      "128:128-v128:128:128-v192:256:256-v256:256:256-v512:512:512-v1024:1024:"
+      "1024";
+  return kDataLayout;
+}
+}  // namespace spir
+
 }  // namespace gpu
 }  // namespace xla
 
diff --git a/xla/service/gpu/target_util.cc b/xla/service/gpu/target_util.cc
index 12b12b3f1..c48318ad8 100644
--- a/xla/service/gpu/target_util.cc
+++ b/xla/service/gpu/target_util.cc
@@ -23,12 +23,12 @@ limitations under the License.
 #include "llvm/IR/IntrinsicsAMDGPU.h"
 #include "llvm/IR/IntrinsicsNVPTX.h"
 #include "llvm/IR/MDBuilder.h"
+#include "tsl/platform/logging.h"
 #include "xla/hlo/ir/hlo_opcode.h"
 #include "xla/primitive_util.h"
 #include "xla/service/llvm_ir/llvm_type_conversion_util.h"
 #include "xla/service/llvm_ir/llvm_util.h"
 #include "xla/status.h"
-#include "tsl/platform/logging.h"
 
 namespace xla {
 namespace gpu {
@@ -45,6 +45,10 @@ struct TargetIntrinsics {
   std::variant<llvm::Intrinsic::ID,
                std::function<llvm::CallInst*(llvm::IRBuilder<>*)>>
       amdgpu_intrinsic_or_function;
+  // SYCL: Target for SPIRV.
+  absl::variant<llvm::Intrinsic::ID,
+                std::function<llvm::CallInst*(llvm::IRBuilder<>*)>>
+      spir_intrinsic_or_function;
 };
 
 // Gets the llvm intrinsic ids on different platforms (NVPTX, AMDGPU)
@@ -52,32 +56,82 @@ struct TargetIntrinsics {
 struct TargetIntrinsics GetIntrinsic(TargetIntrinsicID intrin) {
   switch (intrin) {
     case TargetIntrinsicID::kThreadIdx: {
-      return {llvm::Intrinsic::nvvm_read_ptx_sreg_tid_x,
-              llvm::Intrinsic::amdgcn_workitem_id_x};
+      return {
+          llvm::Intrinsic::nvvm_read_ptx_sreg_tid_x,
+          llvm::Intrinsic::amdgcn_workitem_id_x,
+          [](llvm::IRBuilder<>* b_) -> llvm::CallInst* {
+            return EmitDeviceFunctionCall(
+                "_Z32__spirv_BuiltInLocalInvocationIdi", {b_->getInt32(0)},
+                {U32}, U64, {b_->getContext()}, b_);
+          },
+      };
     }
     case TargetIntrinsicID::kThreadIdy: {
-      return {llvm::Intrinsic::nvvm_read_ptx_sreg_tid_y,
-              llvm::Intrinsic::amdgcn_workitem_id_y};
+      return {
+          llvm::Intrinsic::nvvm_read_ptx_sreg_tid_y,
+          llvm::Intrinsic::amdgcn_workitem_id_y,
+          [](llvm::IRBuilder<>* b_) -> llvm::CallInst* {
+            return EmitDeviceFunctionCall(
+                "_Z32__spirv_BuiltInLocalInvocationIdi", {b_->getInt32(1)},
+                {U32}, U64, {b_->getContext()}, b_);
+          },
+      };
     }
     case TargetIntrinsicID::kThreadIdz: {
-      return {llvm::Intrinsic::nvvm_read_ptx_sreg_tid_z,
-              llvm::Intrinsic::amdgcn_workitem_id_z};
+      return {
+          llvm::Intrinsic::nvvm_read_ptx_sreg_tid_z,
+          llvm::Intrinsic::amdgcn_workitem_id_z,
+          [](llvm::IRBuilder<>* b_) -> llvm::CallInst* {
+            return EmitDeviceFunctionCall(
+                "_Z32__spirv_BuiltInLocalInvocationIdi", {b_->getInt32(2)},
+                {U32}, U64, {b_->getContext()}, b_);
+          },
+      };
     }
     case TargetIntrinsicID::kBlockIdx: {
-      return {llvm::Intrinsic::nvvm_read_ptx_sreg_ctaid_x,
-              llvm::Intrinsic::amdgcn_workgroup_id_x};
+      return {
+          llvm::Intrinsic::nvvm_read_ptx_sreg_ctaid_x,
+          llvm::Intrinsic::amdgcn_workgroup_id_x,
+          [](llvm::IRBuilder<>* b_) -> llvm::CallInst* {
+            return EmitDeviceFunctionCall("_Z26__spirv_BuiltInWorkgroupIdi",
+                                          {b_->getInt32(0)}, {U32}, U64,
+                                          {b_->getContext()}, b_);
+          },
+      };
     }
     case TargetIntrinsicID::kBlockIdy: {
-      return {llvm::Intrinsic::nvvm_read_ptx_sreg_ctaid_y,
-              llvm::Intrinsic::amdgcn_workgroup_id_y};
+      return {
+          llvm::Intrinsic::nvvm_read_ptx_sreg_ctaid_y,
+          llvm::Intrinsic::amdgcn_workgroup_id_y,
+          [](llvm::IRBuilder<>* b_) -> llvm::CallInst* {
+            return EmitDeviceFunctionCall("_Z26__spirv_BuiltInWorkgroupIdi",
+                                          {b_->getInt32(1)}, {U32}, U64,
+                                          {b_->getContext()}, b_);
+          },
+      };
     }
     case TargetIntrinsicID::kBlockIdz: {
-      return {llvm::Intrinsic::nvvm_read_ptx_sreg_ctaid_z,
-              llvm::Intrinsic::amdgcn_workgroup_id_z};
+      return {
+          llvm::Intrinsic::nvvm_read_ptx_sreg_ctaid_z,
+          llvm::Intrinsic::amdgcn_workgroup_id_z,
+          [](llvm::IRBuilder<>* b_) -> llvm::CallInst* {
+            return EmitDeviceFunctionCall("_Z26__spirv_BuiltInWorkgroupIdi",
+                                          {b_->getInt32(2)}, {U32}, U64,
+                                          {b_->getContext()}, b_);
+          },
+      };
     }
     case TargetIntrinsicID::kBarrierId: {
-      return {llvm::Intrinsic::nvvm_barrier0,
-              llvm::Intrinsic::amdgcn_s_barrier};
+      return {llvm::Intrinsic::nvvm_barrier0, llvm::Intrinsic::amdgcn_s_barrier,
+              [](llvm::IRBuilder<>* b_) -> llvm::CallInst* {
+                return EmitDeviceFunctionCall(
+                    "_Z22__spirv_ControlBarrierjjj",
+                    {b_->getInt32(2), b_->getInt32(2), b_->getInt32(272)},
+                    {U32, U32, U32}, U32,
+                    llvm::AttrBuilder(b_->getContext())
+                        .addAttribute(llvm::Attribute::Convergent),
+                    b_);
+              }};
     }
     case TargetIntrinsicID::kBlockDimx: {
       return {llvm::Intrinsic::nvvm_read_ptx_sreg_ntid_x,
@@ -85,6 +139,11 @@ struct TargetIntrinsics GetIntrinsic(TargetIntrinsicID intrin) {
                 return EmitDeviceFunctionCall("__ockl_get_local_size",
                                               {b_->getInt32(0)}, {U32}, U64,
                                               {b_->getContext()}, b_);
+              },
+              [](llvm::IRBuilder<>* b_) -> llvm::CallInst* {
+                return EmitDeviceFunctionCall(
+                    "_Z28__spirv_BuiltInWorkgroupSizei", {b_->getInt32(0)},
+                    {U32}, U64, {b_->getContext()}, b_);
               }};
     }
     case TargetIntrinsicID::kBlockDimy: {
@@ -93,6 +152,11 @@ struct TargetIntrinsics GetIntrinsic(TargetIntrinsicID intrin) {
                 return EmitDeviceFunctionCall("__ockl_get_local_size",
                                               {b_->getInt32(1)}, {U32}, U64,
                                               {b_->getContext()}, b_);
+              },
+              [](llvm::IRBuilder<>* b_) -> llvm::CallInst* {
+                return EmitDeviceFunctionCall(
+                    "_Z28__spirv_BuiltInWorkgroupSizei", {b_->getInt32(1)},
+                    {U32}, U64, {b_->getContext()}, b_);
               }};
     }
     case TargetIntrinsicID::kBlockDimz: {
@@ -101,11 +165,26 @@ struct TargetIntrinsics GetIntrinsic(TargetIntrinsicID intrin) {
                 return EmitDeviceFunctionCall("__ockl_get_local_size",
                                               {b_->getInt32(2)}, {U32}, U64,
                                               {b_->getContext()}, b_);
+              },
+              [](llvm::IRBuilder<>* b_) -> llvm::CallInst* {
+                return EmitDeviceFunctionCall(
+                    "_Z28__spirv_BuiltInWorkgroupSizei", {b_->getInt32(2)},
+                    {U32}, U64, {b_->getContext()}, b_);
               }};
     }
     case TargetIntrinsicID::kGroupBarrierId: {
       return {llvm::Intrinsic::nvvm_bar_warp_sync,
-              llvm::Intrinsic::amdgcn_wave_barrier};
+              llvm::Intrinsic::amdgcn_wave_barrier,
+              [](llvm::IRBuilder<>* b_) -> llvm::CallInst* {
+                // TODO: Fix it.
+                return EmitDeviceFunctionCall(
+                    "_Z22__spirv_ControlBarrierjjj",
+                    {b_->getInt32(2), b_->getInt32(2), b_->getInt32(272)},
+                    {U32, U32, U32}, U32,
+                    llvm::AttrBuilder(b_->getContext())
+                        .addAttribute(llvm::Attribute::Convergent),
+                    b_);
+              }};
     }
   }
 }
@@ -114,6 +193,7 @@ struct TargetIntrinsics GetIntrinsic(TargetIntrinsicID intrin) {
 struct TargetDeviceFunction {
   const std::string nvptx_root;
   const std::string amdgpu_root;
+  const std::string spir_root;
 };
 
 // Gets the device function name on different platforms (NVPTX, AMDGPU)
@@ -122,49 +202,49 @@ struct TargetDeviceFunction GetDeviceFunctionRoot(
     TargetDeviceFunctionID func_id) {
   switch (func_id) {
     case TargetDeviceFunctionID::kAtan2: {
-      return {"__nv_atan2", "__ocml_atan2"};
+      return {"__nv_atan2", "__ocml_atan2", "_Z17__spirv_ocl_atan2"};
     }
     case TargetDeviceFunctionID::kCos: {
-      return {"__nv_cos", "__ocml_cos"};
+      return {"__nv_cos", "__ocml_cos", "_Z15__spirv_ocl_cos"};
     }
     case TargetDeviceFunctionID::kExp: {
-      return {"__nv_exp", "__ocml_exp"};
+      return {"__nv_exp", "__ocml_exp", "_Z15__spirv_ocl_exp"};
     }
     case TargetDeviceFunctionID::kExpm1: {
-      return {"__nv_expm1", "__ocml_expm1"};
+      return {"__nv_expm1", "__ocml_expm1", "_Z17__spirv_ocl_expm1"};
     }
     case TargetDeviceFunctionID::kFmod: {
-      return {"__nv_fmod", "__ocml_fmod"};
+      return {"__nv_fmod", "__ocml_fmod", "_Z16__spirv_ocl_fmod"};
     }
     case TargetDeviceFunctionID::kHypot: {
-      return {"__nv_hypot", "__ocml_hypot"};
+      return {"__nv_hypot", "__ocml_hypot", "_Z17__spirv_ocl_hypot"};
     }
     case TargetDeviceFunctionID::kLog: {
-      return {"__nv_log", "__ocml_log"};
+      return {"__nv_log", "__ocml_log", "_Z15__spirv_ocl_log"};
     }
     case TargetDeviceFunctionID::kLog1p: {
-      return {"__nv_log1p", "__ocml_log1p"};
+      return {"__nv_log1p", "__ocml_log1p", "_Z17__spirv_ocl_log1p"};
     }
     case TargetDeviceFunctionID::kPow: {
-      return {"__nv_pow", "__ocml_pow"};
+      return {"__nv_pow", "__ocml_pow", "_Z15__spirv_ocl_pow"};
     }
     case TargetDeviceFunctionID::kRsqrt: {
-      return {"__nv_rsqrt", "__ocml_rsqrt"};
+      return {"__nv_rsqrt", "__ocml_rsqrt", "_Z17__spirv_ocl_rsqrt"};
     }
     case TargetDeviceFunctionID::kSin: {
-      return {"__nv_sin", "__ocml_sin"};
+      return {"__nv_sin", "__ocml_sin", "_Z15__spirv_ocl_sin"};
     }
     case TargetDeviceFunctionID::kSqrt: {
-      return {"__nv_sqrt", "__ocml_sqrt"};
+      return {"__nv_sqrt", "__ocml_sqrt", "_Z16__spirv_ocl_sqrt"};
     }
     case TargetDeviceFunctionID::kTan: {
-      return {"__nv_tan", "__ocml_tan"};
+      return {"__nv_tan", "__ocml_tan", "_Z15__spirv_ocl_tan"};
     }
     case TargetDeviceFunctionID::kTanh: {
-      return {"__nv_tanh", "__ocml_tanh"};
+      return {"__nv_tanh", "__ocml_tanh", "_Z16__spirv_ocl_tanh"};
     }
     case TargetDeviceFunctionID::kCbrt: {
-      return {"__nv_cbrt", "__ocml_cbrt"};
+      return {"__nv_cbrt", "__ocml_cbrt", "_Z16__spirv_ocl_cbrt"};
     }
   }
 }
@@ -231,6 +311,24 @@ std::string ObtainDeviceFunctionName(TargetDeviceFunctionID func_id,
     } else {
       LOG(FATAL) << "Unexpected type while getting device function name.";
     }
+  } else if (target_triple.isSPIR()) {
+    if (output_type == F32) {
+      if (gpu_root_names.spir_root == "_Z17__spirv_ocl_hypot" ||
+          gpu_root_names.spir_root == "_Z15__spirv_ocl_pow" ||
+          gpu_root_names.spir_root == "_Z17__spirv_ocl_atan2" ||
+          gpu_root_names.spir_root == "_Z16__spirv_ocl_fmod")
+        return StrCat(gpu_root_names.spir_root, "ff");
+      return StrCat(gpu_root_names.spir_root, "f");
+    } else if (output_type == F64) {
+      if (gpu_root_names.spir_root == "_Z17__spirv_ocl_hypot" ||
+          gpu_root_names.spir_root == "_Z15__spirv_ocl_pow" ||
+          gpu_root_names.spir_root == "_Z17__spirv_ocl_atan2" ||
+          gpu_root_names.spir_root == "_Z16__spirv_ocl_fmod")
+        return StrCat(gpu_root_names.spir_root, "dd");
+      return StrCat(gpu_root_names.spir_root, "d");
+    } else {
+      LOG(FATAL) << "Unexpected type while getting device function name.";
+    }
   } else {
     LOG(FATAL) << "Invalid triple " << target_triple.str();
   }
@@ -243,6 +341,7 @@ llvm::CallInst* EmitDeviceFunctionCall(
     absl::string_view name) {
   std::vector<llvm::Type*> ir_input_types;
   llvm::Module* module = b->GetInsertBlock()->getModule();
+  llvm::Triple target_triple = llvm::Triple(module->getTargetTriple());
   for (PrimitiveType input_type : input_types) {
     ir_input_types.push_back(
         llvm_ir::PrimitiveTypeToIrType(input_type, module));
@@ -260,6 +359,9 @@ llvm::CallInst* EmitDeviceFunctionCall(
           .getCallee());
 
   callee->addFnAttrs(attributes);
+  // SYCL: SPIR function
+  if (target_triple.isSPIR())
+    callee->setCallingConv(llvm::CallingConv::SPIR_FUNC);
 
   return b->CreateCall(callee, llvm_ir::AsArrayRef(operands), name.data());
 }
@@ -285,6 +387,18 @@ llvm::CallInst* EmitCallToTargetIntrinsic(
               &gpu_intrinsic_id.amdgpu_intrinsic_or_function);
       return (*builder_func)(b);
     }
+  } else if (target_triple.isSPIR()) {
+    llvm::Intrinsic::ID* llvm_intrinsic_id_ptr =
+        absl::get_if<llvm::Intrinsic::ID>(
+            &gpu_intrinsic_id.spir_intrinsic_or_function);
+    if (llvm_intrinsic_id_ptr) {
+      llvm_intrinsic_id = *llvm_intrinsic_id_ptr;
+    } else {
+      std::function<llvm::CallInst*(llvm::IRBuilder<>*)>* builder_func =
+          absl::get_if<std::function<llvm::CallInst*(llvm::IRBuilder<>*)>>(
+              &gpu_intrinsic_id.spir_intrinsic_or_function);
+      return (*builder_func)(b);
+    }
   } else {
     LOG(FATAL) << "Invalid triple " << target_triple.str();
   }
@@ -312,6 +426,9 @@ void AnnotateFunctionAsGpuKernel(llvm::Module* module, llvm::Function* func,
     // Attach information so AMDGPU can recognize function as a AMDGPU kernel.
     func->setCallingConv(llvm::CallingConv::AMDGPU_KERNEL);
     func->addFnAttr("amdgpu-flat-work-group-size", "1, 1024");
+  } else if (target_triple.isSPIR()) {
+    // Attach information so that it can be recognized as a SPIR kernel.
+    func->setCallingConv(llvm::CallingConv::SPIR_KERNEL);
   } else {
     LOG(FATAL) << "Invalid triple " << target_triple.str();
   }
diff --git a/xla/service/llvm_ir/fused_ir_emitter.cc b/xla/service/llvm_ir/fused_ir_emitter.cc
index 006020e41..de5c3fc23 100644
--- a/xla/service/llvm_ir/fused_ir_emitter.cc
+++ b/xla/service/llvm_ir/fused_ir_emitter.cc
@@ -23,6 +23,7 @@ limitations under the License.
 #include "llvm/IR/IRBuilder.h"
 #include "llvm/IR/Module.h"
 #include "llvm/IR/Value.h"
+#include "llvm/TargetParser/Triple.h"
 #include "xla/hlo/ir/hlo_computation.h"
 #include "xla/hlo/ir/hlo_instruction.h"
 #include "xla/hlo/ir/hlo_opcode.h"
@@ -97,6 +98,8 @@ FusedIrEmitter::IndexedGenerator FusedIrEmitter::HandleConstant(
 
   llvm::Constant* initializer =
       llvm_ir::ConvertLiteralToIrConstant(constant.literal(), module);
+  // SYCL: Hardcode to global addrspace
+  int addrspace = llvm::Triple(module->getTargetTriple()).isSPIR() ? 1 : 0;
   llvm::GlobalVariable* global = new llvm::GlobalVariable(
       *b->GetInsertBlock()->getModule(), initializer->getType(),
       /*isConstant=*/true,
@@ -104,7 +107,7 @@ FusedIrEmitter::IndexedGenerator FusedIrEmitter::HandleConstant(
       /*Initializer=*/initializer,
       /*Name=*/"", /*InsertBefore=*/nullptr,
       /*TLMode=*/llvm::GlobalValue::NotThreadLocal,
-      /*AddressSpace=*/0,
+      /*AddressSpace=*/addrspace,
       /*isExternallyInitialized=*/false);
   global->setUnnamedAddr(llvm::GlobalVariable::UnnamedAddr::Global);
 
diff --git a/xla/service/llvm_ir/ir_array.cc b/xla/service/llvm_ir/ir_array.cc
index fa6ca3fff..82230ee27 100644
--- a/xla/service/llvm_ir/ir_array.cc
+++ b/xla/service/llvm_ir/ir_array.cc
@@ -25,6 +25,7 @@ limitations under the License.
 #include "llvm/IR/Constants.h"
 #include "llvm/IR/Instructions.h"
 #include "llvm/IR/Value.h"
+#include "tsl/platform/logging.h"
 #include "xla/layout_util.h"
 #include "xla/permutation_util.h"
 #include "xla/primitive_util.h"
@@ -34,7 +35,6 @@ limitations under the License.
 #include "xla/statusor.h"
 #include "xla/util.h"
 #include "xla/xla_data.pb.h"
-#include "tsl/platform/logging.h"
 
 namespace xla {
 namespace llvm_ir {
@@ -548,9 +548,23 @@ llvm::Value* IrArray::EmitLinearArrayElementAddress(
   llvm::Module* module = b->GetInsertBlock()->getParent()->getParent();
   llvm::Type* type = PrimitiveTypeToIrType(shape_.element_type(), module);
   if (!primitive_util::Is4BitType(shape_.element_type())) {
-    return b->CreateInBoundsGEP(
-        type, b->CreateBitCast(base_ptr_, type->getPointerTo()), index.linear(),
-        llvm_ir::AsStringRef(name));
+    auto linear_index = llvm::dyn_cast<llvm::BinaryOperator>(index.linear());
+    // only separate const offset when having add
+    if (linear_index && (linear_index->getOpcode() == llvm::Instruction::Add)) {
+      llvm::Value* index_operand_0 = linear_index->getOperand(0);
+      // constant index
+      llvm::Value* index_operand_1 = linear_index->getOperand(1);
+      llvm::Value* ptr_address =
+          b->CreateGEP(type, b->CreateBitCast(base_ptr_, type->getPointerTo()),
+                       index_operand_0, "");
+
+      return b->CreateInBoundsGEP(type, ptr_address, index_operand_1,
+                                  llvm_ir::AsStringRef(name));
+    } else {
+      return b->CreateInBoundsGEP(
+          type, b->CreateBitCast(base_ptr_, type->getPointerTo()),
+          index.linear(), llvm_ir::AsStringRef(name));
+    }
   }
 
   // Handle int4 case by dividing index by 2. Int4 arrays are represented in
diff --git a/xla/service/llvm_ir/llvm_util.cc b/xla/service/llvm_ir/llvm_util.cc
index b3f6e7aac..a8af8464c 100644
--- a/xla/service/llvm_ir/llvm_util.cc
+++ b/xla/service/llvm_ir/llvm_util.cc
@@ -50,6 +50,7 @@ limitations under the License.
 #include "llvm/Support/Casting.h"
 #include "llvm/Support/CodeGen.h"
 #include "llvm/Support/raw_ostream.h"
+#include "llvm/TargetParser/Triple.h"
 #include "llvm/Transforms/Utils/Cloning.h"
 #include "mlir/IR/BuiltinOps.h"  // from @llvm-project
 #include "mlir/IR/Location.h"  // from @llvm-project
@@ -380,8 +381,13 @@ llvm::AllocaInst* EmitAllocaAtFunctionEntryWithCount(llvm::Type* type,
   llvm::Function* function = b->GetInsertBlock()->getParent();
   b->SetInsertPoint(&function->getEntryBlock(),
                     function->getEntryBlock().getFirstInsertionPt());
+  // SYCL: Fix atomic issue by allocating on private addrspace
+  int addrspace =
+      llvm::Triple(b->GetInsertBlock()->getModule()->getTargetTriple()).isSPIR()
+          ? 5
+          : 0;
   llvm::AllocaInst* alloca =
-      b->CreateAlloca(type, element_count, AsStringRef(name));
+      b->CreateAlloca(type, addrspace, element_count, AsStringRef(name));
   if (alignment != 0) {
     alloca->setAlignment(llvm::Align(alignment));
   }
@@ -500,6 +506,7 @@ void SetDereferenceableMetadataForLoad(llvm::LoadInst* load,
 
 llvm::Instruction* AddRangeMetadata(int32_t lower, int32_t upper,
                                     llvm::Instruction* inst) {
+  /* SYCL: This range is for NVPTX target only.
   llvm::LLVMContext& context = inst->getParent()->getContext();
   llvm::IntegerType* i32 = llvm::Type::getInt32Ty(context);
   inst->setMetadata(
@@ -508,6 +515,7 @@ llvm::Instruction* AddRangeMetadata(int32_t lower, int32_t upper,
           context,
           {llvm::ConstantAsMetadata::get(llvm::ConstantInt::get(i32, lower)),
            llvm::ConstantAsMetadata::get(llvm::ConstantInt::get(i32, upper))}));
+  */
   return inst;
 }
 
diff --git a/xla/stream_executor/build_defs.bzl b/xla/stream_executor/build_defs.bzl
index 960502a5c..62557b984 100644
--- a/xla/stream_executor/build_defs.bzl
+++ b/xla/stream_executor/build_defs.bzl
@@ -2,6 +2,7 @@
 
 load("@local_config_cuda//cuda:build_defs.bzl", "if_cuda_is_configured")
 load("@local_config_rocm//rocm:build_defs.bzl", "if_rocm_is_configured")
+load("@local_config_sycl//sycl:build_defs.bzl", "if_sycl_is_configured")
 
 def stream_executor_friends():
     return ["//..."]
@@ -21,7 +22,7 @@ def tf_additional_cudnn_plugin_copts():
 
 # Returns whether any GPU backend is configuered.
 def if_gpu_is_configured(x):
-    return if_cuda_is_configured(x) + if_rocm_is_configured(x)
+    return if_cuda_is_configured(x) + if_rocm_is_configured(x) + if_sycl_is_configured(x)
 
 def if_cuda_or_rocm(x):
     return if_gpu_is_configured(x)
diff --git a/xla/stream_executor/cuda/cuda_driver.cc b/xla/stream_executor/cuda/cuda_driver.cc
index bf16b27f5..c045c2030 100644
--- a/xla/stream_executor/cuda/cuda_driver.cc
+++ b/xla/stream_executor/cuda/cuda_driver.cc
@@ -942,6 +942,14 @@ GpuDriver::GraphNodeGetType(CUgraphNode node) {
       "Feature not supported on CUDA platform (LoadHsaco)");
 }
 
+/* static */ tsl::Status GpuDriver::LoadLevelzero(GpuContext* context,
+                                                  const char* spir_contents,
+                                                  const size_t size,
+                                                  CUmodule* module) {
+  LOG(ERROR) << "Feature not supported on CUDA platform (LoadLevelzero)";
+  return tsl::errors::Internal("Not Implemented");
+}
+
 /* static */ tsl::Status GpuDriver::SynchronousMemsetUint8(GpuContext* context,
                                                            CUdeviceptr location,
                                                            uint8_t value,
diff --git a/xla/stream_executor/cuda/cuda_executor.cc b/xla/stream_executor/cuda/cuda_executor.cc
index 534ef26aa..43a4511c6 100644
--- a/xla/stream_executor/cuda/cuda_executor.cc
+++ b/xla/stream_executor/cuda/cuda_executor.cc
@@ -173,6 +173,12 @@ tsl::Status GpuExecutor::LoadModuleFromHsaco(const char* hsaco,
       "Feature not supported on CUDA platform (LoadModuleFromHsaco)");
 }
 
+tsl::Status GpuExecutor::LoadModuleFromSpir(const char* spir, const size_t size,
+                                            CUmodule* module) {
+  return tsl::errors::Internal(
+      "Feature not supported on CUDA platform (LoadModuleFromSpir)");
+}
+
 tsl::Status GpuExecutor::GetKernel(const MultiKernelLoaderSpec& spec,
                                    KernelBase* kernel) {
   GpuKernel* cuda_kernel = AsGpuKernel(kernel);
diff --git a/xla/stream_executor/gpu/BUILD b/xla/stream_executor/gpu/BUILD
index 827c01adf..dfaaaa6a0 100644
--- a/xla/stream_executor/gpu/BUILD
+++ b/xla/stream_executor/gpu/BUILD
@@ -9,6 +9,10 @@ load(
     "@local_config_rocm//rocm:build_defs.bzl",
     "if_rocm_is_configured",
 )
+load(
+    "@local_config_sycl//sycl:build_defs.bzl",
+    "if_sycl_is_configured",
+)
 load(
     "@tsl//tsl:tsl.bzl",
     "if_libtpu",
@@ -290,6 +294,8 @@ cc_library(
         "@local_config_cuda//cuda:cuda_headers",
     ]) + if_rocm_is_configured([
         "@local_config_rocm//rocm:rocm_headers",
+    ]) + if_sycl_is_configured([
+        "@local_config_sycl//sycl:sycl_headers",
     ]),
 )
 
diff --git a/xla/stream_executor/gpu/gpu_driver.h b/xla/stream_executor/gpu/gpu_driver.h
index ef5280ac9..0411f8ac3 100644
--- a/xla/stream_executor/gpu/gpu_driver.h
+++ b/xla/stream_executor/gpu/gpu_driver.h
@@ -475,6 +475,9 @@ class GpuDriver {
   // (supported on ROCm only)
   static tsl::Status LoadHsaco(GpuContext* context, const char* hsaco_contents,
                                GpuModuleHandle* module);
+  static tsl::Status LoadLevelzero(GpuContext* context,
+                                   const char* spir_contents, const size_t size,
+                                   GpuModuleHandle* module);
 
   // Retrieves a named kernel from a loaded module, and places the resulting
   // handle into function (outparam) on success. Neither kernel_name nor
diff --git a/xla/stream_executor/gpu/gpu_executor.h b/xla/stream_executor/gpu/gpu_executor.h
index b5fdb64ab..89e81a613 100644
--- a/xla/stream_executor/gpu/gpu_executor.h
+++ b/xla/stream_executor/gpu/gpu_executor.h
@@ -320,6 +320,11 @@ class GpuExecutor : public internal::StreamExecutorInterface {
   tsl::Status LoadModuleFromHsaco(const char* hsaco, GpuModuleHandle* module)
       TF_EXCLUSIVE_LOCKS_REQUIRED(in_memory_modules_mu_);
 
+  // (supported on SYCL only)
+  tsl::Status LoadModuleFromSpir(const char* spir, const size_t size,
+                                 GpuModuleHandle* module)
+      TF_EXCLUSIVE_LOCKS_REQUIRED(in_memory_modules_mu_);
+
   bool UnloadGpuBinary(const void* gpu_binary)
       TF_EXCLUSIVE_LOCKS_REQUIRED(in_memory_modules_mu_);
 
diff --git a/xla/stream_executor/gpu/gpu_helpers.h b/xla/stream_executor/gpu/gpu_helpers.h
index cefd0a6af..5f2f78caf 100644
--- a/xla/stream_executor/gpu/gpu_helpers.h
+++ b/xla/stream_executor/gpu/gpu_helpers.h
@@ -52,14 +52,18 @@ T* GpuMemoryMutable(DeviceMemory<T>* mem) {
 static_assert(
     sizeof(std::complex<float>) == sizeof(GpuComplexType),
     "std::complex<float> and GpuComplexType should have the same size");
+#if !GOOGLE_SYCL
 static_assert(offsetof(GpuComplexType, x) == 0,
               "The real part of GpuComplexType should appear first.");
+#endif // !GOOGLE_SYCL
 static_assert(
     sizeof(std::complex<double>) == sizeof(GpuDoubleComplexType),
     "std::complex<double> and GpuDoubleComplexType should have the same "
     "size");
+#if !GOOGLE_SYCL
 static_assert(offsetof(GpuDoubleComplexType, x) == 0,
               "The real part of GpuDoubleComplexType should appear first.");
+#endif // !GOOGLE_SYCL
 
 // Type traits to get CUDA complex types from std::complex<>.
 
diff --git a/xla/stream_executor/gpu/gpu_types.h b/xla/stream_executor/gpu/gpu_types.h
index dea81d66a..c934b326b 100644
--- a/xla/stream_executor/gpu/gpu_types.h
+++ b/xla/stream_executor/gpu/gpu_types.h
@@ -18,7 +18,18 @@ limitations under the License.
 #ifndef XLA_STREAM_EXECUTOR_GPU_GPU_TYPES_H_
 #define XLA_STREAM_EXECUTOR_GPU_GPU_TYPES_H_
 
-#if TENSORFLOW_USE_ROCM
+#if GOOGLE_SYCL
+
+#if __has_include(<sycl/sycl.hpp>)
+#include <sycl/sycl.hpp>
+#elif __has_include(<CL/sycl.hpp>)
+#include <CL/sycl.hpp>
+#else
+#error "Unsupported compiler"
+#endif
+#include <level_zero/ze_api.h>
+
+#elif TENSORFLOW_USE_ROCM
 
 #define __HIP_DISABLE_CPP_FUNCTIONS__
 
@@ -36,7 +47,33 @@ limitations under the License.
 namespace stream_executor {
 namespace gpu {
 
-#if TENSORFLOW_USE_ROCM
+#if GOOGLE_SYCL
+typedef struct SYCLEventWrapper {
+  ::sycl::event* event;
+  ::sycl::queue* queue;
+} EventWrapper;
+
+using GpuContextHandle = const void*;
+using GpuStreamHandle = ::sycl::queue*;
+using GpuEventHandle = EventWrapper*;
+using GpuFunctionHandle = ::sycl::kernel*;
+using GpuFunctionAttribute = const void*;
+using GpuDeviceHandle = ::sycl::device*;
+using GpuDevicePtr = void*;
+using GpuDeviceAttribute = const void*;
+using GpuDeviceProperty = const void*;
+using GpuModuleHandle = ze_module_handle_t;
+using GpuStatus = const void*;
+using GpuFuncCachePreference = const void*;
+using GpuSharedMemConfig = const void*;
+using GpuComplexType = std::complex<float>;
+using GpuDoubleComplexType = std::complex<double>;
+using GpuRngHandle = const void*;
+using GpuGraphHandle = const void*;
+using GpuGraphExecHandle = const void*;
+using GpuGraphNodeHandle = const void*;
+
+#elif TENSORFLOW_USE_ROCM
 
 using GpuContextHandle = hipCtx_t;
 using GpuStreamHandle = hipStream_t;
diff --git a/xla/stream_executor/kernel_spec.cc b/xla/stream_executor/kernel_spec.cc
index f0fabf44d..583d043b9 100644
--- a/xla/stream_executor/kernel_spec.cc
+++ b/xla/stream_executor/kernel_spec.cc
@@ -43,9 +43,9 @@ CudaCubinOnDisk::CudaCubinOnDisk(absl::string_view filename,
                                  absl::string_view kernel_name)
     : OnDiskKernelLoaderSpec(filename, kernel_name) {}
 
-CudaCubinInMemory::CudaCubinInMemory(const char *bytes,
+CudaCubinInMemory::CudaCubinInMemory(const char *bytes, int size,
                                      absl::string_view kernel_name)
-    : KernelLoaderSpec(kernel_name), bytes_(bytes) {}
+    : KernelLoaderSpec(kernel_name), size_(size), bytes_(bytes) {}
 
 bool CompareComputeCapability(const std::tuple<int, int> &lhs,
                               const std::tuple<int, int> &rhs) {
@@ -174,9 +174,9 @@ MultiKernelLoaderSpec *MultiKernelLoaderSpec::AddCudaPtxOnDisk(
 }
 
 MultiKernelLoaderSpec *MultiKernelLoaderSpec::AddCudaCubinInMemory(
-    const char *bytes, absl::string_view kernel_name) {
+    const char *bytes, int size, absl::string_view kernel_name) {
   CHECK(cuda_cubin_in_memory_ == nullptr);
-  cuda_cubin_in_memory_.reset(new CudaCubinInMemory{bytes, kernel_name});
+  cuda_cubin_in_memory_.reset(new CudaCubinInMemory{bytes, size, kernel_name});
   return this;
 }
 
diff --git a/xla/stream_executor/kernel_spec.h b/xla/stream_executor/kernel_spec.h
index 68f45a0ee..a05ed6da6 100644
--- a/xla/stream_executor/kernel_spec.h
+++ b/xla/stream_executor/kernel_spec.h
@@ -224,13 +224,16 @@ class CudaPtxInMemory : public KernelLoaderSpec {
 // Kernel loader specification for a CUBIN blob that resides in memory.
 class CudaCubinInMemory : public KernelLoaderSpec {
  public:
-  CudaCubinInMemory(const char *bytes, absl::string_view kernel_name);
+  CudaCubinInMemory(const char *bytes, int size, absl::string_view kernel_name);
   ~CudaCubinInMemory() override {}
 
   const char *bytes() const { return bytes_; }
+  const int size() const { return size_; }
 
  private:
   const char *bytes_;
+  // SYCL: this is needed only for SPIRV
+  int size_;
 
   CudaCubinInMemory(const CudaCubinInMemory &) = delete;
   void operator=(const CudaCubinInMemory &) = delete;
@@ -282,7 +285,7 @@ class MultiKernelLoaderSpec {
                                           absl::string_view kernel_name);
   MultiKernelLoaderSpec *AddCudaCubinOnDisk(absl::string_view filename,
                                             absl::string_view kernel_name);
-  MultiKernelLoaderSpec *AddCudaCubinInMemory(const char *cubin_bytes,
+  MultiKernelLoaderSpec *AddCudaCubinInMemory(const char *cubin_bytes, int size,
                                               absl::string_view kernel_name);
   MultiKernelLoaderSpec *AddCudaPtxInMemory(absl::string_view ptx,
                                             absl::string_view kernel_name);
diff --git a/xla/stream_executor/rocm/rocm_driver.cc b/xla/stream_executor/rocm/rocm_driver.cc
index 0941efa5b..265344f65 100644
--- a/xla/stream_executor/rocm/rocm_driver.cc
+++ b/xla/stream_executor/rocm/rocm_driver.cc
@@ -821,6 +821,14 @@ GpuDriver::GraphNodeGetType(hipGraphNode_t node) {
   return ret;
 }
 
+/* static */ tsl::Status GpuDriver::LoadLevelzero(GpuContext* context,
+                                                  const char* spir_contents,
+                                                  const size_t size,
+                                                  hipModule_t* module) {
+  LOG(ERROR) << "Feature not supported on ROCm platform (LoadLevelzero)";
+  return tsl::errors::Internal("Not Implemented");
+}
+
 /* static */ tsl::Status GpuDriver::SynchronousMemsetUint8(
     GpuContext* context, hipDeviceptr_t location, uint8 value, size_t size) {
   ScopedActivateContext activation{context};
diff --git a/xla/stream_executor/rocm/rocm_gpu_executor.cc b/xla/stream_executor/rocm/rocm_gpu_executor.cc
index 76864db70..a1a234e66 100644
--- a/xla/stream_executor/rocm/rocm_gpu_executor.cc
+++ b/xla/stream_executor/rocm/rocm_gpu_executor.cc
@@ -389,6 +389,11 @@ tsl::Status GpuExecutor::LoadModuleFromHsaco(const char* hsaco,
   return tsl::OkStatus();
 }
 
+tsl::Status GpuExecutor::LoadModuleFromSpir(const char* spir, const size_t size,
+                                            hipModule_t* module) {
+  LOG(FATAL) << "Feature not supported on ROCM platform (LoadModuleFromSpir)";
+}
+
 // This is a non-essential operation; if there's a failure, proceed without
 // logging an error. It's nearly certain that in case of failures, we'd never
 // get here in the first place; these are very low-impact routines.
diff --git a/xla/stream_executor/stream_executor_pimpl.h b/xla/stream_executor/stream_executor_pimpl.h
index dc15118bb..89ab74d54 100644
--- a/xla/stream_executor/stream_executor_pimpl.h
+++ b/xla/stream_executor/stream_executor_pimpl.h
@@ -701,7 +701,8 @@ StreamExecutor::CreateTypedKernel(absl::string_view kernel_name,
 
   if (!cubin_data.empty()) {
     loader_spec.AddCudaCubinInMemory(
-        reinterpret_cast<const char*>(cubin_data.data()), kernel_name);
+        reinterpret_cast<const char*>(cubin_data.data()), cubin_data.size(),
+        kernel_name);
   }
 
   TF_RETURN_IF_ERROR(GetKernel(loader_spec, kernel_base.get()));
