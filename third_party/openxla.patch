diff --git a/.bazelrc b/.bazelrc
index e26bf6de7..3e2d558ca 100644
--- a/.bazelrc
+++ b/.bazelrc
@@ -48,6 +48,7 @@
 #     xla:          Build TF with XLA
 #     tpu:          Build TF with TPU support
 #     cuda:         Build with full cuda support.
+#     sycl:         Build with Intel GPU support (sycl).
 #     rocm:         Build with AMD GPU support (rocm).
 #     mkl:          Enable full mkl support.
 #     tensorrt:     Enable Tensorrt support.
@@ -291,6 +292,8 @@ build:rocm --define=using_rocm_hipcc=true
 build:rocm --define=tensorflow_mkldnn_contraction_kernel=0
 build:rocm --repo_env TF_NEED_ROCM=1
 
+build:sycl --repo_env TF_NEED_SYCL=1
+
 # Options extracted from configure script
 build:numa --define=with_numa_support=true
 
diff --git a/configure.py b/configure.py
index 7b7c3c58d..c605d0e85 100644
--- a/configure.py
+++ b/configure.py
@@ -767,7 +767,8 @@ def system_specific_test_config(environ_cp):
   if is_windows():
     test_and_build_filters += ['-no_windows', '-windows_excluded']
     if ((environ_cp.get('TF_NEED_CUDA', None) == '1') or
-        (environ_cp.get('TF_NEED_ROCM', None) == '1')):
+        (environ_cp.get('TF_NEED_ROCM', None) == '1') or
+        (environ_cp.get('TF_NEED_SYCL', None) == '1')):
       test_and_build_filters += ['-no_windows_gpu', '-no_gpu']
     else:
       test_and_build_filters.append('-gpu')
@@ -775,13 +776,16 @@ def system_specific_test_config(environ_cp):
     test_and_build_filters += ['-gpu', '-nomac', '-no_mac', '-mac_excluded']
   elif is_linux():
     if ((environ_cp.get('TF_NEED_CUDA', None) == '1') or
-        (environ_cp.get('TF_NEED_ROCM', None) == '1')):
+        (environ_cp.get('TF_NEED_ROCM', None) == '1') or
+        (environ_cp.get('TF_NEED_SYCL', None) == '1')):
       test_and_build_filters.append('-no_gpu')
       write_to_bazelrc('test --test_env=LD_LIBRARY_PATH')
     else:
       test_and_build_filters.append('-gpu')
     if environ_cp.get('TF_NEED_ROCM', None) == '1':
       test_and_build_filters.append('-no_rocm')
+    if environ_cp.get('TF_NEED_SYCL', None) == '1':
+      test_and_build_filters.append('-no_sycl')
 
   write_to_bazelrc('test --test_tag_filters=%s' %
                    ','.join(test_and_build_filters + test_only_filters))
@@ -899,6 +903,18 @@ def get_gcc_compiler(environ_cp):
   return None
 
 
+
+def set_sycl_path(environ_cp):
+  """Set TF_CUDA_PATHS."""
+  ask_sycl_path = (
+      'Please specify base path to look for SYCL '
+      'libraries and headers. [Leave empty to use the default]: ')
+  sycl_path = get_from_env_or_user_or_default(environ_cp, 'SYCL_PATH',
+                                              ask_sycl_path, '')
+  if sycl_path:
+    environ_cp['SYCL_PATH'] = sycl_path
+
+
 def main():
   global _TF_WORKSPACE_ROOT
   global _TF_BAZELRC
@@ -959,6 +975,13 @@ def main():
         write_to_bazelrc(
             'build --copt="-DEIGEN_ALTIVEC_ENABLE_MMA_DYNAMIC_DISPATCH=1"')
 
+  set_action_env_var(
+      environ_cp, 'TF_NEED_SYCL', 'SYCL', False, bazel_config_name='sycl')
+  if (environ_cp.get('TF_NEED_SYCL') == '1'):
+    set_sycl_path(environ_cp)
+  if (environ_cp.get('TF_NEED_SYCL') == '1' and environ_cp.get('SYCL_PATH')):
+    write_action_env_to_bazelrc('SYCL_PATH', environ_cp.get('SYCL_PATH'))
+
   set_action_env_var(
       environ_cp, 'TF_NEED_ROCM', 'ROCm', False, bazel_config_name='rocm')
   if (environ_cp.get('TF_NEED_ROCM') == '1' and
diff --git a/third_party/tsl/third_party/gpus/compress_find_sycl_config.py b/third_party/tsl/third_party/gpus/compress_find_sycl_config.py
new file mode 100644
index 000000000..7fb456fe9
--- /dev/null
+++ b/third_party/tsl/third_party/gpus/compress_find_sycl_config.py
@@ -0,0 +1,36 @@
+# Copyright 2020 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+"""Compresses the contents of 'find_sycl_config.py'.
+
+The compressed file is what is actually being used. It works around remote
+config not being able to upload files yet.
+"""
+import base64
+import zlib
+
+
+def main():
+  with open('find_sycl_config.py', 'rb') as f:
+    data = f.read()
+
+  compressed = zlib.compress(data)
+  b64encoded = base64.b64encode(compressed)
+
+  with open('find_sycl_config.py.gz.base64', 'wb') as f:
+    f.write(b64encoded)
+
+
+if __name__ == '__main__':
+  main()
diff --git a/third_party/tsl/third_party/gpus/find_sycl_config.py b/third_party/tsl/third_party/gpus/find_sycl_config.py
new file mode 100644
index 000000000..16a1b7ce2
--- /dev/null
+++ b/third_party/tsl/third_party/gpus/find_sycl_config.py
@@ -0,0 +1,125 @@
+# Copyright 2020 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+"""Prints SYCL library and header directories and versions found on the system.
+
+The script searches for SYCL library and header files on the system, inspects
+them to determine their version and prints the configuration to stdout.
+The path to inspect is specified through an environment variable (SYCL_PATH).
+If no valid configuration is found, the script prints to stderr and
+returns an error code.
+
+The script takes the directory specified by the SYCL_PATH environment variable.
+The script looks for headers and library files in a hard-coded set of
+subdirectories from base path of the specified directory. If SYCL_PATH is not
+specified, then "/opt/sycl" is used as it default value
+
+"""
+
+import io
+import os
+import re
+import sys
+
+
+class ConfigError(Exception):
+  pass
+
+
+def _get_default_sycl_path():
+  return "/opt/intel/oneapi"
+
+
+def _get_sycl_install_path():
+  """Determines and returns the SYCL installation path."""
+  sycl_install_path = _get_default_sycl_path()
+  if "SYCL_PATH" in os.environ:
+    sycl_install_path = os.environ["SYCL_PATH"]
+  return sycl_install_path
+
+
+def _get_composite_version_number(major, minor, patch):
+  return 10000 * major + 100 * minor + patch
+
+
+def _get_header_version(path, name):
+  """Returns preprocessor defines in C header file."""
+  for line in io.open(path, "r", encoding="utf-8"):
+    match = re.match(r"#define %s +(\d+)" % name, line)
+    if match:
+      value = match.group(1)
+      return int(value)
+
+  raise ConfigError('#define "{}" is either\n'.format(name) +
+                    "  not present in file {} OR\n".format(path) +
+                    "  its value is not an integer literal")
+
+
+def _find_sycl_config(sycl_install_path):
+
+  def sycl_version_numbers(path):
+    possible_version_files = [
+        "compiler/latest/linux/include/sycl/version.hpp",
+        "compiler/latest/include/sycl/version.hpp",
+    ]
+    version_file = None
+    for f in possible_version_files:
+      version_file_path = os.path.join(path, f)
+      if os.path.exists(version_file_path):
+        version_file = version_file_path
+        break
+    if not version_file:
+      raise ConfigError(
+          "SYCL version file not found in {}".format(possible_version_files))
+
+    major = _get_header_version(version_file, "__LIBSYCL_MAJOR_VERSION")
+    minor = _get_header_version(version_file, "__LIBSYCL_MINOR_VERSION")
+    patch = _get_header_version(version_file, "__LIBSYCL_PATCH_VERSION")
+    return major, minor, patch
+
+  major, minor, patch = sycl_version_numbers(sycl_install_path)
+
+  sycl_config = {
+      "sycl_version_number": _get_composite_version_number(major, minor, patch)
+  }
+
+  return sycl_config
+
+
+def find_sycl_config():
+  """Returns a dictionary of SYCL components config info."""
+  sycl_install_path = _get_sycl_install_path()
+  if not os.path.exists(sycl_install_path):
+    raise ConfigError(
+        'Specified SYCL_PATH "{}" does not exist'.format(sycl_install_path))
+
+  result = {}
+
+  result["sycl_toolkit_path"] = sycl_install_path
+  result.update(_find_sycl_config(sycl_install_path))
+
+  return result
+
+
+def main():
+  try:
+    for key, value in sorted(find_sycl_config().items()):
+      print("%s: %s" % (key, value))
+  except ConfigError as e:
+    sys.stderr.write("\nERROR: {}\n\n".format(str(e)))
+    sys.exit(1)
+
+
+if __name__ == "__main__":
+  main()
diff --git a/third_party/tsl/third_party/gpus/find_sycl_config.py.gz.base64 b/third_party/tsl/third_party/gpus/find_sycl_config.py.gz.base64
new file mode 100644
index 000000000..8e21f12b1
--- /dev/null
+++ b/third_party/tsl/third_party/gpus/find_sycl_config.py.gz.base64
@@ -0,0 +1 @@
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
\ No newline at end of file
diff --git a/third_party/tsl/third_party/gpus/sycl/BUILD b/third_party/tsl/third_party/gpus/sycl/BUILD
new file mode 100644
index 000000000..ffd0fb0cd
--- /dev/null
+++ b/third_party/tsl/third_party/gpus/sycl/BUILD
@@ -0,0 +1 @@
+package(default_visibility = ["//visibility:public"])
diff --git a/third_party/tsl/third_party/gpus/sycl/BUILD.tpl b/third_party/tsl/third_party/gpus/sycl/BUILD.tpl
new file mode 100644
index 000000000..aa3688e33
--- /dev/null
+++ b/third_party/tsl/third_party/gpus/sycl/BUILD.tpl
@@ -0,0 +1,182 @@
+load("@bazel_skylib//:bzl_library.bzl", "bzl_library")
+
+licenses(["restricted"])  # MPL2, portions GPL v3, LGPL v3, BSD-like
+
+package(default_visibility = ["//visibility:public"])
+
+config_setting(
+    name = "using_hipcc",
+    values = {
+        "define": "using_rocm_hipcc=true",
+    },
+)
+
+cc_library(
+    name = "rocm_headers",
+    hdrs = [
+        "rocm/rocm_config.h",
+        %{rocm_headers}
+    ],
+    includes = [
+        ".",
+        "rocm/include",
+        "rocm/include/rocrand",
+        "rocm/include/roctracer",
+    ],
+    visibility = ["//visibility:public"],
+)
+
+cc_library(
+    name = "hip",
+    srcs = ["rocm/lib/%{hip_lib}"],
+    data = ["rocm/lib/%{hip_lib}"],
+    includes = [
+        ".",
+        "rocm/include",
+    ],
+    linkstatic = 1,
+    visibility = ["//visibility:public"],
+)
+
+cc_library(
+    name = "rocblas",
+    srcs = ["rocm/lib/%{rocblas_lib}"],
+    data = ["rocm/lib/%{rocblas_lib}"],
+    includes = [
+        ".",
+        "rocm/include",
+    ],
+    linkstatic = 1,
+    visibility = ["//visibility:public"],
+)
+
+cc_library(
+    name = "%{hipfft_or_rocfft}",
+    srcs = ["rocm/lib/%{hipfft_or_rocfft_lib}"],
+    data = ["rocm/lib/%{hipfft_or_rocfft_lib}"],
+    includes = [
+        ".",
+        "rocm/include",
+    ],
+    linkstatic = 1,
+    visibility = ["//visibility:public"],
+)
+
+cc_library(
+    name = "hiprand",
+    srcs = ["rocm/lib/%{hiprand_lib}"],
+    data = ["rocm/lib/%{hiprand_lib}"],
+    includes = [
+        ".",
+        "rocm/include",
+        "rocm/include/rocrand",
+    ],
+    linkstatic = 1,
+    visibility = ["//visibility:public"],
+)
+
+cc_library(
+    name = "miopen",
+    srcs = ["rocm/lib/%{miopen_lib}"],
+    data = ["rocm/lib/%{miopen_lib}"],
+    includes = [
+        ".",
+        "rocm/include",
+    ],
+    linkstatic = 1,
+    visibility = ["//visibility:public"],
+)
+
+cc_library(
+    name = "rccl",
+    srcs = ["rocm/lib/%{rccl_lib}"],
+    data = ["rocm/lib/%{rccl_lib}"],
+    includes = [
+        ".",
+        "rocm/include",
+    ],
+    linkstatic = 1,
+    visibility = ["//visibility:public"],
+)
+
+cc_library(
+    name = "rocm",
+    visibility = ["//visibility:public"],
+    deps = [
+        ":rocm_headers",
+        ":hip",
+        ":rocblas",
+        ":hipblas",
+        ":%{hipfft_or_rocfft}",
+        ":hiprand",
+        ":miopen",
+        ":hipsparse",
+        ":roctracer",
+        ":rocsolver",
+        ":hipsolver",
+    ],
+)
+
+bzl_library(
+    name = "build_defs_bzl",
+    srcs = ["build_defs.bzl"],
+)
+
+cc_library(
+    name = "rocprim",
+    srcs = [
+        "rocm/include/hipcub/hipcub_version.hpp",
+        "rocm/include/rocprim/rocprim_version.hpp",
+    ],
+    hdrs = glob([
+        "rocm/include/hipcub/**",
+        "rocm/include/rocprim/**",
+    ]),
+    includes = [
+        ".",
+        "rocm/include/hipcub",
+        "rocm/include/rocprim",
+    ],
+    visibility = ["//visibility:public"],
+    deps = [
+        "@local_config_rocm//rocm:rocm_headers",
+    ],
+)
+
+cc_library(
+    name = "hipsparse",
+    srcs = ["rocm/lib/%{hipsparse_lib}"],
+    data = ["rocm/lib/%{hipsparse_lib}"],
+)
+
+cc_library(
+    name = "roctracer",
+    data = ["rocm/lib/%{roctracer_lib}"],
+)
+
+cc_library(
+    name = "rocsolver",
+    srcs = ["rocm/lib/%{rocsolver_lib}"],
+    data = ["rocm/lib/%{rocsolver_lib}"],
+)
+
+cc_library(
+    name = "hipsolver",
+    srcs = ["rocm/lib/%{hipsolver_lib}"],
+    data = ["rocm/lib/%{hipsolver_lib}"],
+)
+
+cc_library(
+    name = "hipblas",
+    srcs = ["rocm/lib/%{hipblas_lib}"],
+    data = ["rocm/lib/%{hipblas_lib}"],
+)
+
+filegroup(
+    name = "rocm_root",
+    srcs = [
+        "rocm/bin/clang-offload-bundler",
+    ],
+)
+
+%{copy_rules}
diff --git a/third_party/tsl/third_party/gpus/sycl/build_defs.bzl.tpl b/third_party/tsl/third_party/gpus/sycl/build_defs.bzl.tpl
new file mode 100644
index 000000000..51c0955d2
--- /dev/null
+++ b/third_party/tsl/third_party/gpus/sycl/build_defs.bzl.tpl
@@ -0,0 +1,45 @@
+# # Macros for building SYCL code.
+# def if_sycl(if_true, if_false = []):
+#     """Shorthand for select()'ing on whether we're building with SYCL.
+# 
+#     Returns a select statement which evaluates to if_true if we're building
+#     with SYCL enabled.  Otherwise, the select statement evaluates to if_false.
+# 
+#     """
+#     return select({
+#         "@local_config_sycl//sycl:using_hipcc": if_true,
+#         "//conditions:default": if_false
+#     })
+# 
+# 
+# def sycl_default_copts():
+#     """Default options for all SYCL compilations."""
+#     return if_sycl(["-x", "sycl"] + %{sycl_extra_copts})
+# 
+# def sycl_copts(opts = []):
+#     """Gets the appropriate set of copts for (maybe) SYCL compilation.
+# 
+#       If we're doing SYCL compilation, returns copts for our particular SYCL
+#       compiler.  If we're not doing SYCL compilation, returns an empty list.
+# 
+#       """
+#     return sycl_default_copts() + select({
+#         "//conditions:default": [],
+#         "@local_config_sycl//sycl:using_hipcc": ([
+#             "",
+#         ]),
+#     }) + if_sycl_is_configured(opts)
+
+def if_sycl_is_configured(x):
+    """Tests if the SYCL was enabled during the configure process.
+
+    Unlike if_sycl(), this does not require that we are building with
+    --config=sycl. Used to allow non-SYCL code to depend on SYCL libraries.
+    """
+    if %{sycl_is_configured}:
+      return select({"//conditions:default": x})
+    return select({"//conditions:default": []})
+
+# def sycl_library(copts = [], **kwargs):
+#     """Wrapper over cc_library which adds default SYCL options."""
+#     native.cc_library(copts = sycl_default_copts() + copts, **kwargs)
diff --git a/third_party/tsl/third_party/gpus/sycl_configure.bzl b/third_party/tsl/third_party/gpus/sycl_configure.bzl
new file mode 100644
index 000000000..320b3d303
--- /dev/null
+++ b/third_party/tsl/third_party/gpus/sycl_configure.bzl
@@ -0,0 +1,682 @@
+"""Repository rule for SYCL autoconfiguration.
+
+`sycl_configure` depends on the following environment variables:
+
+  * `TF_NEED_SYCL`: Whether to enable building with SYCL.
+  * `GCC_HOST_COMPILER_PATH`: The GCC host compiler path
+  * `SYCL_PATH`: The path to the SYCL toolkit. Default is `/opt/sycl`.
+  * `TF_SYCL_AMDGPU_TARGETS`: The AMDGPU targets.
+"""
+
+load(
+    ":cuda_configure.bzl",
+    "make_copy_dir_rule",
+    "make_copy_files_rule",
+    "to_list_of_strings",
+)
+load(
+    "//third_party/remote_config:common.bzl",
+    "config_repo_label",
+    "err_out",
+    "execute",
+    "files_exist",
+    "get_bash_bin",
+    "get_cpu_value",
+    "get_host_environ",
+    "get_python_bin",
+    "raw_exec",
+    "realpath",
+    "which",
+)
+
+_GCC_HOST_COMPILER_PATH = "GCC_HOST_COMPILER_PATH"
+_GCC_HOST_COMPILER_PREFIX = "GCC_HOST_COMPILER_PREFIX"
+_SYCL_TOOLKIT_PATH = "SYCL_PATH"
+_TF_SYCL_CONFIG_REPO = "TF_SYCL_CONFIG_REPO"
+
+_DEFAULT_SYCL_TOOLKIT_PATH = "/opt/sycl"
+
+def verify_build_defines(params):
+    """Verify all variables that crosstool/BUILD.sycl.tpl expects are substituted.
+
+    Args:
+      params: dict of variables that will be passed to the BUILD.tpl template.
+    """
+    missing = []
+    for param in [
+        "cxx_builtin_include_directories",
+        "extra_no_canonical_prefixes_flags",
+        "host_compiler_path",
+        "host_compiler_prefix",
+        "linker_bin_path",
+        "unfiltered_compile_flags",
+    ]:
+        if ("%{" + param + "}") not in params:
+            missing.append(param)
+
+    if missing:
+        auto_configure_fail(
+            "BUILD.sycl.tpl template is missing these variables: " +
+            str(missing) +
+            ".\nWe only got: " +
+            str(params) +
+            ".",
+        )
+
+def find_cc(repository_ctx):
+    """Find the C++ compiler."""
+
+    # Return a dummy value for GCC detection here to avoid error
+    target_cc_name = "gcc"
+    cc_path_envvar = _GCC_HOST_COMPILER_PATH
+    cc_name = target_cc_name
+
+    cc_name_from_env = get_host_environ(repository_ctx, cc_path_envvar)
+    if cc_name_from_env:
+        cc_name = cc_name_from_env
+    if cc_name.startswith("/"):
+        # Absolute path, maybe we should make this supported by our which function.
+        return cc_name
+    cc = which(repository_ctx, cc_name)
+    if cc == None:
+        fail(("Cannot find {}, either correct your path or set the {}" +
+              " environment variable").format(target_cc_name, cc_path_envvar))
+    return cc
+
+_INC_DIR_MARKER_BEGIN = "#include <...>"
+
+def _cxx_inc_convert(path):
+    """Convert path returned by cc -E xc++ in a complete path."""
+    path = path.strip()
+    return path
+
+def _get_cxx_inc_directories_impl(repository_ctx, cc, lang_is_cpp):
+    """Compute the list of default C or C++ include directories."""
+    if lang_is_cpp:
+        lang = "c++"
+    else:
+        lang = "c"
+
+    # TODO: We pass -no-canonical-prefixes here to match the compiler flags,
+    #       but in sycl_clang CROSSTOOL file that is a `feature` and we should
+    #       handle the case when it's disabled and no flag is passed
+    result = raw_exec(repository_ctx, [
+        cc,
+        "-no-canonical-prefixes",
+        "-E",
+        "-x" + lang,
+        "-",
+        "-v",
+    ])
+    stderr = err_out(result)
+    index1 = stderr.find(_INC_DIR_MARKER_BEGIN)
+    if index1 == -1:
+        return []
+    index1 = stderr.find("\n", index1)
+    if index1 == -1:
+        return []
+    index2 = stderr.rfind("\n ")
+    if index2 == -1 or index2 < index1:
+        return []
+    index2 = stderr.find("\n", index2 + 1)
+    if index2 == -1:
+        inc_dirs = stderr[index1 + 1:]
+    else:
+        inc_dirs = stderr[index1 + 1:index2].strip()
+
+    return [
+        str(repository_ctx.path(_cxx_inc_convert(p)))
+        for p in inc_dirs.split("\n")
+    ]
+
+def get_cxx_inc_directories(repository_ctx, cc):
+    """Compute the list of default C and C++ include directories."""
+
+    # For some reason `clang -xc` sometimes returns include paths that are
+    # different from the ones from `clang -xc++`. (Symlink and a dir)
+    # So we run the compiler with both `-xc` and `-xc++` and merge resulting lists
+    includes_cpp = _get_cxx_inc_directories_impl(repository_ctx, cc, True)
+    includes_c = _get_cxx_inc_directories_impl(repository_ctx, cc, False)
+
+    includes_cpp_set = depset(includes_cpp)
+    return includes_cpp + [
+        inc
+        for inc in includes_c
+        if inc not in includes_cpp_set.to_list()
+    ]
+
+def auto_configure_fail(msg):
+    """Output failure message when sycl configuration fails."""
+    red = "\033[0;31m"
+    no_color = "\033[0m"
+    fail("\n%sSYCL Configuration Error:%s %s\n" % (red, no_color, msg))
+
+def auto_configure_warning(msg):
+    """Output warning message during auto configuration."""
+    yellow = "\033[1;33m"
+    no_color = "\033[0m"
+    print("\n%sAuto-Configuration Warning:%s %s\n" % (yellow, no_color, msg))
+
+# END cc_configure common functions (see TODO above).
+
+def _sycl_include_path(repository_ctx, sycl_config, bash_bin):
+    """Generates the cxx_builtin_include_directory entries for sycl inc dirs.
+
+    Args:
+      repository_ctx: The repository context.
+      sycl_config: The path to the gcc host compiler.
+
+    Returns:
+      A string containing the Starlark string for each of the gcc
+      host compiler include directories, which can be added to the CROSSTOOL
+      file.
+    """
+    inc_dirs = []
+
+    sycl_toolkit_path = sycl_config.sycl_toolkit_path + "/compiler/latest/linux"
+    mkl_path = sycl_config.sycl_toolkit_path + "/mkl/latest"
+    inc_dirs.append(sycl_toolkit_path + "/include")
+    inc_dirs.append(mkl_path + "/include")
+    inc_dirs.append(sycl_toolkit_path + "/lib/clang/17/include")
+
+    return inc_dirs
+
+def _enable_sycl(repository_ctx):
+    enable_sycl = get_host_environ(repository_ctx, "TF_NEED_SYCL")
+    if enable_sycl == "1":
+        if get_cpu_value(repository_ctx) != "Linux":
+            auto_configure_warning("SYCL configure is only supported on Linux")
+            return False
+        return True
+    return False
+
+def _hipcc_env(repository_ctx):
+    """Returns the environment variable string for hipcc.
+
+    Args:
+        repository_ctx: The repository context.
+
+    Returns:
+        A string containing environment variables for hipcc.
+    """
+    hipcc_env = ""
+    for name in [
+        "HIP_CLANG_PATH",
+        "DEVICE_LIB_PATH",
+        "HIP_VDI_HOME",
+        "HIPCC_VERBOSE",
+        "HIPCC_COMPILE_FLAGS_APPEND",
+        "HIPPCC_LINK_FLAGS_APPEND",
+        "HCC_AMDGPU_TARGET",
+        "HIP_PLATFORM",
+    ]:
+        env_value = get_host_environ(repository_ctx, name)
+        if env_value:
+            hipcc_env = (hipcc_env + " " + name + "=\"" + env_value + "\";")
+    return hipcc_env.strip()
+
+def _crosstool_verbose(repository_ctx):
+    """Returns the environment variable value CROSSTOOL_VERBOSE.
+
+    Args:
+        repository_ctx: The repository context.
+
+    Returns:
+        A string containing value of environment variable CROSSTOOL_VERBOSE.
+    """
+    return get_host_environ(repository_ctx, "CROSSTOOL_VERBOSE", "0")
+
+def _lib_name(lib, version = "", static = False):
+    """Constructs the name of a library on Linux.
+
+    Args:
+      lib: The name of the library, such as "hip"
+      version: The version of the library.
+      static: True the library is static or False if it is a shared object.
+
+    Returns:
+      The platform-specific name of the library.
+    """
+    if static:
+        return "lib%s.a" % lib
+    else:
+        if version:
+            version = ".%s" % version
+        return "lib%s.so%s" % (lib, version)
+
+def _sycl_lib_paths(repository_ctx, lib, basedir):
+    file_name = _lib_name(lib, version = "", static = False)
+    return [
+        repository_ctx.path("%s/lib/%s" % (basedir, file_name)),
+        repository_ctx.path("%s/lib/intel64/%s" % (basedir, file_name)),
+    ]
+
+def _batch_files_exist(repository_ctx, libs_paths, bash_bin):
+    all_paths = []
+    for _, lib_paths in libs_paths:
+        for lib_path in lib_paths:
+            all_paths.append(lib_path)
+    return files_exist(repository_ctx, all_paths, bash_bin)
+
+def _select_sycl_lib_paths(repository_ctx, libs_paths, bash_bin):
+    test_results = _batch_files_exist(repository_ctx, libs_paths, bash_bin)
+
+    libs = {}
+    i = 0
+    for name, lib_paths in libs_paths:
+        selected_path = None
+        for path in lib_paths:
+            if test_results[i] and selected_path == None:
+                # For each lib select the first path that exists.
+                selected_path = path
+            i = i + 1
+        if selected_path == None:
+            auto_configure_fail("Cannot find sycl library %s" % name)
+
+        libs[name] = struct(file_name = selected_path.basename, path = realpath(repository_ctx, selected_path, bash_bin))
+
+    return libs
+
+def _find_libs(repository_ctx, sycl_config, bash_bin):
+    """Returns the SYCL libraries on the system.
+
+    Args:
+      repository_ctx: The repository context.
+      sycl_config: The SYCL config as returned by _get_sycl_config
+      bash_bin: the path to the bash interpreter
+
+    Returns:
+      Map of library names to structs of filename and path
+    """
+    libs_paths = [
+        (name, _sycl_lib_paths(repository_ctx, name, path))
+        for name, path in [
+            # ("sycl", sycl_config.sycl_toolkit_path + "/compiler/latest/linux/"),
+            ("mkl_sycl", sycl_config.sycl_toolkit_path + "/mkl/latest/"),
+            ("mkl_intel_ilp64", sycl_config.sycl_toolkit_path + "/mkl/latest/"),
+            ("mkl_sequential", sycl_config.sycl_toolkit_path + "/mkl/latest/"),
+            ("mkl_core", sycl_config.sycl_toolkit_path + "/mkl/latest/"),
+        ]
+    ]
+    return _select_sycl_lib_paths(repository_ctx, libs_paths, bash_bin)
+
+def _exec_find_sycl_config(repository_ctx, script_path):
+    python_bin = get_python_bin(repository_ctx)
+
+    # If used with remote execution then repository_ctx.execute() can't
+    # access files from the source tree. A trick is to read the contents
+    # of the file in Starlark and embed them as part of the command. In
+    # this case the trick is not sufficient as the find_cuda_config.py
+    # script has more than 8192 characters. 8192 is the command length
+    # limit of cmd.exe on Windows. Thus we additionally need to compress
+    # the contents locally and decompress them as part of the execute().
+    compressed_contents = repository_ctx.read(script_path)
+    decompress_and_execute_cmd = (
+        "from zlib import decompress;" +
+        "from base64 import b64decode;" +
+        "from os import system;" +
+        "script = decompress(b64decode('%s'));" % compressed_contents.rstrip('\n') +
+        "f = open('script.py', 'wb');" +
+        "f.write(script);" +
+        "f.close();" +
+        "system('\"%s\" script.py');" % (python_bin)
+    )
+    return execute(repository_ctx, [python_bin, "-c", decompress_and_execute_cmd])
+
+def find_sycl_config(repository_ctx, script_path):
+    """Returns SYCL config dictionary from running find_sycl_config.py"""
+    exec_result = _exec_find_sycl_config(repository_ctx, script_path)
+    if exec_result.return_code:
+        auto_configure_fail("Failed to run find_sycl_config.py: %s" % err_out(exec_result))
+
+    # Parse the dict from stdout.
+    return dict([tuple(x.split(": ")) for x in exec_result.stdout.splitlines()])
+
+def _get_sycl_config(repository_ctx, bash_bin, find_sycl_config_script):
+    """Detects and returns information about the SYCL installation on the system.
+
+    Args:
+      repository_ctx: The repository context.
+      bash_bin: the path to the path interpreter
+
+    Returns:
+      A struct containing the following fields:
+        sycl_toolkit_path: The SYCL toolkit installation directory.
+        amdgpu_targets: A list of the system's AMDGPU targets.
+        sycl_version_number: The version of SYCL on the system.
+        miopen_version_number: The version of MIOpen on the system.
+        hipruntime_version_number: The version of HIP Runtime on the system.
+    """
+    config = find_sycl_config(repository_ctx, find_sycl_config_script)
+    sycl_toolkit_path = config["sycl_toolkit_path"]
+    sycl_version_number = config["sycl_version_number"]
+    return struct(
+        sycl_toolkit_path = sycl_toolkit_path,
+        sycl_version_number = sycl_version_number,
+    )
+
+def _tpl_path(repository_ctx, labelname):
+    return repository_ctx.path(Label("//third_party/gpus/%s.tpl" % labelname))
+
+def _tpl(repository_ctx, tpl, substitutions = {}, out = None):
+    if not out:
+        out = tpl.replace(":", "/")
+    repository_ctx.template(
+        out,
+        _tpl_path(repository_ctx, tpl),
+        substitutions,
+    )
+
+_DUMMY_CROSSTOOL_BZL_FILE = """
+def error_gpu_disabled():
+  fail("ERROR: Building with --config=sycl but TensorFlow is not configured " +
+       "to build with GPU support. Please re-run ./configure and enter 'Y' " +
+       "at the prompt to build with GPU support.")
+
+  native.genrule(
+      name = "error_gen_crosstool",
+      outs = ["CROSSTOOL"],
+      cmd = "echo 'Should not be run.' && exit 1",
+  )
+
+  native.filegroup(
+      name = "crosstool",
+      srcs = [":CROSSTOOL"],
+      output_licenses = ["unencumbered"],
+  )
+"""
+
+_DUMMY_CROSSTOOL_BUILD_FILE = """
+load("//crosstool:error_gpu_disabled.bzl", "error_gpu_disabled")
+
+error_gpu_disabled()
+"""
+
+def _create_dummy_repository(repository_ctx):
+    # Set up BUILD file for sycl/.
+    _tpl(
+        repository_ctx,
+        "sycl:build_defs.bzl",
+        {
+            "%{sycl_is_configured}": "False",
+            "%{sycl_extra_copts}": "[]",
+            "%{sycl_gpu_architectures}": "[]",
+            "%{sycl_version_number}": "0",
+        },
+    )
+    _tpl(
+        repository_ctx,
+        "sycl:BUILD",
+        {
+            "%{sycl_lib}": _lib_name("sycl"),
+            "%{rocblas_lib}": _lib_name("rocblas"),
+            "%{hipblas_lib}": _lib_name("hipblas"),
+            "%{miopen_lib}": _lib_name("miopen"),
+            "%{rccl_lib}": _lib_name("rccl"),
+            "%{hipfft_or_rocfft}": _lib_name("hipfft"),
+            "%{hipfft_or_rocfft_lib}": _lib_name("hipfft"),
+            "%{hiprand_lib}": _lib_name("hiprand"),
+            "%{hipsparse_lib}": _lib_name("hipsparse"),
+            "%{roctracer_lib}": _lib_name("roctracer64"),
+            "%{rocsolver_lib}": _lib_name("rocsolver"),
+            "%{hipsolver_lib}": _lib_name("hipsolver"),
+            "%{copy_rules}": "",
+            "%{sycl_headers}": "",
+        },
+    )
+
+    # Create dummy files for the SYCL toolkit since they are still required by
+    # tensorflow/compiler/xla/stream_executor/sycl:sycl_rpath
+    repository_ctx.file("sycl/hip/include/hip/hip_runtime.h", "")
+
+    # # Set up sycl_config.h, which is used by
+    # # tensorflow/compiler/xla/stream_executor/dso_loader.cc.
+    # _tpl(
+    #     repository_ctx,
+    #     "sycl:sycl_config.h",
+    #     {
+    #         "%{sycl_toolkit_path}": _DEFAULT_SYCL_TOOLKIT_PATH,
+    #     },
+    #     "sycl/sycl/sycl_config.h",
+    # )
+
+    # If sycl_configure is not configured to build with GPU support, and the user
+    # attempts to build with --config=sycl, add a dummy build rule to intercept
+    # this and fail with an actionable error message.
+    repository_ctx.file(
+        "crosstool/error_gpu_disabled.bzl",
+        _DUMMY_CROSSTOOL_BZL_FILE,
+    )
+    repository_ctx.file("crosstool/BUILD", _DUMMY_CROSSTOOL_BUILD_FILE)
+
+def _norm_path(path):
+    """Returns a path with '/' and remove the trailing slash."""
+    path = path.replace("\\", "/")
+    if path[-1] == "/":
+        path = path[:-1]
+    return path
+
+def _genrule(src_dir, genrule_name, command, outs):
+    """Returns a string with a genrule.
+
+    Genrule executes the given command and produces the given outputs.
+    """
+    return (
+        "genrule(\n" +
+        '    name = "' +
+        genrule_name + '",\n' +
+        "    outs = [\n" +
+        outs +
+        "\n    ],\n" +
+        '    cmd = """\n' +
+        command +
+        '\n   """,\n' +
+        ")\n"
+    )
+
+def _compute_sycl_extra_copts(repository_ctx, amdgpu_targets):
+    amdgpu_target_flags = ["--amdgpu-target=" +
+                           amdgpu_target for amdgpu_target in amdgpu_targets]
+    return str(amdgpu_target_flags)
+
+def _create_local_sycl_repository(repository_ctx):
+    """Creates the repository containing files set up to build with SYCL."""
+
+    tpl_paths = {labelname: _tpl_path(repository_ctx, labelname) for labelname in [
+        "sycl:build_defs.bzl",
+        "sycl:BUILD",
+        # "sycl:sycl_config.h",
+    ]}
+
+    find_sycl_config_script = repository_ctx.path(Label("//third_party/gpus:find_sycl_config.py.gz.base64"))
+
+    bash_bin = get_bash_bin(repository_ctx)
+    sycl_config = _get_sycl_config(repository_ctx, bash_bin, find_sycl_config_script)
+
+    # For SYCL 4.1 and above use hipfft, older SYCL versions use rocfft
+    sycl_version_number = int(sycl_config.sycl_version_number)
+    # hipfft_or_rocfft = "rocfft" if sycl_version_number < 40100 else "hipfft"
+
+    # Copy header and library files to execroot.
+    # sycl_toolkit_path
+    sycl_toolkit_path = sycl_config.sycl_toolkit_path
+    copy_rules = [
+        make_copy_dir_rule(
+            repository_ctx,
+            name = "sycl-include",
+            src_dir = sycl_toolkit_path + "/include",
+            out_dir = "sycl/include",
+            exceptions = ["gtest", "gmock"],
+        ),
+    ]
+
+    sycl_libs = _find_libs(repository_ctx, sycl_config, bash_bin)
+    sycl_lib_srcs = []
+    sycl_lib_outs = []
+    for lib in sycl_libs.values():
+        sycl_lib_srcs.append(lib.path)
+        sycl_lib_outs.append("sycl/lib/" + lib.file_name)
+    copy_rules.append(make_copy_files_rule(
+        repository_ctx,
+        name = "sycl-lib",
+        srcs = sycl_lib_srcs,
+        outs = sycl_lib_outs,
+    ))
+
+    clang_offload_bundler_path = sycl_toolkit_path + "/llvm/bin/clang-offload-bundler"
+
+    # copy files mentioned in third_party/gpus/sycl/BUILD
+    copy_rules.append(make_copy_files_rule(
+        repository_ctx,
+        name = "sycl-bin",
+        srcs = [
+            clang_offload_bundler_path,
+        ],
+        outs = [
+            "sycl/bin/" + "clang-offload-bundler",
+        ],
+    ))
+
+    # Set up BUILD file for sycl/
+    repository_ctx.template(
+        "sycl/build_defs.bzl",
+        tpl_paths["sycl:build_defs.bzl"],
+        {
+            "%{sycl_is_configured}": "True",
+            # "%{sycl_extra_copts}": _compute_sycl_extra_copts(
+            #     repository_ctx,
+            #     sycl_config.amdgpu_targets,
+            # ),
+            # "%{sycl_gpu_architectures}": str(sycl_config.amdgpu_targets),
+            "%{sycl_version_number}": str(sycl_version_number),
+        },
+    )
+
+    repository_dict = {
+        # "%{sycl_lib}": sycl_libs["sycl"].file_name,
+        # "%{rocblas_lib}": sycl_libs["rocblas"].file_name,
+        # "%{hipfft_or_rocfft}": hipfft_or_rocfft,
+        # "%{hipfft_or_rocfft_lib}": sycl_libs[hipfft_or_rocfft].file_name,
+        # "%{hiprand_lib}": sycl_libs["hiprand"].file_name,
+        # "%{miopen_lib}": sycl_libs["MIOpen"].file_name,
+        # "%{rccl_lib}": sycl_libs["rccl"].file_name,
+        # "%{hipsparse_lib}": sycl_libs["hipsparse"].file_name,
+        # "%{roctracer_lib}": sycl_libs["roctracer64"].file_name,
+        # "%{rocsolver_lib}": sycl_libs["rocsolver"].file_name,
+        "%{copy_rules}": "\n".join(copy_rules),
+        "%{sycl_headers}": ('":sycl-include",\n'),
+    }
+
+    repository_ctx.template(
+        "sycl/BUILD",
+        tpl_paths["sycl:BUILD"],
+        repository_dict,
+    )
+
+    # Set up crosstool/
+
+    cc = find_cc(repository_ctx)
+
+    host_compiler_includes = get_cxx_inc_directories(repository_ctx, cc)
+
+    host_compiler_prefix = get_host_environ(repository_ctx, _GCC_HOST_COMPILER_PREFIX, "/usr/bin")
+
+    sycl_defines = {}
+
+    sycl_defines["%{host_compiler_prefix}"] = host_compiler_prefix
+
+    sycl_defines["%{linker_bin_path}"] = sycl_config.sycl_toolkit_path + "/hcc/compiler/bin"
+
+    # For gcc, do not canonicalize system header paths; some versions of gcc
+    # pick the shortest possible path for system includes when creating the
+    # .d file - given that includes that are prefixed with "../" multiple
+    # time quickly grow longer than the root of the tree, this can lead to
+    # bazel's header check failing.
+    sycl_defines["%{extra_no_canonical_prefixes_flags}"] = "\"-fno-canonical-system-headers\""
+
+    sycl_defines["%{unfiltered_compile_flags}"] = to_list_of_strings([
+        "-DTENSORFLOW_USE_SYCL=1",
+        "-D__HIP_PLATFORM_HCC__",
+        "-DEIGEN_USE_HIP",
+    ])
+
+    sycl_defines["%{host_compiler_path}"] = "clang/bin/crosstool_wrapper_driver_is_not_gcc"
+
+    sycl_defines["%{cxx_builtin_include_directories}"] = to_list_of_strings(
+        host_compiler_includes + _sycl_include_path(repository_ctx, sycl_config, bash_bin),
+    )
+
+    verify_build_defines(sycl_defines)
+
+def _create_remote_sycl_repository(repository_ctx, remote_config_repo):
+    """Creates pointers to a remotely configured repo set up to build with SYCL."""
+    _tpl(
+        repository_ctx,
+        "sycl:build_defs.bzl",
+        {
+            "%{sycl_is_configured}": "True",
+            "%{sycl_extra_copts}": _compute_sycl_extra_copts(
+                repository_ctx,
+                [],  #_compute_capabilities(repository_ctx)
+            ),
+        },
+    )
+    repository_ctx.template(
+        "sycl/BUILD",
+        config_repo_label(remote_config_repo, "sycl:BUILD"),
+        {},
+    )
+    repository_ctx.template(
+        "sycl/build_defs.bzl",
+        config_repo_label(remote_config_repo, "sycl:build_defs.bzl"),
+        {},
+    )
+    repository_ctx.template(
+        "sycl/sycl/sycl_config.h",
+        config_repo_label(remote_config_repo, "sycl:sycl/sycl_config.h"),
+        {},
+    )
+
+def _sycl_autoconf_impl(repository_ctx):
+    """Implementation of the sycl_autoconf repository rule."""
+    if not _enable_sycl(repository_ctx):
+        _create_dummy_repository(repository_ctx)
+    elif get_host_environ(repository_ctx, _TF_SYCL_CONFIG_REPO) != None:
+        _create_remote_sycl_repository(
+            repository_ctx,
+            get_host_environ(repository_ctx, _TF_SYCL_CONFIG_REPO),
+        )
+    else:
+        _create_local_sycl_repository(repository_ctx)
+
+_ENVIRONS = [
+    _GCC_HOST_COMPILER_PATH,
+    _GCC_HOST_COMPILER_PREFIX,
+    "TF_NEED_SYCL",
+    _SYCL_TOOLKIT_PATH,
+]
+
+remote_sycl_configure = repository_rule(
+    implementation = _create_local_sycl_repository,
+    environ = _ENVIRONS,
+    remotable = True,
+    attrs = {
+        "environ": attr.string_dict(),
+    },
+)
+
+sycl_configure = repository_rule(
+    implementation = _sycl_autoconf_impl,
+    environ = _ENVIRONS + [_TF_SYCL_CONFIG_REPO],
+)
+"""Detects and configures the local SYCL toolchain.
+
+Add the following to your WORKSPACE FILE:
+
+```python
+sycl_configure(name = "local_config_sycl")
+```
+
+Args:
+  name: A unique name for this workspace rule.
+"""
diff --git a/third_party/tsl/third_party/grpc/upb_platform_fix.patch b/third_party/tsl/third_party/grpc/upb_platform_fix.patch
index 6edd66067..022c9d155 100644
--- a/third_party/tsl/third_party/grpc/upb_platform_fix.patch
+++ b/third_party/tsl/third_party/grpc/upb_platform_fix.patch
@@ -11,3 +11,12 @@ index ad85b202..2311b2e4 100644
  )
 
  config_setting(
+@@ -24,7 +24,7 @@ exports_files([
+
+ CPPOPTS = [
+     # copybara:strip_for_google3_begin
+-    "-Werror",
++    # "-Werror",
+     "-Wno-long-long",
+     # copybara:strip_end
+ ]
diff --git a/third_party/tsl/third_party/llvm/build.patch b/third_party/tsl/third_party/llvm/build.patch
index bbf8f587a..ab0df933f 100644
--- a/third_party/tsl/third_party/llvm/build.patch
+++ b/third_party/tsl/third_party/llvm/build.patch
@@ -2,6 +2,14 @@ diff --git a/utils/bazel/llvm-project-overlay/llvm/BUILD.bazel b/utils/bazel/llv
 index 2b88729d748b..e12d979b4908 100644
 --- a/utils/bazel/llvm-project-overlay/llvm/BUILD.bazel
 +++ b/utils/bazel/llvm-project-overlay/llvm/BUILD.bazel
+@@ -25,6 +25,7 @@ exports_files(["LICENSE.TXT"])
+ # widely available feature to enable unlimited stack frame instead of using
+ # this `Make` variable.
+ llvm_copts = [
++    "-fvisibility=hidden",
+     "$(STACK_FRAME_UNLIMITED)",
+ ]
+
 @@ -207,13 +207,15 @@ cc_library(
          "lib/Support/BLAKE3/llvm_blake3_prefix.h",
      ] + select({
@@ -44,3 +52,24 @@ index 2b88729d748b..e12d979b4908 100644
          "//conditions:default": [
              "BLAKE3_NO_AVX2",
              "BLAKE3_NO_AVX512",
+diff --git a/mlir/lib/Bytecode/Reader/BytecodeReader.cpp b/mlir/lib/Bytecode/Reader/BytecodeReader.cpp
+index 177372c68046..40d49dc13b2f 100644
+--- a/mlir/lib/Bytecode/Reader/BytecodeReader.cpp
++++ b/mlir/lib/Bytecode/Reader/BytecodeReader.cpp
+@@ -1549,6 +1549,8 @@ private:
+   const std::shared_ptr<llvm::SourceMgr> &bufferOwnerRef;
+ };
+
++// TODO: Disable clang opt since it crashes with clang-17 compiler.
++#pragma clang optimize off
+ LogicalResult BytecodeReader::Impl::read(
+     Block *block, llvm::function_ref<bool(Operation *)> lazyOpsCallback) {
+   EncodingReader reader(buffer.getBuffer(), fileLoc);
+@@ -1628,6 +1630,7 @@ LogicalResult BytecodeReader::Impl::read(
+   // Finally, process the IR section.
+   return parseIRSection(*sectionDatas[bytecode::Section::kIR], block);
+ }
++#pragma clang optimize on
+
+ LogicalResult BytecodeReader::Impl::parseVersion(EncodingReader &reader) {
+   if (failed(reader.parseVarInt(version)))
diff --git a/third_party/tsl/third_party/llvm/spirv.patch b/third_party/tsl/third_party/llvm/spirv.patch
new file mode 100644
index 000000000..5b8db357e
--- /dev/null
+++ b/third_party/tsl/third_party/llvm/spirv.patch
@@ -0,0 +1,76 @@
+diff --git a/llvm/lib/Passes/PassBuilderPipelines.cpp b/llvm/lib/Passes/PassBuilderPipelines.cpp
+index 34d3b9d7467e..bd26295c6527 100644
+--- a/llvm/lib/Passes/PassBuilderPipelines.cpp
++++ b/llvm/lib/Passes/PassBuilderPipelines.cpp
+@@ -180,6 +180,10 @@ static cl::opt<bool> EnableGlobalAnalyses(
+     "enable-global-analyses", cl::init(true), cl::Hidden,
+     cl::desc("Enable inter-procedural analyses"));
+ 
++static cl::opt<bool>
++    SYCLOptimizationMode("sycl-opt", cl::init(false), cl::Hidden,
++                         cl::desc("Enable SYCL optimization mode."));
++
+ static cl::opt<bool>
+     RunPartialInlining("enable-partial-inlining", cl::init(false), cl::Hidden,
+                        cl::desc("Run Partial inlinining pass"));
+@@ -396,6 +400,7 @@ PassBuilder::buildO1FunctionSimplificationPipeline(OptimizationLevel Level,
+   // Form canonically associated expression trees, and simplify the trees using
+   // basic mathematical properties. For example, this will form (nearly)
+   // minimal multiplication trees.
++  if (!SYCLOptimizationMode) {
+   FPM.addPass(ReassociatePass());
+ 
+   // Add the primary loop simplification pipeline.
+@@ -467,7 +472,7 @@ PassBuilder::buildO1FunctionSimplificationPipeline(OptimizationLevel Level,
+   FPM.addPass(createFunctionToLoopPassAdaptor(std::move(LPM2),
+                                               /*UseMemorySSA=*/false,
+                                               /*UseBlockFrequencyInfo=*/false));
+-
++  }
+   // Delete small array after loop unroll.
+   FPM.addPass(SROAPass(SROAOptions::ModifyCFG));
+ 
+@@ -573,6 +578,7 @@ PassBuilder::buildFunctionSimplificationPipeline(OptimizationLevel Level,
+   // Form canonically associated expression trees, and simplify the trees using
+   // basic mathematical properties. For example, this will form (nearly)
+   // minimal multiplication trees.
++  if (!SYCLOptimizationMode) {
+   FPM.addPass(ReassociatePass());
+ 
+   // Add the primary loop simplification pipeline.
+@@ -647,7 +653,7 @@ PassBuilder::buildFunctionSimplificationPipeline(OptimizationLevel Level,
+   FPM.addPass(createFunctionToLoopPassAdaptor(std::move(LPM2),
+                                               /*UseMemorySSA=*/false,
+                                               /*UseBlockFrequencyInfo=*/false));
+-
++  }
+   // Delete small array after loop unroll.
+   FPM.addPass(SROAPass(SROAOptions::ModifyCFG));
+ 
+@@ -705,6 +711,9 @@ PassBuilder::buildFunctionSimplificationPipeline(OptimizationLevel Level,
+ 
+   invokeScalarOptimizerLateEPCallbacks(FPM, Level);
+ 
++  if (SYCLOptimizationMode)
++    FPM.addPass(SimplifyCFGPass());
++  else
+   FPM.addPass(SimplifyCFGPass(SimplifyCFGOptions()
+                                   .convertSwitchRangeToICmp(true)
+                                   .hoistCommonInsts(true)
+@@ -1338,6 +1347,7 @@ PassBuilder::buildModuleOptimizationPipeline(OptimizationLevel Level,
+ 
+   invokeVectorizerStartEPCallbacks(OptimizePM, Level);
+ 
++  if (!SYCLOptimizationMode) {
+   LoopPassManager LPM;
+   // First rotate loops that may have been un-rotated by prior passes.
+   // Disable header duplication at -Oz.
+@@ -1361,7 +1371,7 @@ PassBuilder::buildModuleOptimizationPipeline(OptimizationLevel Level,
+   OptimizePM.addPass(InjectTLIMappings());
+ 
+   addVectorPasses(Level, OptimizePM, /* IsFullLTO */ false);
+-
++  }
+   // LoopSink pass sinks instructions hoisted by LICM, which serves as a
+   // canonicalization pass that enables other optimizations. As a result,
+   // LoopSink pass needs to be a very late IR pass to avoid undoing LICM
diff --git a/third_party/tsl/third_party/llvm/workspace.bzl b/third_party/tsl/third_party/llvm/workspace.bzl
index 0d5a44fe7..d1468201f 100644
--- a/third_party/tsl/third_party/llvm/workspace.bzl
+++ b/third_party/tsl/third_party/llvm/workspace.bzl
@@ -17,6 +17,7 @@ def repo(name):
         ],
         build_file = "//third_party/llvm:llvm.BUILD",
         patch_file = [
+            "//third_party/llvm:spirv.patch",
             "//third_party/llvm:generated.patch",  # Autogenerated, don't remove.
             "//third_party/llvm:build.patch",
             "//third_party/llvm:mathextras.patch",
diff --git a/third_party/tsl/tsl/framework/contraction/BUILD b/third_party/tsl/tsl/framework/contraction/BUILD
index 980ca390d..2374e6d10 100644
--- a/third_party/tsl/tsl/framework/contraction/BUILD
+++ b/third_party/tsl/tsl/framework/contraction/BUILD
@@ -104,7 +104,6 @@ cc_library(
         "//tsl:macos_arm64": [],
         "//conditions:default": [
             "TENSORFLOW_USE_CUSTOM_CONTRACTION_KERNEL",
-            "TENSORFLOW_USE_MKLDNN_CONTRACTION_KERNEL",
         ],
     }),
     deps = [
@@ -121,7 +120,7 @@ cc_library(
         "//tsl:linux_ppc64le": [],
         "//tsl:linux_s390x": [],
         "//tsl:macos_arm64": [],
-        "//conditions:default": ["@mkl_dnn_v1//:mkl_dnn"],
+        "//conditions:default": [],
     }),
 )
 
diff --git a/third_party/tsl/workspace2.bzl b/third_party/tsl/workspace2.bzl
index b18535eea..0a944da74 100644
--- a/third_party/tsl/workspace2.bzl
+++ b/third_party/tsl/workspace2.bzl
@@ -4,6 +4,7 @@
 load("@bazel_skylib//lib:versions.bzl", "versions")
 load("//third_party/gpus:cuda_configure.bzl", "cuda_configure")
 load("//third_party/gpus:rocm_configure.bzl", "rocm_configure")
+load("//third_party/gpus:sycl_configure.bzl", "sycl_configure")
 load("//third_party/tensorrt:tensorrt_configure.bzl", "tensorrt_configure")
 load("//third_party/nccl:nccl_configure.bzl", "nccl_configure")
 load("//third_party/git:git_configure.bzl", "git_configure")
@@ -74,6 +75,7 @@ def _tf_toolchains():
     syslibs_configure(name = "local_config_syslibs")
     python_configure(name = "local_config_python")
     rocm_configure(name = "local_config_rocm")
+    sycl_configure(name = "local_config_sycl")
     remote_execution_configure(name = "local_config_remote_execution")
 
     # For windows bazel build
diff --git a/xla/mlir_hlo/lhlo_gpu/IR/lhlo_gpu_ops.td b/xla/mlir_hlo/lhlo_gpu/IR/lhlo_gpu_ops.td
index 72fd56b2a..87451f0c5 100644
--- a/xla/mlir_hlo/lhlo_gpu/IR/lhlo_gpu_ops.td
+++ b/xla/mlir_hlo/lhlo_gpu/IR/lhlo_gpu_ops.td
@@ -306,6 +306,16 @@ def LHLOGPU_AllToAllDoneOp: LHLOGPU_Op<"all_to_all_done"> {
   let arguments = (ins MHLO_Token:$token);
 }
 
+def LHLOGPU_fusedQKVOp : LHLOGPU_Op<"fQKV"> {
+  let arguments = (ins
+    Arg<LHLO_Buffer, "", [MemRead]>:$input,
+    Arg<LHLO_Buffer, "", [MemRead]>:$weight,
+    Arg<LHLO_Buffer, "", [MemWrite]>:$bmm_output1,
+    Arg<LHLO_Buffer, "", [MemWrite]>:$bmm_output2,
+    Arg<LHLO_Buffer, "", [MemWrite]>:$bmm_output3,
+    MHLO_DotDimensionNumbers:$dot_dimension_numbers);
+}
+
 def LHLOGPU_fusedMHAOp : LHLOGPU_Op<"fMHA"> {
   let arguments = (ins
     Arg<LHLO_Buffer, "", [MemRead]>:$lhs_bmm1,
@@ -317,6 +327,7 @@ def LHLOGPU_fusedMHAOp : LHLOGPU_Op<"fMHA"> {
     MHLO_DotDimensionNumbers:$bmm2_dot_dimension_numbers,
     I64ArrayAttr:$intermediate_tensor_dimensions,
     I64ArrayAttr:$intermediate_tensor_layout,
+    F64Attr:$fmha_scale,
     OptionalAttr<F64Attr>:$dropout_rate,
     FusedMhaDagSignatureAttr:$fused_mha_dag,
     FusedMHAAlgorithmConfigAttr:$algorithm_config,
diff --git a/xla/mlir_hlo/lhlo_gpu/IR/lhlo_gpu_ops_enums.td b/xla/mlir_hlo/lhlo_gpu/IR/lhlo_gpu_ops_enums.td
index 2a527ac55..94e0d78be 100644
--- a/xla/mlir_hlo/lhlo_gpu/IR/lhlo_gpu_ops_enums.td
+++ b/xla/mlir_hlo/lhlo_gpu/IR/lhlo_gpu_ops_enums.td
@@ -136,6 +136,7 @@ def FusedMhaDagSoftmaxDropout : I32EnumAttrCase<"SoftmaxDropout", 5>;
 def FusedMhaDagSoftmax : I32EnumAttrCase<"Softmax", 6>;
 def FusedMhaDagScaleBiasSoftmaxDropout : I32EnumAttrCase<"ScaleBiasSoftmaxDropout", 7>;
 def FusedMhaDagScaleBiasSoftmax : I32EnumAttrCase<"ScaleBiasSoftmax", 8>;
+def FusedMhaDagScaleSoftmax : I32EnumAttrCase<"ScaleSoftmax", 9>;
 
 def FusedMhaDagSignature: I32EnumAttr<"FusedMhaDagSignature",
     "DAG configuration for Fused Multi-Headed Attention",
@@ -147,7 +148,8 @@ def FusedMhaDagSignature: I32EnumAttr<"FusedMhaDagSignature",
     FusedMhaDagSoftmaxDropout,
     FusedMhaDagSoftmax,
     FusedMhaDagScaleBiasSoftmaxDropout,
-    FusedMhaDagScaleBiasSoftmax]> {
+    FusedMhaDagScaleBiasSoftmax,
+    FusedMhaDagScaleSoftmax]> {
   let genSpecializedAttr = 0;
   let cppNamespace = "::mlir::lmhlo_gpu";
 }
diff --git a/xla/pjrt/gpu/gpu_helpers.cc b/xla/pjrt/gpu/gpu_helpers.cc
index b7064ac25..dcb1efbd8 100644
--- a/xla/pjrt/gpu/gpu_helpers.cc
+++ b/xla/pjrt/gpu/gpu_helpers.cc
@@ -37,11 +37,12 @@ namespace xla {
 StatusOr<LocalClient*> GetGpuXlaClient(
     const std::optional<std::string>& platform_name,
     const std::optional<std::set<int>>& allowed_devices) {
+  // SYCL: hardcode to xpu
   TF_ASSIGN_OR_RETURN(
       se::Platform * platform,
-      PlatformUtil::GetPlatform(platform_name ? *platform_name : "gpu"));
+      PlatformUtil::GetPlatform(platform_name ? *platform_name : "xpu"));
   if (platform->VisibleDeviceCount() <= 0) {
-    return FailedPrecondition("No visible GPU devices.");
+    return FailedPrecondition("No visible XPU devices.");
   }
   LocalClientOptions options;
   options.set_platform(platform);
@@ -113,7 +114,8 @@ StatusOr<std::unique_ptr<tsl::BFCAllocator>> CreateBFCAllocator(
   opts.allow_growth = !preallocate;
   return std::make_unique<tsl::BFCAllocator>(
       std::move(sub_allocator), allocator_memory,
-      absl::StrCat("GPU_", device_ordinal, "_bfc"), opts);
+      // SYCL: hardcode to xpu
+      absl::StrCat("XPU_", device_ordinal, "_bfc"), opts);
 }
 
 // Returns a GPU pinned host memory allocator to use when staging host->GPU
@@ -131,7 +133,8 @@ std::unique_ptr<tsl::BFCAllocator> GetGpuHostAllocator(
   opts.allow_growth = true;
   return std::make_unique<tsl::BFCAllocator>(std::move(sub_allocator),
                                              kGpuHostMemoryLimitBytes,
-                                             /*name=*/"xla_gpu_host_bfc", opts);
+                                             // SYCL: hardcode to xpu
+                                             /*name=*/"xla_xpu_host_bfc", opts);
 }
 
 }  // namespace xla
diff --git a/xla/service/algebraic_simplifier.h b/xla/service/algebraic_simplifier.h
index 32be0a59d..bde8901d2 100644
--- a/xla/service/algebraic_simplifier.h
+++ b/xla/service/algebraic_simplifier.h
@@ -27,6 +27,7 @@ limitations under the License.
 #include <vector>
 
 #include "absl/container/inlined_vector.h"
+#include "tsl/util/env_var.h"
 #include "xla/hlo/ir/dfs_hlo_visitor_with_default.h"
 #include "xla/hlo/ir/hlo_instruction.h"
 #include "xla/hlo/ir/hlo_module.h"
@@ -400,7 +401,9 @@ class AlgebraicSimplifierVisitor : public DfsHloRewriteVisitor {
   virtual bool IsValidLayout(const Shape& shape) { return true; }
   // Allow backend targets to determine whether a layout is inefficient.
   virtual bool ShouldStrengthReduceDotToReduce(const HloInstruction* hlo) {
-    return true;
+    bool llm_flag = false;
+    tsl::ReadBoolFromEnvVar("LLM", false, &llm_flag);
+    return !llm_flag;
   }
 
  protected:
diff --git a/xla/service/dump.cc b/xla/service/dump.cc
index ad67a9e0f..dd6687d38 100644
--- a/xla/service/dump.cc
+++ b/xla/service/dump.cc
@@ -615,6 +615,8 @@ void DumpToFileInDirOrStdout(const HloModule& module, string_view file_prefix,
   if (opts.dumping_to_stdout()) return op->dump();
 
   mlir::OpPrintingFlags print_flags = mlir::OpPrintingFlags().useLocalScope();
+  // Avoid printing large constant weight.
+  print_flags.elideLargeElementsAttrs(8);
   // Enable debug info so that it is easier to see the corresponding HLO node.
   if (file_prefix == "lmhlo") {
     print_flags.enableDebugInfo(/*enable=*/true,
diff --git a/xla/service/gpu/cublas_cudnn.cc b/xla/service/gpu/cublas_cudnn.cc
index 240e69ac4..65ed0bd9b 100644
--- a/xla/service/gpu/cublas_cudnn.cc
+++ b/xla/service/gpu/cublas_cudnn.cc
@@ -39,6 +39,7 @@ bool IsCublasLtMatmulF8(const HloInstruction& hlo) {
          hlo.custom_call_target() == kCublasLtMatmulF8CallTarget;
 }
 
+const absl::string_view kCudnnfQKVCallTarget = "__cudnn$fQKV";
 const absl::string_view kGemmCallTarget = "__cublas$gemm";
 const absl::string_view kCublasLtMatmulCallTarget = "__cublas$lt$matmul";
 const absl::string_view kCublasLtMatmulF8CallTarget = "__cublas$lt$matmul$f8";
@@ -65,6 +66,8 @@ const absl::string_view kCudnnfMHAScaleBiasMaskSoftmaxDropoutCallTarget =
     "__cudnn$fhmaScaleBiasMaskSoftmaxDropout";
 const absl::string_view kCudnnfMHAScaleBiasSoftmaxDropoutCallTarget =
     "__cudnn$fhmaScaleBiasSoftmaxDropout";
+const absl::string_view kCudnnfMHAScaleSoftmaxCallTarget =
+    "__cudnn$fmhaScaleSoftmax";
 const absl::string_view kCudnnfMHAScaleBiasSoftmaxCallTarget =
     "__cudnn$fhmaScaleBiasSoftmax";
 const absl::string_view kCudnnfMHAScaleMaskSoftmaxCallTarget =
@@ -94,6 +97,11 @@ bool IsCudnnConvolutionReorder(const HloInstruction& hlo) {
          target == kCudnnConvReorderFilterAndBiasCallTarget;
 }
 
+bool IsCustomCallTofQKV(const HloInstruction& hlo) {
+  const auto& target = hlo.custom_call_target();
+  return target == kCudnnfQKVCallTarget;
+}
+
 bool IsCustomCallTofMHA(const HloInstruction& hlo) {
   const auto& target = hlo.custom_call_target();
   return target == kCudnnfMHABmmBmmCallTarget ||
@@ -102,6 +110,7 @@ bool IsCustomCallTofMHA(const HloInstruction& hlo) {
          target == kCudnnfMHAScaleBiasMaskSoftmaxDropoutCallTarget ||
          target == kCudnnfMHAScaleMaskSoftmaxCallTarget ||
          target == kCudnnfMHAScaleMaskSoftmaxDropoutCallTarget ||
+         target == kCudnnfMHAScaleSoftmaxCallTarget ||
          target == kCudnnfMHASoftmaxDropoutCallTarget ||
          target == kCudnnfMHAScaleBiasSoftmaxCallTarget ||
          target == kCudnnfMHAScaleBiasSoftmaxDropoutCallTarget;
@@ -150,6 +159,8 @@ StatusOr<CudnnfMHAKind> GetCudnnfMHAKind(
     return CudnnfMHAKind::kScaleMaskSoftmax;
   if (target == kCudnnfMHAScaleMaskSoftmaxDropoutCallTarget)
     return CudnnfMHAKind::kScaleMaskSoftmaxDropout;
+  if (target == kCudnnfMHAScaleSoftmaxCallTarget)
+    return CudnnfMHAKind::kScaleSoftmax;
   if (target == kCudnnfMHASoftmaxDropoutCallTarget)
     return CudnnfMHAKind::kSoftmaxDropout;
   if (target == kCudnnfMHASoftmaxCallTarget) return CudnnfMHAKind::kSoftmax;
@@ -181,6 +192,8 @@ std::string CudnnfMHAKindToString(CudnnfMHAKind kind) {
       return "fmha_bias_softmax_with_dropout";
     case CudnnfMHAKind::kScaleBiasSoftmax:
       return "fmha_bias_softmax";
+    case CudnnfMHAKind::kScaleSoftmax:
+      return "fmha_scale_softmax";
   }
 }
 
diff --git a/xla/service/gpu/cublas_cudnn.h b/xla/service/gpu/cublas_cudnn.h
index 50eddecea..fff26fb91 100644
--- a/xla/service/gpu/cublas_cudnn.h
+++ b/xla/service/gpu/cublas_cudnn.h
@@ -53,6 +53,7 @@ enum class CudnnfMHAKind {
   kScaleMaskSoftmaxDropout,
   kSoftmaxDropout,
   kSoftmax,
+  kScaleSoftmax,
   kScaleBiasSoftmax,
   kScaleBiasSoftmaxDropout,
 };
@@ -152,6 +153,7 @@ bool IsCudnnConvolutionReorder(const HloInstruction& hlo);
 // 6. BMM1 - Softmax - Dropout - BMM2
 // 7. BMM1 - Softmax - BMM2
 // 8. BMM1 - scale - Bias - Softmax - BMM2
+extern const absl::string_view kCudnnfQKVCallTarget;
 extern const absl::string_view kCudnnfMHABmmBmmCallTarget;
 extern const absl::string_view kCudnnfMHASoftmaxCallTarget;
 extern const absl::string_view kCudnnfMHAScaleBiasMaskSoftmaxCallTarget;
@@ -161,9 +163,12 @@ extern const absl::string_view kCudnnfMHAScaleMaskSoftmaxDropoutCallTarget;
 extern const absl::string_view kCudnnfMHASoftmaxDropoutCallTarget;
 extern const absl::string_view kCudnnfMHAScaleBiasSoftmaxDropoutCallTarget;
 extern const absl::string_view kCudnnfMHAScaleBiasSoftmaxCallTarget;
+extern const absl::string_view kCudnnfMHAScaleSoftmaxCallTarget;
 
 bool IsCustomCallTofMHA(const HloInstruction& hlo);
 
+bool IsCustomCallTofQKV(const HloInstruction& hlo);
+
 StatusOr<CudnnfMHAKind> GetCudnnfMHAKind(const HloCustomCallInstruction* instr);
 
 std::string CudnnfMHAKindToString(CudnnfMHAKind kind);
diff --git a/xla/service/gpu/gpu_layout_assignment.cc b/xla/service/gpu/gpu_layout_assignment.cc
index 590f5ff41..6dcb1939b 100644
--- a/xla/service/gpu/gpu_layout_assignment.cc
+++ b/xla/service/gpu/gpu_layout_assignment.cc
@@ -66,7 +66,7 @@ HeuristicLayoutAssignment(const HloInstruction* instr,
       std::make_tuple(DataLayout::kBatchDepthYX4, FilterLayout::kOutputInputYX4,
                       DataLayout::kBatchDepthYX4);
   constexpr auto kAllNHWC =
-      std::make_tuple(DataLayout::kBatchYXDepth, FilterLayout::kOutputYXInput,
+      std::make_tuple(DataLayout::kBatchYXDepth, FilterLayout::kYXInputOutput,
                       DataLayout::kBatchYXDepth);
 
   // Integer convolution must use NHWC or NCHW_VECT_C.
diff --git a/xla/service/gpu/ir_emission_utils.cc b/xla/service/gpu/ir_emission_utils.cc
index 5603f6b87..9a78219a3 100644
--- a/xla/service/gpu/ir_emission_utils.cc
+++ b/xla/service/gpu/ir_emission_utils.cc
@@ -107,11 +107,11 @@ bool IsMatrixMultiplication(const HloInstruction& dot) {
   const DotDimensionNumbers& dim_numbers = dot.dot_dimension_numbers();
 
   PrimitiveType output_primitive_type = dot.shape().element_type();
+  // Disable F64, C64, C128
   bool type_is_allowed =
       (output_primitive_type == F8E4M3FN || output_primitive_type == F8E5M2 ||
        output_primitive_type == F16 || output_primitive_type == BF16 ||
-       output_primitive_type == F32 || output_primitive_type == F64 ||
-       output_primitive_type == C64 || output_primitive_type == C128) ||
+       output_primitive_type == F32) ||
       (output_primitive_type == S32 && lhs_shape.element_type() == S8 &&
        rhs_shape.element_type() == S8);
   bool shapes_are_valid =
@@ -365,6 +365,31 @@ llvm::Value* EmitNVPTXShflDown(llvm::Value* value, llvm::Value* offset,
       intrinsic, {b->getInt32(-1), value, offset, b->getInt32(WarpSize() - 1)});
 }
 
+// Helper function to emit call to NVPTX shfl_down intrinsic.
+llvm::Value* EmitSPIRShflDown(llvm::Value* value, llvm::Value* offset,
+                              llvm::IRBuilder<>* b) {
+  llvm::Module* module = b->GetInsertBlock()->getModule();
+  llvm::Intrinsic::ID llvm_intrinsic_id;
+  CHECK_EQ(value->getType()->getPrimitiveSizeInBits(), 32);
+  if (value->getType()->isFloatTy()) {
+    return EmitDeviceFunctionCall(
+        "__builtin_spirv_OpSubgroupShuffleDownINTEL_f32_f32_i32",
+        {value, value, offset}, {F32, F32, U32}, F32,
+        llvm::AttrBuilder(b->getContext())
+            .addAttribute(llvm::Attribute::NoUnwind)
+            .addAttribute(llvm::Attribute::Convergent),
+        b);
+  } else {
+    return EmitDeviceFunctionCall(
+        "__builtin_spirv_OpSubgroupShuffleDownINTEL_i32_i32_i32",
+        {value, value, offset}, {U32, U32, U32}, U32,
+        llvm::AttrBuilder(b->getContext())
+            .addAttribute(llvm::Attribute::NoUnwind)
+            .addAttribute(llvm::Attribute::Convergent),
+        b);
+  }
+}
+
 llvm::Value* EmitFullWarpShuffleDown(llvm::Value* value, llvm::Value* offset,
                                      llvm::IRBuilder<>* builder) {
   int bit_width = value->getType()->getPrimitiveSizeInBits();
@@ -377,6 +402,8 @@ llvm::Value* EmitFullWarpShuffleDown(llvm::Value* value, llvm::Value* offset,
       return EmitNVPTXShflDown(value, offset, builder);
     } else if (target_triple.getArch() == llvm::Triple::amdgcn) {
       return EmitAMDGPUShflDown(value, offset, builder);
+    } else if (target_triple.isSPIR()) {
+      return EmitSPIRShflDown(value, offset, builder);
     } else {
       LOG(FATAL) << "Invalid triple " << target_triple.str();
     }
@@ -398,6 +425,9 @@ llvm::Value* EmitFullWarpShuffleDown(llvm::Value* value, llvm::Value* offset,
     } else if (target_triple.getArch() == llvm::Triple::amdgcn) {
       insert_val = EmitAMDGPUShflDown(builder->CreateExtractElement(x, i),
                                       offset, builder);
+    } else if (target_triple.isSPIR()) {
+      insert_val = EmitSPIRShflDown(builder->CreateExtractElement(x, i), offset,
+                                    builder);
     } else {
       LOG(FATAL) << "Invalid triple " << target_triple.str();
     }
diff --git a/xla/service/gpu/matmul_utils.h b/xla/service/gpu/matmul_utils.h
index c46ba6922..dedac9e37 100644
--- a/xla/service/gpu/matmul_utils.h
+++ b/xla/service/gpu/matmul_utils.h
@@ -37,6 +37,23 @@ limitations under the License.
 #include "xla/stream_executor/scratch_allocator.h"
 #endif  // GOOGLE_CUDA
 
+namespace stream_executor {
+namespace cuda {
+namespace BlasLt {
+enum class Epilogue {
+  kDefault = 1,                   // No special postprocessing
+  kReLU = 2,                      // Apply point-wise ReLU function
+  kBias = 4,                      // Add broadcasted bias vector
+  kBiasThenReLU = kBias | kReLU,  // Apply bias and then ReLU transform
+  kGELU = 32,                // Apply GELU point-wise transform to the results
+  kGELUWithAux = 32 | 1024,  // Apply GELU with auxiliary output.
+  kBiasThenGELU = kBias | kGELU,  // Apply bias and then approximate GELU.
+  kBiasThenGELUWithAux = kBiasThenGELU | 1024,
+};
+}
+}  // namespace cuda
+}  // namespace stream_executor
+
 namespace xla {
 namespace gpu {
 
@@ -119,6 +136,7 @@ struct GemmConfig {
   double beta;
   std::optional<int64_t> algorithm;
   int64_t compute_precision;
+  se::cuda::BlasLt::Epilogue epilogue;
 };
 
 StatusOr<se::blas::ComputationType> GetBlasComputationType(
diff --git a/xla/service/gpu/parallel_loop_emitter.cc b/xla/service/gpu/parallel_loop_emitter.cc
index 7577f166f..857f3cf4e 100644
--- a/xla/service/gpu/parallel_loop_emitter.cc
+++ b/xla/service/gpu/parallel_loop_emitter.cc
@@ -178,9 +178,7 @@ ParallelLoopEmitter::EmitIndexAndSetExitBasicBlock(absl::string_view loop_name,
   // for that dimensions.  This helps LLVM generate vectorized codes
   // in that cases.
   llvm::Value* row_index = nullptr;
-  if (!launch_config_.row_vectorized) {
-    array_indices.emplace_back(linear_index_base, shape_, b_);
-  } else {
+  if (launch_config_.row_vectorized) {
     // Simpler index for row computation.
     // This will allow LLVM to vectorize.
     row_index = b_->CreateMul(
@@ -192,19 +190,19 @@ ParallelLoopEmitter::EmitIndexAndSetExitBasicBlock(absl::string_view loop_name,
     array_indices.emplace_back(linear_index_base, multidim, shape_, b_);
   }
 
-  for (int i = 1; i < launch_config_.unroll_factor; ++i) {
+  for (int i = 0; i < launch_config_.unroll_factor; ++i) {
     llvm::Value* linear_index =
         b_->CreateAdd(linear_index_base, llvm::ConstantInt::get(index_type, i),
                       absl::StrCat("linear_index", i),
                       /*HasNUW=*/true, /*HasNSW=*/true);
-    if (!launch_config_.row_vectorized) {
-      array_indices.emplace_back(linear_index, shape_, b_);
-    } else {
+    if (launch_config_.row_vectorized && i > 0) {
       std::vector<llvm::Value*> multidim(shape_.rank(), nullptr);
       multidim.back() = b_->CreateAdd(
           row_index, llvm::ConstantInt::get(index_type, i),
           absl::StrCat("row_index_plus", i), /*HasNUW=*/true, /*HasNSW=*/true);
       array_indices.emplace_back(linear_index, multidim, shape_, b_);
+    } else {
+      array_indices.emplace_back(linear_index, shape_, b_);
     }
   }
 
diff --git a/xla/service/gpu/stream_executor_util.cc b/xla/service/gpu/stream_executor_util.cc
index fb042054f..04121065f 100644
--- a/xla/service/gpu/stream_executor_util.cc
+++ b/xla/service/gpu/stream_executor_util.cc
@@ -140,6 +140,13 @@ StreamExecutorConvLayoutsToXlaLayouts(const ConvolutionDimensionNumbers& dnums,
                            dnums.kernel_spatial_dimensions().end());
       filter_layout.push_back(dnums.kernel_input_feature_dimension());
       break;
+    case FilterLayout::kYXInputOutput:  // HWIO
+      filter_layout.insert(filter_layout.end(),
+                           dnums.kernel_spatial_dimensions().begin(),
+                           dnums.kernel_spatial_dimensions().end());
+      filter_layout.push_back(dnums.kernel_input_feature_dimension());
+      filter_layout.push_back(dnums.kernel_output_feature_dimension());
+      break;
     default:
       return InternalError("Invalid filter layout %s for conv with dnums %s,",
                            FilterLayoutString(filter),
@@ -177,7 +184,7 @@ XlaConvShapesToStreamExecutorLayouts(const ConvolutionDimensionNumbers& dnums,
   Layout nhwc_input, nhwc_filter, nhwc_output;
   std::tie(nhwc_input, nhwc_filter, nhwc_output) =
       StreamExecutorConvLayoutsToXlaLayouts(dnums, DataLayout::kBatchYXDepth,
-                                            FilterLayout::kOutputYXInput,
+                                            FilterLayout::kYXInputOutput,
                                             DataLayout::kBatchYXDepth)
           .value();
 
@@ -226,7 +233,7 @@ XlaConvShapesToStreamExecutorLayouts(const ConvolutionDimensionNumbers& dnums,
           ConvolutionDimensionNumbersToString(dnums), vect_size);
     }
   } else if (LayoutUtil::Equal(filter.layout(), nhwc_filter)) {
-    filter_layout = FilterLayout::kOutputYXInput;
+    filter_layout = FilterLayout::kYXInputOutput;
   } else {
     return InternalError(
         "Invalid filter layout %s for conv with dnums %s, expected one of (%s, "
@@ -329,7 +336,8 @@ StatusOr<std::unique_ptr<se::KernelBase>> CreateKernel(
 
   if (!cubin_data.empty()) {
     loader_spec.AddCudaCubinInMemory(
-        reinterpret_cast<const char*>(cubin_data.data()), kernel_name);
+        reinterpret_cast<const char*>(cubin_data.data()), cubin_data.size(),
+        kernel_name);
   }
 
   auto kernel_base = std::make_unique<se::KernelBase>(stream_exec);
diff --git a/xla/service/gpu/stream_executor_util.h b/xla/service/gpu/stream_executor_util.h
index 4ba827110..0c2167edd 100644
--- a/xla/service/gpu/stream_executor_util.h
+++ b/xla/service/gpu/stream_executor_util.h
@@ -106,6 +106,8 @@ StatusOr<se::dnn::FusedMHAKind> GetDNNFusedMHAKindFromCudnnfMHAKind(
     CudnnfMHAKind kind);
 
 StatusOr<se::dnn::DataType> GetDNNDataTypeFromPrimitiveType(PrimitiveType type);
+StatusOr<se::dnn::FusedMHAKind> GetDNNFusedMHAKindFromCudnnfMHAKind(
+    CudnnfMHAKind kind);
 
 // Returns result with the smallest time which has not failed.
 // If deterministic output is requested, returns first (not failing) result.
diff --git a/xla/service/gpu/target_constants.h b/xla/service/gpu/target_constants.h
index 92f31a6c1..ff12c19d9 100644
--- a/xla/service/gpu/target_constants.h
+++ b/xla/service/gpu/target_constants.h
@@ -54,6 +54,28 @@ inline const char* DataLayout() {
 
 }  // namespace amdgpu
 
+namespace spir {
+// SYCL: The triple that represents our target on SPIR backend.
+inline const char* TargetTriple() {
+  static constexpr char kTargetTriple[] = "spir64-unknown-unknown";
+  return kTargetTriple;
+}
+
+// The data layout of the emitted module.
+inline const char* DataLayout() {
+  // Specifies the address space as global address space
+  // A1: Specifies the address space of objects created by ‘alloca’.
+  // P1: Specifies the address space that corresponds to program memory.
+  // G1: Specifies the address space of global variables.
+  static constexpr char kDataLayout[] =
+      "e-p:64:64:64-i1:8:8-i8:8:8-i16:16:16-i32:32:32-i64:64:64-f32:"
+      "32:32-f64:64:64-v16:16:16-v24:32:32-v32:32:32-v48:64:64-v64:64:64-v96:"
+      "128:128-v128:128:128-v192:256:256-v256:256:256-v512:512:512-v1024:1024:"
+      "1024";
+  return kDataLayout;
+}
+}  // namespace spir
+
 }  // namespace gpu
 }  // namespace xla
 
diff --git a/xla/service/gpu/target_util.cc b/xla/service/gpu/target_util.cc
index 97ca63888..d7fd1fb11 100644
--- a/xla/service/gpu/target_util.cc
+++ b/xla/service/gpu/target_util.cc
@@ -23,12 +23,12 @@ limitations under the License.
 #include "llvm/IR/IntrinsicsAMDGPU.h"
 #include "llvm/IR/IntrinsicsNVPTX.h"
 #include "llvm/IR/MDBuilder.h"
+#include "tsl/platform/logging.h"
 #include "xla/hlo/ir/hlo_opcode.h"
 #include "xla/primitive_util.h"
 #include "xla/service/llvm_ir/llvm_type_conversion_util.h"
 #include "xla/service/llvm_ir/llvm_util.h"
 #include "xla/status.h"
-#include "tsl/platform/logging.h"
 
 namespace xla {
 namespace gpu {
@@ -45,6 +45,10 @@ struct TargetIntrinsics {
   std::variant<llvm::Intrinsic::ID,
                std::function<llvm::CallInst*(llvm::IRBuilder<>*)>>
       amdgpu_intrinsic_or_function;
+  // SYCL: Target for SPIRV.
+  absl::variant<llvm::Intrinsic::ID,
+                std::function<llvm::CallInst*(llvm::IRBuilder<>*)>>
+      spir_intrinsic_or_function;
 };
 
 // Gets the llvm intrinsic ids on different platforms (NVPTX, AMDGPU)
@@ -52,32 +56,82 @@ struct TargetIntrinsics {
 struct TargetIntrinsics GetIntrinsic(TargetIntrinsicID intrin) {
   switch (intrin) {
     case TargetIntrinsicID::kThreadIdx: {
-      return {llvm::Intrinsic::nvvm_read_ptx_sreg_tid_x,
-              llvm::Intrinsic::amdgcn_workitem_id_x};
+      return {
+          llvm::Intrinsic::nvvm_read_ptx_sreg_tid_x,
+          llvm::Intrinsic::amdgcn_workitem_id_x,
+          [](llvm::IRBuilder<>* b_) -> llvm::CallInst* {
+            return EmitDeviceFunctionCall(
+                "_Z32__spirv_BuiltInLocalInvocationIdi", {b_->getInt32(0)},
+                {U32}, U64, {b_->getContext()}, b_);
+          },
+      };
     }
     case TargetIntrinsicID::kThreadIdy: {
-      return {llvm::Intrinsic::nvvm_read_ptx_sreg_tid_y,
-              llvm::Intrinsic::amdgcn_workitem_id_y};
+      return {
+          llvm::Intrinsic::nvvm_read_ptx_sreg_tid_y,
+          llvm::Intrinsic::amdgcn_workitem_id_y,
+          [](llvm::IRBuilder<>* b_) -> llvm::CallInst* {
+            return EmitDeviceFunctionCall(
+                "_Z32__spirv_BuiltInLocalInvocationIdi", {b_->getInt32(1)},
+                {U32}, U64, {b_->getContext()}, b_);
+          },
+      };
     }
     case TargetIntrinsicID::kThreadIdz: {
-      return {llvm::Intrinsic::nvvm_read_ptx_sreg_tid_z,
-              llvm::Intrinsic::amdgcn_workitem_id_z};
+      return {
+          llvm::Intrinsic::nvvm_read_ptx_sreg_tid_z,
+          llvm::Intrinsic::amdgcn_workitem_id_z,
+          [](llvm::IRBuilder<>* b_) -> llvm::CallInst* {
+            return EmitDeviceFunctionCall(
+                "_Z32__spirv_BuiltInLocalInvocationIdi", {b_->getInt32(2)},
+                {U32}, U64, {b_->getContext()}, b_);
+          },
+      };
     }
     case TargetIntrinsicID::kBlockIdx: {
-      return {llvm::Intrinsic::nvvm_read_ptx_sreg_ctaid_x,
-              llvm::Intrinsic::amdgcn_workgroup_id_x};
+      return {
+          llvm::Intrinsic::nvvm_read_ptx_sreg_ctaid_x,
+          llvm::Intrinsic::amdgcn_workgroup_id_x,
+          [](llvm::IRBuilder<>* b_) -> llvm::CallInst* {
+            return EmitDeviceFunctionCall("_Z26__spirv_BuiltInWorkgroupIdi",
+                                          {b_->getInt32(0)}, {U32}, U64,
+                                          {b_->getContext()}, b_);
+          },
+      };
     }
     case TargetIntrinsicID::kBlockIdy: {
-      return {llvm::Intrinsic::nvvm_read_ptx_sreg_ctaid_y,
-              llvm::Intrinsic::amdgcn_workgroup_id_y};
+      return {
+          llvm::Intrinsic::nvvm_read_ptx_sreg_ctaid_y,
+          llvm::Intrinsic::amdgcn_workgroup_id_y,
+          [](llvm::IRBuilder<>* b_) -> llvm::CallInst* {
+            return EmitDeviceFunctionCall("_Z26__spirv_BuiltInWorkgroupIdi",
+                                          {b_->getInt32(1)}, {U32}, U64,
+                                          {b_->getContext()}, b_);
+          },
+      };
     }
     case TargetIntrinsicID::kBlockIdz: {
-      return {llvm::Intrinsic::nvvm_read_ptx_sreg_ctaid_z,
-              llvm::Intrinsic::amdgcn_workgroup_id_z};
+      return {
+          llvm::Intrinsic::nvvm_read_ptx_sreg_ctaid_z,
+          llvm::Intrinsic::amdgcn_workgroup_id_z,
+          [](llvm::IRBuilder<>* b_) -> llvm::CallInst* {
+            return EmitDeviceFunctionCall("_Z26__spirv_BuiltInWorkgroupIdi",
+                                          {b_->getInt32(2)}, {U32}, U64,
+                                          {b_->getContext()}, b_);
+          },
+      };
     }
     case TargetIntrinsicID::kBarrierId: {
-      return {llvm::Intrinsic::nvvm_barrier0,
-              llvm::Intrinsic::amdgcn_s_barrier};
+      return {llvm::Intrinsic::nvvm_barrier0, llvm::Intrinsic::amdgcn_s_barrier,
+              [](llvm::IRBuilder<>* b_) -> llvm::CallInst* {
+                return EmitDeviceFunctionCall(
+                    "__builtin_spirv_OpControlBarrier_i32_i32_i32",
+                    {b_->getInt32(2), b_->getInt32(2), b_->getInt32(272)},
+                    {U32, U32, U32}, VOID,
+                    llvm::AttrBuilder(b_->getContext())
+                        .addAttribute(llvm::Attribute::Convergent),
+                    b_);
+              }};
     }
     case TargetIntrinsicID::kBlockDimx: {
       return {llvm::Intrinsic::nvvm_read_ptx_sreg_ntid_x,
@@ -85,6 +139,11 @@ struct TargetIntrinsics GetIntrinsic(TargetIntrinsicID intrin) {
                 return EmitDeviceFunctionCall("__ockl_get_local_size",
                                               {b_->getInt32(0)}, {U32}, U64,
                                               {b_->getContext()}, b_);
+              },
+              [](llvm::IRBuilder<>* b_) -> llvm::CallInst* {
+                return EmitDeviceFunctionCall(
+                    "_Z28__spirv_BuiltInWorkgroupSizei", {b_->getInt32(0)},
+                    {U32}, U64, {b_->getContext()}, b_);
               }};
     }
     case TargetIntrinsicID::kBlockDimy: {
@@ -93,6 +152,11 @@ struct TargetIntrinsics GetIntrinsic(TargetIntrinsicID intrin) {
                 return EmitDeviceFunctionCall("__ockl_get_local_size",
                                               {b_->getInt32(1)}, {U32}, U64,
                                               {b_->getContext()}, b_);
+              },
+              [](llvm::IRBuilder<>* b_) -> llvm::CallInst* {
+                return EmitDeviceFunctionCall(
+                    "_Z28__spirv_BuiltInWorkgroupSizei", {b_->getInt32(1)},
+                    {U32}, U64, {b_->getContext()}, b_);
               }};
     }
     case TargetIntrinsicID::kBlockDimz: {
@@ -101,11 +165,26 @@ struct TargetIntrinsics GetIntrinsic(TargetIntrinsicID intrin) {
                 return EmitDeviceFunctionCall("__ockl_get_local_size",
                                               {b_->getInt32(2)}, {U32}, U64,
                                               {b_->getContext()}, b_);
+              },
+              [](llvm::IRBuilder<>* b_) -> llvm::CallInst* {
+                return EmitDeviceFunctionCall(
+                    "_Z28__spirv_BuiltInWorkgroupSizei", {b_->getInt32(2)},
+                    {U32}, U64, {b_->getContext()}, b_);
               }};
     }
     case TargetIntrinsicID::kGroupBarrierId: {
       return {llvm::Intrinsic::nvvm_bar_warp_sync,
-              llvm::Intrinsic::amdgcn_wave_barrier};
+              llvm::Intrinsic::amdgcn_wave_barrier,
+              [](llvm::IRBuilder<>* b_) -> llvm::CallInst* {
+                // TODO: Fix it.
+                return EmitDeviceFunctionCall(
+                    "__builtin_spirv_OpControlBarrier_i32_i32_i32",
+                    {b_->getInt32(2), b_->getInt32(2), b_->getInt32(272)},
+                    {U32, U32, U32}, VOID,
+                    llvm::AttrBuilder(b_->getContext())
+                        .addAttribute(llvm::Attribute::Convergent),
+                    b_);
+              }};
     }
   }
 }
@@ -114,6 +193,7 @@ struct TargetIntrinsics GetIntrinsic(TargetIntrinsicID intrin) {
 struct TargetDeviceFunction {
   const std::string nvptx_root;
   const std::string amdgpu_root;
+  const std::string spir_root;
 };
 
 // Gets the device function name on different platforms (NVPTX, AMDGPU)
@@ -122,55 +202,55 @@ struct TargetDeviceFunction GetDeviceFunctionRoot(
     TargetDeviceFunctionID func_id) {
   switch (func_id) {
     case TargetDeviceFunctionID::kAtan2: {
-      return {"__nv_atan2", "__ocml_atan2"};
+      return {"__nv_atan2", "__ocml_atan2", "__builtin_spirv_OpenCL_atan2"};
     }
     case TargetDeviceFunctionID::kCos: {
-      return {"__nv_cos", "__ocml_cos"};
+      return {"__nv_cos", "__ocml_cos", "__builtin_spirv_OpenCL_cos"};
     }
     case TargetDeviceFunctionID::kErfcinv: {
-      return {"__nv_erfcinv", "__ocml_erfcinv"};
+      return {"__nv_erfcinv", "__ocml_erfcinv", "__imf_erfcinv"};
     }
     case TargetDeviceFunctionID::kExp: {
-      return {"__nv_exp", "__ocml_exp"};
+      return {"__nv_exp", "__ocml_exp", "__builtin_spirv_OpenCL_exp"};
     }
     case TargetDeviceFunctionID::kExpm1: {
-      return {"__nv_expm1", "__ocml_expm1"};
+      return {"__nv_expm1", "__ocml_expm1", "__builtin_spirv_OpenCL_expm1"};
     }
     case TargetDeviceFunctionID::kFmod: {
-      return {"__nv_fmod", "__ocml_fmod"};
+      return {"__nv_fmod", "__ocml_fmod", "__builtin_spirv_OpenCL_fmod"};
     }
     case TargetDeviceFunctionID::kHypot: {
-      return {"__nv_hypot", "__ocml_hypot"};
+      return {"__nv_hypot", "__ocml_hypot", "__builtin_spirv_OpenCL_hypot"};
     }
     case TargetDeviceFunctionID::kLog: {
-      return {"__nv_log", "__ocml_log"};
+      return {"__nv_log", "__ocml_log", "__builtin_spirv_OpenCL_log"};
     }
     case TargetDeviceFunctionID::kLog1p: {
-      return {"__nv_log1p", "__ocml_log1p"};
+      return {"__nv_log1p", "__ocml_log1p", "__builtin_spirv_OpenCL_log1p"};
     }
     case TargetDeviceFunctionID::kPow: {
-      return {"__nv_pow", "__ocml_pow"};
+      return {"__nv_pow", "__ocml_pow", "__builtin_spirv_OpenCL_pow"};
     }
     case TargetDeviceFunctionID::kRound: {
-      return {"__nv_round", "__ocml_round"};
+      return {"__nv_round", "__ocml_round", "__builtin_spirv_OpenCL_round"};
     }
     case TargetDeviceFunctionID::kRsqrt: {
-      return {"__nv_rsqrt", "__ocml_rsqrt"};
+      return {"__nv_rsqrt", "__ocml_rsqrt", "__builtin_spirv_OpenCL_rsqrt"};
     }
     case TargetDeviceFunctionID::kSin: {
-      return {"__nv_sin", "__ocml_sin"};
+      return {"__nv_sin", "__ocml_sin", "__builtin_spirv_OpenCL_sin"};
     }
     case TargetDeviceFunctionID::kSqrt: {
-      return {"__nv_sqrt", "__ocml_sqrt"};
+      return {"__nv_sqrt", "__ocml_sqrt", "__builtin_spirv_OpenCL_sqrt"};
     }
     case TargetDeviceFunctionID::kTan: {
-      return {"__nv_tan", "__ocml_tan"};
+      return {"__nv_tan", "__ocml_tan", "__builtin_spirv_OpenCL_tan"};
     }
     case TargetDeviceFunctionID::kTanh: {
-      return {"__nv_tanh", "__ocml_tanh"};
+      return {"__nv_tanh", "__ocml_tanh", "__builtin_spirv_OpenCL_tanh"};
     }
     case TargetDeviceFunctionID::kCbrt: {
-      return {"__nv_cbrt", "__ocml_cbrt"};
+      return {"__nv_cbrt", "__ocml_cbrt", "__builtin_spirv_OpenCL_cbrt"};
     }
   }
 }
@@ -235,6 +315,28 @@ std::string ObtainDeviceFunctionName(TargetDeviceFunctionID func_id,
     } else {
       LOG(FATAL) << "Unexpected type while getting device function name.";
     }
+  } else if (target_triple.isSPIR()) {
+    if (output_type == F32) {
+      if (gpu_root_names.spir_root == "__builtin_spirv_OpenCL_hypot" ||
+          gpu_root_names.spir_root == "__builtin_spirv_OpenCL_pow" ||
+          gpu_root_names.spir_root == "__builtin_spirv_OpenCL_atan2" ||
+          gpu_root_names.spir_root == "__builtin_spirv_OpenCL_fmod")
+        return StrCat(gpu_root_names.spir_root, "_f32_f32");
+      if (gpu_root_names.spir_root == "__imf_erfcinv")
+        return StrCat(gpu_root_names.spir_root, "f");
+      return StrCat(gpu_root_names.spir_root, "_f32");
+    } else if (output_type == F64) {
+      if (gpu_root_names.spir_root == "__builtin_spirv_OpenCL_hypot" ||
+          gpu_root_names.spir_root == "__builtin_spirv_OpenCL_pow" ||
+          gpu_root_names.spir_root == "__builtin_spirv_OpenCL_atan2" ||
+          gpu_root_names.spir_root == "__builtin_spirv_OpenCL_fmod")
+        return StrCat(gpu_root_names.spir_root, "_f64_f64");
+      if (gpu_root_names.spir_root == "__imf_erfcinv")
+        return StrCat(gpu_root_names.spir_root, "d");
+      return StrCat(gpu_root_names.spir_root, "_f64");
+    } else {
+      LOG(FATAL) << "Unexpected type while getting device function name.";
+    }
   } else {
     LOG(FATAL) << "Invalid triple " << target_triple.str();
   }
@@ -247,6 +349,7 @@ llvm::CallInst* EmitDeviceFunctionCall(
     absl::string_view name) {
   std::vector<llvm::Type*> ir_input_types;
   llvm::Module* module = b->GetInsertBlock()->getModule();
+  llvm::Triple target_triple = llvm::Triple(module->getTargetTriple());
   for (PrimitiveType input_type : input_types) {
     ir_input_types.push_back(
         llvm_ir::PrimitiveTypeToIrType(input_type, module));
@@ -264,6 +367,9 @@ llvm::CallInst* EmitDeviceFunctionCall(
           .getCallee());
 
   callee->addFnAttrs(attributes);
+  // SYCL: SPIR function
+  if (target_triple.isSPIR())
+    callee->setCallingConv(llvm::CallingConv::SPIR_FUNC);
 
   return b->CreateCall(callee, llvm_ir::AsArrayRef(operands), name.data());
 }
@@ -289,6 +395,18 @@ llvm::CallInst* EmitCallToTargetIntrinsic(
               &gpu_intrinsic_id.amdgpu_intrinsic_or_function);
       return (*builder_func)(b);
     }
+  } else if (target_triple.isSPIR()) {
+    llvm::Intrinsic::ID* llvm_intrinsic_id_ptr =
+        absl::get_if<llvm::Intrinsic::ID>(
+            &gpu_intrinsic_id.spir_intrinsic_or_function);
+    if (llvm_intrinsic_id_ptr) {
+      llvm_intrinsic_id = *llvm_intrinsic_id_ptr;
+    } else {
+      std::function<llvm::CallInst*(llvm::IRBuilder<>*)>* builder_func =
+          absl::get_if<std::function<llvm::CallInst*(llvm::IRBuilder<>*)>>(
+              &gpu_intrinsic_id.spir_intrinsic_or_function);
+      return (*builder_func)(b);
+    }
   } else {
     LOG(FATAL) << "Invalid triple " << target_triple.str();
   }
@@ -316,6 +434,8 @@ void AnnotateFunctionAsGpuKernel(llvm::Module* module, llvm::Function* func,
     // Attach information so AMDGPU can recognize function as a AMDGPU kernel.
     func->setCallingConv(llvm::CallingConv::AMDGPU_KERNEL);
     func->addFnAttr("amdgpu-flat-work-group-size", "1, 1024");
+  } else if (target_triple.isSPIR()) {
+    // Do nothing
   } else {
     LOG(FATAL) << "Invalid triple " << target_triple.str();
   }
diff --git a/xla/service/gpu/thunk.cc b/xla/service/gpu/thunk.cc
index e94af57c6..6ba4429b2 100644
--- a/xla/service/gpu/thunk.cc
+++ b/xla/service/gpu/thunk.cc
@@ -75,6 +75,7 @@ Thunk::ExecuteParams::ExecuteParams(
     CASE(kTriangularSolve);
     CASE(kWhile);
     CASE(kFusedMHA);
+    CASE(kFusedQKV);
   }
 }
 
diff --git a/xla/service/gpu/thunk.h b/xla/service/gpu/thunk.h
index 25803cf7d..c24f8158d 100644
--- a/xla/service/gpu/thunk.h
+++ b/xla/service/gpu/thunk.h
@@ -84,7 +84,8 @@ class Thunk {
     kSequential,
     kTriangularSolve,
     kWhile,
-    kFusedMHA
+    kFusedMHA,
+    kFusedQKV
   };
 
   struct ThunkInfo {
diff --git a/xla/service/layout_normalization.cc b/xla/service/layout_normalization.cc
index 59475870b..b62196e2c 100644
--- a/xla/service/layout_normalization.cc
+++ b/xla/service/layout_normalization.cc
@@ -59,19 +59,18 @@ class LayoutNormalizationVisitor : public DfsHloRewriteVisitor {
   // To handle a constant, just give the literal data a new layout.
   Status HandleConstant(HloInstruction* hlo) override {
     Literal& literal = *Cast<HloConstantInstruction>(hlo)->mutable_literal();
-    const Shape& shape = hlo->shape();
     if (literal.shape().IsTuple()) {
       // TODO(cheshire): Tuple constants.
       return OkStatus();
     }
 
-    Shape normalized_shape = Normalize(hlo->shape());
+    const Shape& shape = hlo->shape();
+    Shape normalized_shape = Normalize(shape);
     *literal.mutable_shape_do_not_use() = normalized_shape;
-
-    HloInstruction* normalized = hlo->parent()->AddInstruction(
-        HloInstruction::CreateConstant(std::move(literal)), &hlo->metadata());
-    HloInstruction* bc_to_orig = MakeBitcastHlo(normalized, shape);
-    TF_RETURN_IF_ERROR(ReplaceInstruction(hlo, bc_to_orig));
+    HloInstruction* bc_to_orig = MakeBitcastHlo(hlo, shape);
+    *hlo->mutable_shape() = normalized_shape;
+    TF_RETURN_IF_ERROR(hlo->ReplaceAllUsesWithDifferentShape(bc_to_orig));
+    MarkAsChanged();
     return OkStatus();
   }
 
@@ -101,6 +100,7 @@ class LayoutNormalizationVisitor : public DfsHloRewriteVisitor {
                                      &hlo->metadata()));
     *normalized_slice->mutable_shape()->mutable_layout() =
         normalized_input->shape().layout();
+    SetVisited(*normalized_slice);
     HloInstruction* bc_to_orig = MakeBitcastHlo(normalized_slice, s);
     TF_RETURN_IF_ERROR(ReplaceInstruction(hlo, bc_to_orig));
     return OkStatus();
@@ -126,6 +126,7 @@ class LayoutNormalizationVisitor : public DfsHloRewriteVisitor {
 
     auto normalized_shape = Normalize(shape);
     auto bc_to_normalized = MakeBitcastHlo(hlo, normalized_shape);
+    SetVisited(*bc_to_normalized);
     auto bc_to_orig = MakeBitcastHlo(bc_to_normalized, shape);
     TF_RETURN_IF_ERROR(hlo->ReplaceUsesWith(users, bc_to_orig));
     MarkAsChanged();
@@ -152,6 +153,7 @@ class LayoutNormalizationVisitor : public DfsHloRewriteVisitor {
     auto normalized_concat =
         hlo->AddInstruction(HloInstruction::CreateConcatenate(
             normalized_shape, normalized_inputs, normalized_concat_dim));
+    SetVisited(*normalized_concat);
     auto bc_to_orig = MakeBitcastHlo(normalized_concat, hlo->shape());
     TF_RETURN_IF_ERROR(ReplaceInstruction(hlo, bc_to_orig));
     return OkStatus();
@@ -187,6 +189,7 @@ class LayoutNormalizationVisitor : public DfsHloRewriteVisitor {
         MakeReduceWindowHlo(normalized_input, hlo->mutable_operand(1),
                             new_window, hlo->called_computations()[0],
                             &hlo->metadata()));
+    SetVisited(*rw);
 
     HloInstruction* bc_to_orig = MakeBitcastHlo(rw, hlo->shape());
     TF_RETURN_IF_ERROR(ReplaceInstruction(hlo, bc_to_orig));
@@ -221,6 +224,7 @@ class LayoutNormalizationVisitor : public DfsHloRewriteVisitor {
     }
     auto normalized_broadcast = MakeBroadcastHlo(
         normalized_input, br_dimensions, normalized_shape, &hlo->metadata());
+    SetVisited(*normalized_broadcast);
     VLOG(3) << "Generated broadcast: " << normalized_broadcast->ToString();
     auto bc_to_orig = MakeBitcastHlo(normalized_broadcast, s);
     TF_RETURN_IF_ERROR(ReplaceInstruction(hlo, bc_to_orig));
@@ -275,6 +279,7 @@ class LayoutNormalizationVisitor : public DfsHloRewriteVisitor {
           new_unary,
           MakeUnaryHlo(hlo->opcode(), normalized_input, &hlo->metadata()));
     }
+    SetVisited(*new_unary);
     auto bc_to_orig = MakeBitcastHlo(new_unary, s);
     TF_RETURN_IF_ERROR(ReplaceInstruction(hlo, bc_to_orig));
     return OkStatus();
@@ -312,6 +317,7 @@ class LayoutNormalizationVisitor : public DfsHloRewriteVisitor {
       TF_ASSIGN_OR_RETURN(
           new_binary, MakeBinaryHlo(hlo->opcode(), a0, b0, &hlo->metadata()));
     }
+    SetVisited(*new_binary);
     auto bc_to_orig = MakeBitcastHlo(new_binary, s);
     TF_RETURN_IF_ERROR(ReplaceInstruction(hlo, bc_to_orig));
     return OkStatus();
@@ -334,6 +340,7 @@ class LayoutNormalizationVisitor : public DfsHloRewriteVisitor {
     auto normalized_reshape_s = Normalize(s);
     TF_ASSIGN_OR_RETURN(auto new_reshape,
                         MakeReshapeHlo(normalized_reshape_s, a0));
+    SetVisited(*new_reshape);
     auto bc_to_orig = MakeBitcastHlo(new_reshape, s);
     TF_RETURN_IF_ERROR(ReplaceInstruction(hlo, bc_to_orig));
     return OkStatus();
@@ -380,6 +387,7 @@ class LayoutNormalizationVisitor : public DfsHloRewriteVisitor {
       auto dimensions = ComposePermutations(t, l_perm);
       auto normalized_transpose = hlo->AddInstruction(
           HloInstruction::CreateTranspose(normalized_shape, a0, dimensions));
+      SetVisited(*normalized_transpose);
       VLOG(3) << "Generated normalized physical transpose: "
               << normalized_transpose->ToString();
       auto bc_to_orig = MakeBitcastHlo(normalized_transpose, s);
@@ -415,6 +423,7 @@ class LayoutNormalizationVisitor : public DfsHloRewriteVisitor {
     auto dimensions = ComposePermutations(l0_perm, l_perm);
     auto t = hlo->AddInstruction(
         HloInstruction::CreateTranspose(s_normalized, a0, dimensions));
+    SetVisited(*t);
     auto bc_to_orig = MakeBitcastHlo(t, s);
     TF_RETURN_IF_ERROR(ReplaceInstruction(hlo, bc_to_orig));
     return OkStatus();
@@ -435,6 +444,7 @@ class LayoutNormalizationVisitor : public DfsHloRewriteVisitor {
     absl::c_sort(new_dimensions);
     auto normalized_reverse = hlo->AddInstruction(
         HloInstruction::CreateReverse(a0->shape(), a0, new_dimensions));
+    SetVisited(*normalized_reverse);
     auto bc_to_orig = MakeBitcastHlo(normalized_reverse, s);
     TF_RETURN_IF_ERROR(ReplaceInstruction(hlo, bc_to_orig));
     return OkStatus();
@@ -466,6 +476,7 @@ class LayoutNormalizationVisitor : public DfsHloRewriteVisitor {
 
     auto padded_normalized = hlo->AddInstruction(HloInstruction::CreatePad(
         s_normalized, normalized_input, padded_by, new_padding));
+    SetVisited(*padded_normalized);
     auto bc_to_orig = MakeBitcastHlo(padded_normalized, s);
     TF_RETURN_IF_ERROR(ReplaceInstruction(hlo, bc_to_orig));
     return OkStatus();
@@ -477,6 +488,7 @@ class LayoutNormalizationVisitor : public DfsHloRewriteVisitor {
           std::optional<HloInstruction*> transformed_custom_call,
           custom_call_transformer_(Cast<HloCustomCallInstruction>(hlo)));
       if (transformed_custom_call) {
+        SetVisited(*(*transformed_custom_call)->operand(0));
         TF_RETURN_IF_ERROR(ReplaceInstruction(hlo, *transformed_custom_call));
         return OkStatus();
       }
@@ -519,6 +531,7 @@ class LayoutNormalizationVisitor : public DfsHloRewriteVisitor {
                             &hlo->metadata()));
     *normalized_dynamic_slice->mutable_shape()->mutable_layout() =
         normalized_input->shape().layout();
+    SetVisited(*normalized_dynamic_slice);
     HloInstruction* bc_to_orig = MakeBitcastHlo(normalized_dynamic_slice, s);
     TF_RETURN_IF_ERROR(ReplaceInstruction(hlo, bc_to_orig));
     return OkStatus();
@@ -545,6 +558,7 @@ class LayoutNormalizationVisitor : public DfsHloRewriteVisitor {
         MakeDynamicUpdateSliceHlo(new_operand, new_update, new_start_indices,
                                   &hlo->metadata()));
     *new_dus->mutable_shape()->mutable_layout() = new_operand->shape().layout();
+    SetVisited(*new_dus);
 
     HloInstruction* bc_to_orig = MakeBitcastHlo(new_dus, s);
     TF_RETURN_IF_ERROR(ReplaceInstruction(hlo, bc_to_orig));
@@ -578,6 +592,7 @@ class LayoutNormalizationVisitor : public DfsHloRewriteVisitor {
     HloInstruction* normalized = hlo->parent()->AddInstruction(
         HloInstruction::CreateTernary(new_shape, opcode, p_0, i1_0, i2_0));
     hlo->SetupDerivedInstruction(normalized);
+    SetVisited(*normalized);
 
     HloInstruction* bc_to_orig = MakeBitcastHlo(normalized, s);
     TF_RETURN_IF_ERROR(ReplaceInstruction(hlo, bc_to_orig));
diff --git a/xla/service/layout_normalization_test.cc b/xla/service/layout_normalization_test.cc
index 3f01cc5a3..7c972ebef 100644
--- a/xla/service/layout_normalization_test.cc
+++ b/xla/service/layout_normalization_test.cc
@@ -555,6 +555,31 @@ ENTRY main {
   )");
 }
 
+TEST_F(LayoutNormalizationTest, ConstantAvoidRevisitOfUser) {
+  const char* hlo = R"(
+HloModule module
+
+ENTRY main {
+  c = f32[5,4]{0,1} constant({...})
+  s = f32[5,4]{0,1} sine(c)
+  t = f32[5,4]{0,1} tanh(s)
+  ROOT o = f32[5,4]{0,1} add(s, t)
+}
+)";
+  // If we allowed visiting the normalized user 's' of the constant, we would
+  // run into a CHECK failure, because the constant was normalized in-place and
+  // therefore would not be revisited.
+  CheckLayoutNormalization(hlo, R"(
+// CHECK: [[constant_2:%[^ ]+]] = f32[4,5]{1,0} constant({...})
+// CHECK-NEXT: [[sine:%[^ ]+]] = f32[4,5]{1,0} sine([[constant_2]])
+// CHECK-NEXT: [[bitcast_1:%[^ ]+]] = f32[5,4]{0,1} bitcast([[sine]])
+// CHECK-NEXT: [[bitcast_2:%[^ ]+]] = f32[4,5]{1,0} bitcast([[bitcast_1]])
+// CHECK-NEXT: [[tanh:%[^ ]+]] = f32[4,5]{1,0} tanh([[bitcast_2]])
+// CHECK-NEXT: [[add_3:%[^ ]+]] = f32[4,5]{1,0} add([[bitcast_2]], [[tanh]])
+// CHECK-NEXT: ROOT [[bitcast_3_4:%[^ ]+]] = f32[5,4]{0,1} bitcast([[add_3]])
+  )");
+}
+
 TEST_F(LayoutNormalizationTest, Slice) {
   const char* hlo = R"(
 HloModule module
diff --git a/xla/service/llvm_ir/fused_ir_emitter.cc b/xla/service/llvm_ir/fused_ir_emitter.cc
index 006020e41..0add8fbe3 100644
--- a/xla/service/llvm_ir/fused_ir_emitter.cc
+++ b/xla/service/llvm_ir/fused_ir_emitter.cc
@@ -104,7 +104,7 @@ FusedIrEmitter::IndexedGenerator FusedIrEmitter::HandleConstant(
       /*Initializer=*/initializer,
       /*Name=*/"", /*InsertBefore=*/nullptr,
       /*TLMode=*/llvm::GlobalValue::NotThreadLocal,
-      /*AddressSpace=*/0,
+      /*AddressSpace=*/1,  // SYCL: Hardcode to global addrspace
       /*isExternallyInitialized=*/false);
   global->setUnnamedAddr(llvm::GlobalVariable::UnnamedAddr::Global);
 
diff --git a/xla/service/llvm_ir/ir_array.cc b/xla/service/llvm_ir/ir_array.cc
index 7708db56e..6be405a23 100644
--- a/xla/service/llvm_ir/ir_array.cc
+++ b/xla/service/llvm_ir/ir_array.cc
@@ -24,6 +24,7 @@ limitations under the License.
 #include "llvm/IR/Constants.h"
 #include "llvm/IR/Instructions.h"
 #include "llvm/IR/Value.h"
+#include "tsl/platform/logging.h"
 #include "xla/layout_util.h"
 #include "xla/permutation_util.h"
 #include "xla/service/llvm_ir/llvm_type_conversion_util.h"
@@ -32,7 +33,6 @@ limitations under the License.
 #include "xla/statusor.h"
 #include "xla/util.h"
 #include "xla/xla_data.pb.h"
-#include "tsl/platform/logging.h"
 
 namespace xla {
 namespace llvm_ir {
@@ -494,9 +494,24 @@ llvm::Value* IrArray::EmitArrayElementAddress(const IrArray::Index& index,
   if (use_linear_index && index.LinearValidOnShape(shape_)) {
     llvm::Module* module = b->GetInsertBlock()->getParent()->getParent();
     llvm::Type* type = PrimitiveTypeToIrType(shape_.element_type(), module);
-    return b->CreateInBoundsGEP(
-        type, b->CreateBitCast(base_ptr_, type->getPointerTo()), index.linear(),
-        llvm_ir::AsStringRef(name));
+    
+    auto linear_index = dyn_cast<llvm::BinaryOperator>(index.linear());
+    // only separate const offset when having add
+    if (linear_index && (linear_index->getOpcode() == llvm::Instruction::Add)) {
+      llvm::Value* index_operand_0 = linear_index->getOperand(0);
+      // constant index
+      llvm::Value* index_operand_1 = linear_index->getOperand(1);
+      llvm::Value* ptr_address =
+          b->CreateGEP(type, b->CreateBitCast(base_ptr_, type->getPointerTo()),
+                       index_operand_0, "");
+
+      return b->CreateInBoundsGEP(type, ptr_address, index_operand_1,
+                                  llvm_ir::AsStringRef(name));
+    } else {
+      return b->CreateInBoundsGEP(
+          type, b->CreateBitCast(base_ptr_, type->getPointerTo()),
+          index.linear(), llvm_ir::AsStringRef(name));
+    }
   }
 
   std::vector<llvm::Value*> actual_index;
diff --git a/xla/service/llvm_ir/llvm_util.cc b/xla/service/llvm_ir/llvm_util.cc
index 97e6b63e3..a38e2a08c 100644
--- a/xla/service/llvm_ir/llvm_util.cc
+++ b/xla/service/llvm_ir/llvm_util.cc
@@ -235,6 +235,8 @@ llvm::Type* PrimitiveTypeToIrType(PrimitiveType element_type,
       // Tokens do not have a physical representation, but the compiler needs
       // some placeholder type, so use int8_t*.
       return llvm::Type::getInt8PtrTy(module->getContext());
+    case VOID:
+      return llvm::Type::getVoidTy(module->getContext());
     default:
       LOG(FATAL) << "unsupported type " << element_type;
   }
@@ -317,8 +319,9 @@ llvm::AllocaInst* EmitAllocaAtFunctionEntryWithCount(llvm::Type* type,
   llvm::Function* function = b->GetInsertBlock()->getParent();
   b->SetInsertPoint(&function->getEntryBlock(),
                     function->getEntryBlock().getFirstInsertionPt());
+  // SYCL: Fix atomic issue by allocating on private addrspace
   llvm::AllocaInst* alloca =
-      b->CreateAlloca(type, element_count, AsStringRef(name));
+      b->CreateAlloca(type, /*addrspace*/5, element_count, AsStringRef(name));
   if (alignment != 0) {
     alloca->setAlignment(llvm::Align(alignment));
   }
@@ -437,6 +440,7 @@ void SetDereferenceableMetadataForLoad(llvm::LoadInst* load,
 
 llvm::Instruction* AddRangeMetadata(int32_t lower, int32_t upper,
                                     llvm::Instruction* inst) {
+  /* SYCL: This range is for NVPTX target only.
   llvm::LLVMContext& context = inst->getParent()->getContext();
   llvm::IntegerType* i32 = llvm::Type::getInt32Ty(context);
   inst->setMetadata(
@@ -445,6 +449,7 @@ llvm::Instruction* AddRangeMetadata(int32_t lower, int32_t upper,
           context,
           {llvm::ConstantAsMetadata::get(llvm::ConstantInt::get(i32, lower)),
            llvm::ConstantAsMetadata::get(llvm::ConstantInt::get(i32, upper))}));
+  */
   return inst;
 }
 
diff --git a/xla/stream_executor/build_defs.bzl b/xla/stream_executor/build_defs.bzl
index 1f0fd03d3..b2dd37acc 100644
--- a/xla/stream_executor/build_defs.bzl
+++ b/xla/stream_executor/build_defs.bzl
@@ -1,5 +1,6 @@
 load("@local_config_cuda//cuda:build_defs.bzl", "if_cuda_is_configured")
 load("@local_config_rocm//rocm:build_defs.bzl", "if_rocm_is_configured")
+load("@local_config_sycl//sycl:build_defs.bzl", "if_sycl_is_configured")
 
 def stream_executor_friends():
     return ["//..."]
@@ -22,7 +23,7 @@ def tf_additional_cudnn_plugin_copts():
 
 # Returns whether any GPU backend is configuered.
 def if_gpu_is_configured(x):
-    return if_cuda_is_configured(x) + if_rocm_is_configured(x)
+    return if_cuda_is_configured(x) + if_rocm_is_configured(x) + if_sycl_is_configured(x)
 
 def if_cuda_or_rocm(x):
     return if_gpu_is_configured(x)
diff --git a/xla/stream_executor/cuda/cuda_driver.cc b/xla/stream_executor/cuda/cuda_driver.cc
index 3d9264374..c8a9b0a35 100644
--- a/xla/stream_executor/cuda/cuda_driver.cc
+++ b/xla/stream_executor/cuda/cuda_driver.cc
@@ -576,6 +576,14 @@ bool DeviceOptionsToContextFlags(const DeviceOptions& device_options,
       "Feature not supported on CUDA platform (LoadHsaco)");
 }
 
+/* static */ tsl::Status GpuDriver::LoadLevelzero(GpuContext* context,
+                                                  const char* spir_contents,
+                                                  const size_t size,
+                                                  CUmodule* module) {
+  LOG(ERROR) << "Feature not supported on CUDA platform (LoadLevelzero)";
+  return tsl::errors::Internal("Not Implemented");
+}
+
 /* static */ tsl::Status GpuDriver::SynchronousMemsetUint8(GpuContext* context,
                                                            CUdeviceptr location,
                                                            uint8_t value,
diff --git a/xla/stream_executor/cuda/cuda_gpu_executor.cc b/xla/stream_executor/cuda/cuda_gpu_executor.cc
index 3e171c793..6cad72bf2 100644
--- a/xla/stream_executor/cuda/cuda_gpu_executor.cc
+++ b/xla/stream_executor/cuda/cuda_gpu_executor.cc
@@ -182,6 +182,12 @@ tsl::Status GpuExecutor::LoadModuleFromHsaco(const char* hsaco,
       "Feature not supported on CUDA platform (LoadModuleFromHsaco)");
 }
 
+tsl::Status GpuExecutor::LoadModuleFromSpir(const char* spir, const size_t size,
+                                            CUmodule* module) {
+  return tsl::errors::Internal(
+      "Feature not supported on CUDA platform (LoadModuleFromSpir)");
+}
+
 tsl::Status GpuExecutor::GetKernel(const MultiKernelLoaderSpec& spec,
                                    KernelBase* kernel) {
   GpuKernel* cuda_kernel = AsGpuKernel(kernel);
diff --git a/xla/stream_executor/gpu/gpu_driver.h b/xla/stream_executor/gpu/gpu_driver.h
index 9fce9d68d..4badb209b 100644
--- a/xla/stream_executor/gpu/gpu_driver.h
+++ b/xla/stream_executor/gpu/gpu_driver.h
@@ -295,6 +295,9 @@ class GpuDriver {
   // (supported on ROCm only)
   static tsl::Status LoadHsaco(GpuContext* context, const char* hsaco_contents,
                                GpuModuleHandle* module);
+  static tsl::Status LoadLevelzero(GpuContext* context,
+                                   const char* spir_contents, const size_t size,
+                                   GpuModuleHandle* module);
 
   // Retrieves a named kernel from a loaded module, and places the resulting
   // handle into function (outparam) on success. Neither kernel_name nor
diff --git a/xla/stream_executor/gpu/gpu_executor.h b/xla/stream_executor/gpu/gpu_executor.h
index dba3e08a3..14f694069 100644
--- a/xla/stream_executor/gpu/gpu_executor.h
+++ b/xla/stream_executor/gpu/gpu_executor.h
@@ -320,6 +320,11 @@ class GpuExecutor : public internal::StreamExecutorInterface {
   tsl::Status LoadModuleFromHsaco(const char* hsaco, GpuModuleHandle* module)
       TF_EXCLUSIVE_LOCKS_REQUIRED(in_memory_modules_mu_);
 
+  // (supported on SYCL only)
+  tsl::Status LoadModuleFromSpir(const char* spir, const size_t size,
+                                 GpuModuleHandle* module)
+      TF_EXCLUSIVE_LOCKS_REQUIRED(in_memory_modules_mu_);
+
   bool UnloadGpuBinary(const void* gpu_binary)
       TF_EXCLUSIVE_LOCKS_REQUIRED(in_memory_modules_mu_);
 
diff --git a/xla/stream_executor/gpu/gpu_types.h b/xla/stream_executor/gpu/gpu_types.h
index 94337c729..656d1195a 100644
--- a/xla/stream_executor/gpu/gpu_types.h
+++ b/xla/stream_executor/gpu/gpu_types.h
@@ -18,7 +18,20 @@ limitations under the License.
 #ifndef XLA_STREAM_EXECUTOR_GPU_GPU_TYPES_H_
 #define XLA_STREAM_EXECUTOR_GPU_GPU_TYPES_H_
 
-#if TENSORFLOW_USE_ROCM
+#define TENSORFLOW_USE_SYCL 1
+
+#if TENSORFLOW_USE_SYCL
+
+#if __has_include(<sycl/sycl.hpp>)
+#include <sycl/sycl.hpp>
+#elif __has_include(<CL/sycl.hpp>)
+#include <CL/sycl.hpp>
+#else
+#error "Unsupported compiler"
+#endif
+#include <level_zero/ze_api.h>
+
+#elif TENSORFLOW_USE_ROCM
 
 #define __HIP_DISABLE_CPP_FUNCTIONS__
 
@@ -42,7 +55,26 @@ typedef struct curandGenerator_st* curandGenerator_t;
 namespace stream_executor {
 namespace gpu {
 
-#if TENSORFLOW_USE_ROCM
+#if TENSORFLOW_USE_SYCL
+
+using GpuContextHandle = const void*;
+using GpuStreamHandle = ::sycl::queue*;
+using GpuEventHandle = ::sycl::event*;
+using GpuFunctionHandle = ::sycl::kernel*;
+using GpuFunctionAttribute = const void*;
+using GpuDeviceHandle = ::sycl::device*;
+using GpuDevicePtr = void*;
+using GpuDeviceAttribute = const void*;
+using GpuDeviceProperty = const void*;
+using GpuModuleHandle = ze_module_handle_t;
+using GpuStatus = const void*;
+using GpuFuncCachePreference = const void*;
+using GpuSharedMemConfig = const void*;
+using GpuComplexType = const void*;
+using GpuDoubleComplexType = const void*;
+using GpuRngHandle = const void*;
+
+#elif TENSORFLOW_USE_ROCM
 
 using GpuContextHandle = hipCtx_t;
 using GpuStreamHandle = hipStream_t;
diff --git a/xla/stream_executor/kernel_spec.cc b/xla/stream_executor/kernel_spec.cc
index a8efbb398..878316ae5 100644
--- a/xla/stream_executor/kernel_spec.cc
+++ b/xla/stream_executor/kernel_spec.cc
@@ -34,9 +34,9 @@ CudaCubinOnDisk::CudaCubinOnDisk(absl::string_view filename,
                                  absl::string_view kernelname)
     : OnDiskKernelLoaderSpec(filename, kernelname) {}
 
-CudaCubinInMemory::CudaCubinInMemory(const char *bytes,
+CudaCubinInMemory::CudaCubinInMemory(const char *bytes, int size,
                                      absl::string_view kernelname)
-    : KernelLoaderSpec(kernelname), bytes_(bytes) {}
+    : KernelLoaderSpec(kernelname), size_(size), bytes_(bytes) {}
 
 bool CompareComputeCapability(const std::tuple<int, int> &lhs,
                               const std::tuple<int, int> &rhs) {
@@ -198,9 +198,9 @@ MultiKernelLoaderSpec *MultiKernelLoaderSpec::AddCudaPtxOnDisk(
 }
 
 MultiKernelLoaderSpec *MultiKernelLoaderSpec::AddCudaCubinInMemory(
-    const char *bytes, absl::string_view kernelname) {
+    const char *bytes, int size, absl::string_view kernelname) {
   CHECK(cuda_cubin_in_memory_ == nullptr);
-  cuda_cubin_in_memory_.reset(new CudaCubinInMemory{bytes, kernelname});
+  cuda_cubin_in_memory_.reset(new CudaCubinInMemory{bytes, size, kernelname});
   return this;
 }
 
diff --git a/xla/stream_executor/kernel_spec.h b/xla/stream_executor/kernel_spec.h
index 8b33a5fe8..9c3ae880b 100644
--- a/xla/stream_executor/kernel_spec.h
+++ b/xla/stream_executor/kernel_spec.h
@@ -258,13 +258,16 @@ class OpenCLTextInMemory : public KernelLoaderSpec {
 // Kernel loader specification for a CUBIN blob that resides in memory.
 class CudaCubinInMemory : public KernelLoaderSpec {
  public:
-  CudaCubinInMemory(const char *bytes, absl::string_view kernelname);
+  CudaCubinInMemory(const char *bytes, int size, absl::string_view kernelname);
   ~CudaCubinInMemory() override {}
 
   const char *bytes() const { return bytes_; }
+  const int size() const { return size_; }
 
  private:
   const char *bytes_;
+  // SYCL: this is needed only for SPIRV
+  int size_;
 
   SE_DISALLOW_COPY_AND_ASSIGN(CudaCubinInMemory);
 };
@@ -338,7 +341,7 @@ class MultiKernelLoaderSpec {
                                           absl::string_view kernelname);
   MultiKernelLoaderSpec *AddCudaCubinOnDisk(absl::string_view filename,
                                             absl::string_view kernelname);
-  MultiKernelLoaderSpec *AddCudaCubinInMemory(const char *cubin_bytes,
+  MultiKernelLoaderSpec *AddCudaCubinInMemory(const char *cubin_bytes, int size,
                                               absl::string_view kernelname);
   MultiKernelLoaderSpec *AddCudaPtxInMemory(absl::string_view ptx,
                                             absl::string_view kernelname);
diff --git a/xla/stream_executor/rocm/rocm_driver.cc b/xla/stream_executor/rocm/rocm_driver.cc
index a65f5e87b..267b0fe39 100644
--- a/xla/stream_executor/rocm/rocm_driver.cc
+++ b/xla/stream_executor/rocm/rocm_driver.cc
@@ -457,6 +457,14 @@ GpuDriver::ContextGetSharedMemConfig(GpuContext* context) {
   return ret;
 }
 
+/* static */ tsl::Status GpuDriver::LoadLevelzero(GpuContext* context,
+                                                  const char* spir_contents,
+                                                  const size_t size,
+                                                  hipModule_t* module) {
+  LOG(ERROR) << "Feature not supported on ROCm platform (LoadLevelzero)";
+  return tsl::errors::Internal("Not Implemented");
+}
+
 /* static */ tsl::Status GpuDriver::SynchronousMemsetUint8(
     GpuContext* context, hipDeviceptr_t location, uint8 value, size_t size) {
   ScopedActivateContext activation{context};
diff --git a/xla/stream_executor/rocm/rocm_gpu_executor.cc b/xla/stream_executor/rocm/rocm_gpu_executor.cc
index 105fadf78..a756b8170 100644
--- a/xla/stream_executor/rocm/rocm_gpu_executor.cc
+++ b/xla/stream_executor/rocm/rocm_gpu_executor.cc
@@ -368,6 +368,11 @@ tsl::Status GpuExecutor::LoadModuleFromHsaco(const char* hsaco,
   return tsl::OkStatus();
 }
 
+tsl::Status GpuExecutor::LoadModuleFromSpir(const char* spir, const size_t size,
+                                            hipModule_t* module) {
+  LOG(FATAL) << "Feature not supported on ROCM platform (LoadModuleFromSpir)";
+}
+
 // This is a non-essential operation; if there's a failure, proceed without
 // logging an error. It's nearly certain that in case of failures, we'd never
 // get here in the first place; these are very low-impact routines.
diff --git a/xla/stream_executor/stream_executor_pimpl.h b/xla/stream_executor/stream_executor_pimpl.h
index c2bb16bce..41a1e3f89 100644
--- a/xla/stream_executor/stream_executor_pimpl.h
+++ b/xla/stream_executor/stream_executor_pimpl.h
@@ -812,7 +812,8 @@ StreamExecutor::CreateTypedKernel(absl::string_view kernel_name,
 
   if (!cubin_data.empty()) {
     loader_spec.AddCudaCubinInMemory(
-        reinterpret_cast<const char*>(cubin_data.data()), kernel_name);
+        reinterpret_cast<const char*>(cubin_data.data()), cubin_data.size(),
+        kernel_name);
   }
 
   TF_RETURN_IF_ERROR(GetKernel(loader_spec, kernel_base.get()));
diff --git a/xla/translate/mhlo_to_lhlo_with_xla/mhlo_to_lhlo_with_xla.cc b/xla/translate/mhlo_to_lhlo_with_xla/mhlo_to_lhlo_with_xla.cc
index 397e7707d..18cdbbb6c 100644
--- a/xla/translate/mhlo_to_lhlo_with_xla/mhlo_to_lhlo_with_xla.cc
+++ b/xla/translate/mhlo_to_lhlo_with_xla/mhlo_to_lhlo_with_xla.cc
@@ -760,6 +760,10 @@ tsl::StatusOr<mlir::Operation*> LhloDialectEmitter::EmitCustomCallOp(
     return EmitDnnfMHA(custom_call_instr);
   }
 
+  if (xla::gpu::IsCustomCallTofQKV(*instr)) {
+    return EmitDnnfQKV(custom_call_instr);
+  }
+
   // For custom call, if there are any token operands or results, they will not
   // be represented in LHLO so we need to remember the mapping. First create
   // operands where each token is replaced with a null Value.
@@ -930,6 +934,8 @@ tsl::StatusOr<lmhlo_gpu::FusedMhaDagSignature> AsLhloFusedMhaDagSignature(
       return lmhlo_gpu::FusedMhaDagSignature::SoftmaxDropout;
     case xla::gpu::CudnnfMHAKind::kSoftmax:
       return lmhlo_gpu::FusedMhaDagSignature::Softmax;
+    case xla::gpu::CudnnfMHAKind::kScaleSoftmax:
+      return lmhlo_gpu::FusedMhaDagSignature::ScaleSoftmax;
     case xla::gpu::CudnnfMHAKind::kScaleBiasSoftmax:
       return lmhlo_gpu::FusedMhaDagSignature::ScaleBiasSoftmax;
     case xla::gpu::CudnnfMHAKind::kScaleBiasSoftmaxDropout:
@@ -1354,12 +1360,16 @@ tsl::StatusOr<Operation*> LhloDialectEmitter::EmitDnnfMHA(
       TF_RETURN_IF_ERROR(GetOrCreateView(custom_call, &operands));
       auto fmha_default =
           CreateOpWithoutAttrs<lmhlo_gpu::fusedMHAOp>(custom_call, operands);
+      fmha_default.setFmhaScaleAttr(
+          builder_.getF64FloatAttr(config.fmha_scale()));
       return set_common_fmha_attributes(fmha_default);
     }
     case xla::gpu::CudnnfMHAKind::kSoftmaxDropout: {
       TF_RETURN_IF_ERROR(GetOrCreateView(custom_call, &operands));
       auto fmha_softmax_dropout =
           CreateOpWithoutAttrs<lmhlo_gpu::fusedMHAOp>(custom_call, operands);
+      fmha_softmax_dropout.setFmhaScaleAttr(
+          builder_.getF64FloatAttr(config.fmha_scale()));
       fmha_softmax_dropout.setDropoutRateAttr(
           builder_.getF64FloatAttr(config.dropout_rate()));
       fmha_softmax_dropout.setSeedAttr(
@@ -1443,9 +1453,39 @@ tsl::StatusOr<Operation*> LhloDialectEmitter::EmitDnnfMHA(
           builder_.getF64FloatAttr(config.fmha_scale()));
       return set_common_fmha_attributes(fmha_bias_softmax);
     }
+    case xla::gpu::CudnnfMHAKind::kScaleSoftmax: {
+      TF_RETURN_IF_ERROR(GetOrCreateView(custom_call, &operands));
+      auto fmha_softmax =
+          CreateOpWithoutAttrs<lmhlo_gpu::fusedMHAOp>(custom_call, operands);
+      fmha_softmax.setFmhaScaleAttr(
+          builder_.getF64FloatAttr(config.fmha_scale()));
+      return set_common_fmha_attributes(fmha_softmax);
+    }
   }
 }
 
+tsl::StatusOr<Operation*> LhloDialectEmitter::EmitDnnfQKV(
+    const HloCustomCallInstruction* custom_call) {
+  
+  TF_ASSIGN_OR_RETURN(
+      auto const config,
+      custom_call->backend_config<xla::gpu::GemmBackendConfig>());
+
+  if (custom_call->operand_count() != 2) {
+    return xla::InvalidArgument("GEMM custom call should have 2 operands");
+  }
+
+  // QKV GEMM have two operands.
+  TF_ASSIGN_OR_RETURN(
+      lmhlo_gpu::fusedQKVOp op,
+      CreateOpWithoutAttrs<lmhlo_gpu::fusedQKVOp>(custom_call,
+                                              /*num_operands=*/2));
+  op.setDotDimensionNumbersAttr(
+      GetDotDimensionNumbersAttr(builder_, config.dot_dimension_numbers()));
+  
+  return op.getOperation();
+}
+
 // Convert an XLA HLO constant to a global_memref + get_global_memref pair.
 tsl::StatusOr<mlir::memref::GetGlobalOp> LhloDialectEmitter::EmitConstant(
     const HloInstruction* instr) {
diff --git a/xla/translate/mhlo_to_lhlo_with_xla/mhlo_to_lhlo_with_xla.h b/xla/translate/mhlo_to_lhlo_with_xla/mhlo_to_lhlo_with_xla.h
index 7022b416d..8e0fa63eb 100644
--- a/xla/translate/mhlo_to_lhlo_with_xla/mhlo_to_lhlo_with_xla.h
+++ b/xla/translate/mhlo_to_lhlo_with_xla/mhlo_to_lhlo_with_xla.h
@@ -84,6 +84,8 @@ class LhloDialectEmitter : public xla::ConstDfsHloVisitorWithDefault {
       const xla::HloCustomCallInstruction* custom_call);
   xla::StatusOr<Operation*> EmitDnnfMHA(
       const xla::HloCustomCallInstruction* custom_call);
+  xla::StatusOr<Operation*> EmitDnnfQKV(
+      const xla::HloCustomCallInstruction* custom_call);    
 
   tsl::StatusOr<memref::GetGlobalOp> EmitConstant(
       const xla::HloInstruction* instr);
diff --git a/xla/xla_data.proto b/xla/xla_data.proto
index c4c0f97d2..33b7e98ed 100644
--- a/xla/xla_data.proto
+++ b/xla/xla_data.proto
@@ -28,6 +28,7 @@ enum PrimitiveType {
   // Invalid primitive type to serve as default.
   PRIMITIVE_TYPE_INVALID = 0;
 
+  VOID = 100;
   // Predicates are two-state booleans.
   PRED = 1;
 
